{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33673adc-6079-48ac-9520-0b7cdc59bbf6",
   "metadata": {},
   "source": [
    "# Proposition Analysis\n",
    "\n",
    "This notebook implements Python code to apply a large language model (LLM) to the tasks of first generating a research/analysis plan and second executing that plan to generate an analysis report.\n",
    "The key input is a proposition -- a statement or question -- that is to be researched.\n",
    "The output is an R Markdown text file containing the report capturing the results and summarization of the research.\n",
    "\n",
    "As the plan, a query graph is created consisting of Query nodes. Some of the nodes pose a question to the LLM, while others first pose a search query to the Serper Google Search API in order to gather current information, and then use the search results as the context for the question it poses to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552843c2-8dd1-4066-a7ed-b3e128f6776e",
   "metadata": {},
   "source": [
    "## Test of Jason Liu's `instructor` Package\n",
    "\n",
    "This notebook began as a quick test of the `instructor` package by Jason Liu ([Instructor Documentation](https://jxnl.github.io/instructor)).  It's strengths are the following:\n",
    "\n",
    "* Can query an LLM based off a class object (i.e., the `response_model` argument) without even having to write a custom system prompt.\n",
    "  - Results in faster implementation of LLM-based applications.\n",
    "* Will auto-validate the LLM's response to ensure that the results are as expected.\n",
    "  - Results in more reliable, production-ready code leveraging LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b95aa-ceaa-4bca-8b15-728efa40f48b",
   "metadata": {},
   "source": [
    "### See the MIT License Notice (per the QueryPlan, Query, ComputedQuery, ComputedQueryGPT4, MergedResponses classes)\n",
    "\n",
    "The Pydantic model classes `QueryPlan`, `Query`, `ComputedQuery`, `ComputedQueryGPT4`, and `MergedResponses` are modifications of the same/similar named classes in the `instructor` package examples for query planning and task planning. Therefore, the MIT License notice appears in a Markdown chunk just above the class definitions.\n",
    "\n",
    "These model classes are modified versions and/or extensions of those same/similar named classes in the [`instructor` package](https://github.com/jxnl/instructor) examples by Jason Liu:\n",
    "\n",
    "* [query_planner_execution.py](https://github.com/jxnl/instructor/blob/main/examples/query_planner_execution/query_planner_execution.py)\n",
    "* [task_planner_topological_sort.py](https://github.com/jxnl/instructor/blob/main/examples/task_planner/task_planner_topological_sort.py)\n",
    "\n",
    "Therefore, the MIT License Notice below, within the \"Model Class Definitions\" section, is included per the instructions at the [`instructor` package's Github repository](https://github.com/jxnl/instructor/tree/main?tab=MIT-1-ov-file#readme).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedfd4e-0c7f-462e-9b58-01250422915b",
   "metadata": {},
   "source": [
    "### To Go Farther...\n",
    "\n",
    "See the Google Collab notebook here: https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbHZMSGlSN2lvX3h1T296Wng1dkpRaHdoWHJfd3xBQ3Jtc0tsMTB4MENQWkRDR2lxZU1ucEtZd0lBM0M3Q25aWFFvWDE5ZjU2N1M0Tjh6ZjFDRmlacTFuYUxwUGJ2aVVOQzY0aDUtU1E0SDE2Y3NoVEdfNjMwLXk0Q0lnVE93N20wZDJMN0I3M3BQRF9rYnpwODNVZw&q=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1Hb-khdfXvYKz8zUoYy7l8yuZ2MDoMAkF%3Fusp%3Dsharing&v=b_fRz09ZP_M\n",
    "\n",
    "* Also see [\"Bridging Language Models with Python with Instructor, Pydantic, and OpenAIâ€™s function calls.\"](https://medium.com/@jxnlco/bridging-language-model-with-python-with-instructor-pydantic-and-openais-function-calling-f32fb1cdb401) by Jason Liu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbf847e-9754-4858-8685-ade3771ed7ea",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7c3b1-5cd4-4216-b565-e9b017217e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how to install the latest version of the instructor package:\n",
    "#!pip install git+https://github.com/jxnl/instructor.git#egg=instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b39a1-f665-48a7-bb89-f2308ac35a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import enum\n",
    "import instructor\n",
    "\n",
    "from typing import Any, Dict, List, Generator, Union, Iterator\n",
    "from openai import OpenAI, ChatCompletion\n",
    "from pydantic import Field, BaseModel, validator, field_validator\n",
    "\n",
    "#client = instructor.patch(OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb62aba-4b5b-4ffd-85f0-fff0f0df4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = '....'\n",
    "os.environ[\"OPENAI_API_KEY\"] = '****' #PUT your API Key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a8dfc-90c0-4f9b-9659-b7301476ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This enables response_model keyword\n",
    "# from client.chat.completions.create\n",
    "client = instructor.patch(OpenAI())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2f2fdf-ebfa-45f3-8c0b-aa8ec9ea56cc",
   "metadata": {},
   "source": [
    "## Search Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68400a2d-49a6-4084-91b9-3a531f64881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "os.environ[\"SERPERAPI_API_KEY\"] = \"****\" #PUT your API Key\n",
    "def search_GoogleSerperAPI(topic: str, raw_results=False):\n",
    "    search = GoogleSerperAPIWrapper(serper_api_key=os.environ['SERPERAPI_API_KEY'])\n",
    "    results = search.results(topic)\n",
    "    if raw_results:\n",
    "        return results\n",
    "    \n",
    "    results_text = \"\"\n",
    "    if results.get('knowledgeGraph'):\n",
    "        rkb = results['knowledgeGraph']\n",
    "        if rkb.get('attributes') is None:\n",
    "            attr_str = ''\n",
    "        else:\n",
    "            attr_str = \"\\n\" + \"\\n\".join([f\"{k}: {v}\" for k,v in rkb['attributes'].items()])\n",
    "\n",
    "        wbs = rkb.get('website')\n",
    "        if wbs is None:\n",
    "            results_text += f\"[{rkb['title']}]({rkb['descriptionLink']}):\\n{rkb['description']}\" + attr_str\n",
    "        else:\n",
    "            results_text += f\"[{rkb['title']}]({rkb['website']}):\\n{rkb['description']}\" + attr_str\n",
    "    \n",
    "    for rec in results['organic']:\n",
    "        if rec.get('attributes') is None:\n",
    "            attr_str = ''\n",
    "        else:\n",
    "            attr = rec.get('attributes')\n",
    "            if attr:\n",
    "                attr_str = \"\\n\" + \"\\n\".join([f\"{k}: {v}\" for k,v in attr.items()])\n",
    "        results_text += f\"\\n* [{rec['title']}]({rec['link']}):\\n{rec['snippet']}\" + attr_str\n",
    "        \n",
    "    return results_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea82f41-f8eb-4d53-bee9-fdee3110fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_raw_search(raw_search: dict) -> str:\n",
    "    results_text = \"\"\n",
    "    if raw_search.get('knowledgeGraph'):\n",
    "        rkb = raw_search['knowledgeGraph']\n",
    "        if rkb.get('attributes') is None:\n",
    "            attr_str = ''\n",
    "        else:\n",
    "            attr_str = \"\\n\" + \"\\n\".join([f\"{k}: {v}\" for k,v in rkb['attributes'].items()])\n",
    "\n",
    "        wbs = rkb.get('website')\n",
    "        try:\n",
    "            if wbs is None:\n",
    "                results_text += f\"[{rkb['title']}]({rkb['descriptionLink']}):\\n{rkb['description']}\" + attr_str\n",
    "            else:\n",
    "                results_text += f\"[{rkb['title']}]({rkb['website']}):\\n{rkb['description']}\" + attr_str\n",
    "        except:\n",
    "            results_text += attr_str\n",
    "    \n",
    "    for rec in raw_search['organic']:\n",
    "        if rec.get('attributes') is None:\n",
    "            attr_str = ''\n",
    "        else:\n",
    "            attr_str = \"\\n\" + \"\\n\".join([f\"{k}: {v}\" for k,v in rec['attributes'].items()])\n",
    "        snippet = rec.get('snippet','')\n",
    "        results_text += f\"\\n* [{rec['title']}]({rec['link']}):\\n{snippet}\" + attr_str\n",
    "\n",
    "    # Strip out the 'Missing:...' and 'Show results with:...' sections\n",
    "    results_text = re.sub(r'\\n*Missing:.+?([\\|\\n])','',results_text)\n",
    "    results_text = re.sub(r'(\\| )*Show results with:.+?([\\|\\n])',r'\\2',results_text)\n",
    "        \n",
    "    return results_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db144b-9bf8-4056-b46c-0e41f7c08de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def to_ascii(text):\n",
    "    # Normalize the Unicode string to its closest ASCII representation\n",
    "    normalized = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "    # Encode to ASCII bytes, then decode back to string, ignoring non-ASCII characters\n",
    "    ascii_text = normalized.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    return ascii_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8a2fc-d25d-404e-b0cd-16855743800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List,Optional\n",
    "from typing_extensions import Annotated\n",
    "from pydantic import AfterValidator\n",
    "import re\n",
    "\n",
    "def validate_response(v:str) -> str:\n",
    "    if len(v)<1:\n",
    "        raise ValueError('Response cannot be an empty string. It must be generated from the internet search results (`raw_results`) in response to the query (`query`).')\n",
    "    return v\n",
    "\n",
    "class SearchSummary(BaseModel):\n",
    "    \"\"\"\n",
    "    The internet search query and the results of an internet search for the query.\n",
    "    \"\"\"\n",
    "    \n",
    "    context: str = Field(...,description=\"The context within which to answer the query. (This may be an empty string.)\")\n",
    "    query: str = Field(...,description=\"The internet search query to perform and then responded to within the given context, if any.\")\n",
    "    #response: str = Field(default='', validate_default=True,description=\"A clear concise response to the query based upon the raw results and generated within the given context.\")\n",
    "    response: Annotated[str, AfterValidator(validate_response)] = Field(...,description=\"A clear concise response to the query based upon the raw results and generated within the given context.\")\n",
    "    raw_results: Optional[str] = Field(default=None, validate_default=True,description=\"The raw results of the internet search.\")\n",
    "\n",
    "    @field_validator('raw_results')\n",
    "    @classmethod\n",
    "    def generate_raw_results(cls, v, validation_info, **kwargs):\n",
    "        # 'values' is a dictionary containing the field values of the model\n",
    "        # Here we generate the response based on the query\n",
    "        values = validation_info.data\n",
    "        query = values.get('query', '')\n",
    "        raw_valid = \"...\"\n",
    "        if query:\n",
    "            raw_valid = search_GoogleSerperAPI(topic = query,raw_results=False)\n",
    "        return raw_valid\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90e975f5-6862-4cd7-b4e6-39a74b00a5d1",
   "metadata": {},
   "source": [
    "## Model Class Definitions\n",
    "\n",
    "These model classes are modified versions and/or extensions of those same/similar named classes in the [`instructor` package](https://github.com/jxnl/instructor) examples by Jason Liu:\n",
    "\n",
    "* [query_planner_execution.py](https://github.com/jxnl/instructor/blob/main/examples/query_planner_execution/query_planner_execution.py)\n",
    "* [task_planner_topological_sort.py](https://github.com/jxnl/instructor/blob/main/examples/task_planner/task_planner_topological_sort.py)\n",
    "\n",
    "Therefore, the MIT License Notice below is included per the instructions at the [`instructor` package's Github repository](https://github.com/jxnl/instructor/tree/main?tab=MIT-1-ov-file#readme):\n",
    "\n",
    "### MIT License Notice\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2023 Michael L. Thompson\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\n",
    "### Knowledge Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3224400b-fc73-4a50-a889-c944aaadd2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Union\n",
    "\n",
    "class Node(BaseModel):\n",
    "    \"\"\"\n",
    "    Concepts, entities, principles, etc. that are central to the topic of discourse.\n",
    "    \"\"\"\n",
    "    id: int\n",
    "    label: str\n",
    "    color: str\n",
    "    description: str\n",
    "\n",
    "class Edge(BaseModel):\n",
    "    \"\"\"\n",
    "    Representation of a Relationship between the source and the target.\n",
    "    For example, if causal, the source is a direct cause of the target, which is its effect. Therefore, it represents a cause-effect relationship\n",
    "    within a causal mechanism.\n",
    "    For example, if inferential, the source is an antecedent proposition and the target is the consequent, which follows logically from the proposition.\n",
    "    Therefore, inferential chains of reasoning are represented.\n",
    "    Relationships can be anything meaningful in the context of the concepts represented by the source and target. They DO NOT have to be causal or\n",
    "    inferential.\n",
    "    \"\"\"\n",
    "    source: int\n",
    "    target: int\n",
    "    label: str\n",
    "    description: str\n",
    "    color: str = \"black\"\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"\n",
    "    You are an expert at capturing the conceptual mapping of concepts and their relationships as a detailed intricate web of knowledge \n",
    "    known as a \"Knowledge Graph\".\n",
    "    Create such a Knowledge Graph.\n",
    "    DO NOT have disjoint/unconnected subgraphs. (Every node should be reachable from every other node along an undirected path.)\n",
    "    \"\"\"\n",
    "    topic: str = Field(..., description=\"The central topic captured by the concepts (nodes) and relationships (edges) in the Knowledge Graph.\")\n",
    "    nodes: List[Node] = Field(..., default_factory=list)\n",
    "    edges: List[Edge] = Field(..., default_factory=list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7123b42-253d-407e-b2e4-5243749d8bc9",
   "metadata": {},
   "source": [
    "### Query Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8535022-786e-4387-9319-259d3bcac92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See documentation on accessing context at BaseModel initialization:\n",
    "# https://docs.pydantic.dev/latest/concepts/validators/#using-validation-context-with-basemodel-initialization\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from contextvars import ContextVar\n",
    "\n",
    "_init_context_var = ContextVar('_init_context_var', default=None)\n",
    "\n",
    "@contextmanager\n",
    "def init_context(value: Dict[str, Any]) -> Iterator[None]:\n",
    "    token = _init_context_var.set(value)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        _init_context_var.reset(token)\n",
    "\n",
    "class TokenCost(BaseModel):\n",
    "    tokens: dict\n",
    "    cost: dict\n",
    "    \n",
    "class ComputeQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    Responds to a query by answering it\n",
    "    \"\"\"\n",
    "\n",
    "    task_id: str = Field(...,description=\"The id of the query node from which `query` came.\")\n",
    "    query: str\n",
    "    response:  Optional[str] = Field(default=None, validate_default=True,description=\"The response, answer and/or search results, to the `query`.\")\n",
    "    \n",
    "    def __init__(__pydantic_self__, **data: Any) -> None:\n",
    "        __pydantic_self__.__pydantic_validator__.validate_python(\n",
    "            data,\n",
    "            self_instance=__pydantic_self__,\n",
    "            context=_init_context_var.get(),\n",
    "        )\n",
    "\n",
    "    @field_validator('response')\n",
    "    @classmethod\n",
    "    def generate_response(cls, v, validation_info, **kwargs):\n",
    "        # 'values' is a dictionary containing the field values of the model\n",
    "        # Here we generate the response based on the query\n",
    "        values = validation_info.data\n",
    "        query = values.get('query', '')\n",
    "        \n",
    "        if v is None:\n",
    "            client = validation_info.context.get('client')\n",
    "            model = validation_info.context.get('model')\n",
    "            if query:\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model, \n",
    "                    temperature=0, \n",
    "                    messages=[\n",
    "                        {'role':'system',\n",
    "                         'content':\"\"\"You are an expert at logical and probabilistic reasoning. \n",
    "Use step-by-step logical reasoning to answer each question posed to you.\n",
    "Answer as accurately and thoroughly as possible. Base your conclusions upon fact-based data. \n",
    "Cite the data in your responses, including URLs when available.\"\"\"},{'role':'user', 'content':query}], \n",
    "                    max_tokens=3000\n",
    "                )\n",
    "                return completion.choices[0].message.content\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9deaacb-a007-400a-952e-804cd1c8f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False\n",
    "if TEST:\n",
    "    client = instructor.patch(OpenAI())\n",
    "    with init_context({'client': client, 'model': 'gpt-4-turbo-preview'}):\n",
    "        resp = ComputeQuery(task_id='root',query='Hello, how are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba2ef1-3dd0-4657-b9d1-805b62807f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    print(resp)\n",
    "    #tkncst = resp.completion.usage\n",
    "    #print(tkncst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b935e-d31f-49da-aa1f-418e47ea6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergedResponses(BaseModel):\n",
    "    \"\"\"\n",
    "    Models a merged response of multiple queries.\n",
    "    Currently we just concatenate them but we can do much more complex things.\n",
    "    \"\"\"\n",
    "\n",
    "    responses: List[ComputeQuery]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad579428-f20a-4e60-8861-0c0da456d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def markdown_table_to_dataframe(markdown_text):\n",
    "    # Split the text into lines\n",
    "    lines = markdown_text.strip().split('\\n')\n",
    "    \n",
    "    # Extract the header line and data lines\n",
    "    header_line = lines[0]\n",
    "    data_lines = lines[2:]  # Skip the Markdown alignment row\n",
    "\n",
    "    # Split the header and data lines into cells\n",
    "    headers = header_line.split('|')[1:-1]  # Skip the outer empty cells\n",
    "    headers = [txt.strip() for txt in headers]\n",
    "    data = [line.split('|')[1:-1] for line in data_lines]\n",
    "\n",
    "    # Create and return a DataFrame\n",
    "    return pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# Example usage\n",
    "markdown_text = \"\"\"\n",
    "| Column1 | Column2 | Column3 |\n",
    "|---------|---------|---------|\n",
    "| data1   | data2   | data3   |\n",
    "| data4   | data5   | data6   |\n",
    "\"\"\"\n",
    "df = markdown_table_to_dataframe(markdown_text)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472c985-f445-43a1-82d4-2637c0b3d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#        * DO NOT create a Query node with only a single dependency that performs a search. Instead, combine the question and the search query \n",
    "#          into a single Query node with an assigned `search_query` field to perform the search and `question` field to ask the question.\n",
    "\n",
    "class Query(BaseModel):\n",
    "    \"\"\"\n",
    "    A question in a question-answer query plan that may or may not require search. \n",
    "    If need data or current information by internet searching, specify a `search_query`:\n",
    "        * Pose a valid question for the `question` field, or as a \"Tabulate..\" statement, if use a list of search strings for `search_query`.\n",
    "        * Specify a `search_query`, if you DO need to search the internet to best answer the question.\n",
    "        * Specify a list of search strings for `search_query` if you need a set of statistics, indicators, metrics on a set of entities: include a search string for each combo.\n",
    "        * Leave `search_query`=[], if you DO NOT need to search the internet.\n",
    "    In the case of unknown quantities in either the `question` or the `search_query`, to be supplied by answering the questions `dependencies` or searching, \n",
    "    place square brackets,'[' and ']' around a descriptive term or phrase describing the unknown.\n",
    "    \"\"\"\n",
    "\n",
    "    id: str = Field(..., description=\"Unique id of the query\")\n",
    "    question: str = Field(...,description=\"Question we are asking.\")\n",
    "    dependencies: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of at most 9 sub-questions that need to be answered or searched before we can ask the question. Use a subquery when anything may be unknown, and we need to ask multiple questions to get the answer. Dependences must only be other queries.\",\n",
    "    )\n",
    "    search_query: Optional[List[str]] = Field(default=[],description=\"Search queries seeking latest information so as to provide context for and improve the answer to the `question`. If have more than one entity or indicator to search for, specify a list of search queries over all entity & indicator combinations.\")\n",
    "        \n",
    "    @field_validator('dependencies')\n",
    "    @classmethod\n",
    "    def validate_dependencies(cls, v, validation_info, **kwargs):\n",
    "        dependencies = v\n",
    "        # 'values' is a dictionary containing the field values of the model\n",
    "        values = validation_info.data\n",
    "        id = values.get('id', '')\n",
    "        MAX_DEPENDENCIES = 9\n",
    "        if len(dependencies) > MAX_DEPENDENCIES:\n",
    "            raise ValueError(f\"\"\"Field `dependencies` has more than {MAX_DEPENDENCIES}, which is the maximumn number allowed.\n",
    "Fix this by doing one or both of the following:\n",
    "1. Create a `Query` node with a longer list of search queries in its `search_query` field so that more entitities & statistics can be gathered into a single node, and assign a \"Tabulate...\" statement to its `question` field.\n",
    "2. Create an intermediate layer of new `Query` nodes as dependencies:\n",
    "   * Subset the current list of `dependencies` into logical sets of no more than {MAX_DEPENDENCIES} `Query` nodes each according to the `question` field of each node in the list.\n",
    "   * For each subset, create a new `Query` node with its field `dependencies` set to the `Query` nodes in the subset and an appropriate `question` field.\n",
    "   * Assign the list of these new set of `Query` nodes as the fixed `dependencies` field.\"\"\")\n",
    "        return dependencies\n",
    "     \n",
    "    async def aexecute_dynamic(self, sub_answers: MergedResponses,client, model=\"gpt-3.5-turbo\", print_level=0) -> ComputeQuery:\n",
    "        \"\"\"\n",
    "        Executes the task by asking the question and returning the answer.\n",
    "        Dynamically generates the search query for any node that has both a search_query string and at least 1 dependency.\n",
    "        \"\"\"\n",
    "        \n",
    "        def fill_in_dynamic(query,context):\n",
    "            system_prompt = \"\"\"You are an expert at filling in unknown entities, enclosed by square brackets '[]', with the correct values gathered from <<CONTEXT>>. \n",
    "Based upon <<CONTEXT>>, generate a <<QUERY STRING>> by replacing any unknowns in the <<PROPOSED QUERY STRING>> with the correct information drawn from <<CONTEXT>>.\n",
    "The new <<QUERY STRING>> should have a resolved term or phrase in place of the unknown.\n",
    "Respond with only the <<QUERY STRING>> and no other text. \n",
    "DO NOT attempt to answer the <<PROPOSED QUERY STRING>> or the new <<QUERY STRING>>.\n",
    "If the <<PROPOSED QUERY STRING>> does not contain unknowns and so is perfectly fine, just respond with the <<PROPOSED QUERY STRING>>.\"\"\"\n",
    "            user_prompt = f\"\"\"\n",
    "<<CONTEXT>>: \n",
    "{context}\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "<<PROPOSED QUERY STRING>>: \n",
    "{query}\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "<<QUERY STRING>>:\n",
    "\"\"\"\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\", \n",
    "                temperature=0, \n",
    "                messages=[ {'role':'system', 'content': system_prompt},{'role':'user', 'content': user_prompt}],\n",
    "                max_tokens=10000).choices[0].message.content\n",
    "            \n",
    "            return response\n",
    "\n",
    "\n",
    "        def search_for_context(search_query_original,context='',compound=False,print_level=0):\n",
    "            search_query = search_query_original\n",
    "            if compound:\n",
    "                if re.search(r'\\[[^\\]]+\\]',search_query):\n",
    "                    # Dynamically generate the search query by calling an LLM to construct the search query\n",
    "                    if print_level > 1:\n",
    "                        print(f\"\\t\\t====== dynamic search query generation for: '{search_query_original}' ========\")\n",
    "                        print(f\"\\t\\t=== using context: '{context}' ========\")\n",
    "                    search_query = fill_in_dynamic(query=search_query_original, context=context)\n",
    "                    if print_level > 1:\n",
    "                        print(f\"\\t\\t====== dynamic search query result: '{search_query}' ========\")\n",
    "\n",
    "\n",
    "            # Retrieve the answer to the search query from the search_cache, if present; else perform a Google Serper API search\n",
    "            search_context = search_cache.get(search_query,None)\n",
    "            \n",
    "            if search_context is None:\n",
    "                # Perform an internet search to add context to the query.\n",
    "                if print_level > 1:\n",
    "                    print(f\"...performing search: {search_query} ...\")\n",
    "                search_context = str(search_GoogleSerperAPI(topic = search_query,raw_results=True))\n",
    "                search_cache[search_query] = search_context\n",
    "            else:\n",
    "                if print_level > 1:\n",
    "                    print(f\"...retrieved search from cache: {search_query} ...\")\n",
    "\n",
    "            # Augment the existing context (possibly from dependencies, else empty) with the search results `search_context`\n",
    "            if compound:\n",
    "                if len(search_context)>0:\n",
    "                    #context += '\\n\\n' + search_context\n",
    "                    # Must add in search_context so that context remains a valid dictionary when executing eval(context)\n",
    "                    context = f\"{{'merged_responses': {context},\\n\\n'search_results': {search_context} }}\"\n",
    "                if print_level > 1:\n",
    "                    print('\\t... done gathering subqueries *')\n",
    "            else:\n",
    "                context = search_context\n",
    "                \n",
    "            return (context,search_query)\n",
    "\n",
    "        \n",
    "        \n",
    "        if print_level > 0:\n",
    "            print(\"Executing\",f\"`{self.question}`\")\n",
    "        length_dependencies = len(sub_answers.responses)\n",
    "        compound = length_dependencies > 0\n",
    "        query_string = self.question\n",
    "        do_search = len(self.search_query)>0\n",
    "        search_context=''\n",
    "        sleep_time = 1\n",
    "\n",
    "        if compound:\n",
    "            if print_level > 1:\n",
    "                print(\"\\tExecuting with\", length_dependencies, \"dependencies\")\n",
    "                print(f\"\\t* Gathering subqueries for {self.question}...\")\n",
    "            # Get the json equivalent of the MergedResponses object\n",
    "            #sub_answers_text = sub_answers.model_dump_json()\n",
    "            \n",
    "            dict0 = sub_answers.model_dump()\n",
    "            dict0['responses'] = [ {key: re.sub(r\"Context: ##`.+?`##\",\"\",value,flags=re.DOTALL) for key,value in cq_dict.items() if key in ['task_id','query','response']} for cq_dict in dict0['responses'] ]\n",
    "            cqs = [ ComputeQuery(**cq_dict) for cq_dict in dict0['responses'] ]\n",
    "\n",
    "            # This should rarely happen, but... check if potentially too much context.\n",
    "            total_chars = sum([ len(cq.response) for cq in cqs ])\n",
    "            if total_chars > 20000:\n",
    "                cqs_truncated = []\n",
    "                for cq in cqs:\n",
    "                    response_split = cq.response.split(' ')\n",
    "                    if len(response_split) > 485: # If too long, truncate response\n",
    "                        if print_level > 1:\n",
    "                            print(f\"\\t\\t... truncating id={cq.task_id} ...\")\n",
    "                        response_split = response_split[:485] + ['...']\n",
    "                    cq.response = ' '.join(response_split)\n",
    "                    cqs_truncated.append(cq)\n",
    "                cqs = cqs_truncated\n",
    "                \n",
    "            sub_answers_text = MergedResponses(responses=cqs).model_dump_json()\n",
    "            \n",
    "            # Strip away the contexts of the subqueries, which often are identical or at least share a lot of text in common among all of them.\n",
    "            #sub_answers_text = re.sub(r\"Context: ##`.*?`##\",\"\",sub_answers_text,flags=re.DOTALL)\n",
    "\n",
    "            # At this point context is a string that will return a valid dictionary when executing eval(context)\n",
    "            context = sub_answers_text \n",
    "            sleep_time = 2\n",
    "            if re.search(r'\\[[^\\]]+\\]',query_string):\n",
    "                # Dynamically generate the query string by calling an LLM to construct replace missing info between square brackets with the correct info\n",
    "                if print_level > 1:\n",
    "                    print(f\"\\t\\t====== dynamic query string generation for: '{query_string}' ========\")\n",
    "                    print(f\"\\t\\t=== using context: '{context}' ========\")\n",
    "                    query_string = fill_in_dynamic(query=query_string, context=context)\n",
    "                if print_level > 1:\n",
    "                    print(f\"\\t\\t====== dynamic query string result: '{query_string}' ========\")\n",
    "        else:\n",
    "            context = ''\n",
    "\n",
    "        if do_search:\n",
    "            if isinstance(self.search_query,str):\n",
    "                context,srch_qry = search_for_context(search_query_original=self.search_query,context=context,compound=compound,print_level=print_level)\n",
    "                self.search_query = srch_qry\n",
    "            elif isinstance(self.search_query,list):\n",
    "                context_list = []\n",
    "                srch_qries = []\n",
    "                for sq in self.search_query:\n",
    "                    cntxt,srch_qry = search_for_context(search_query_original=sq,context=context,compound=compound,print_level=print_level)\n",
    "                    context_list.append(cntxt)\n",
    "                    srch_qries.append(srch_qry)\n",
    "                context = f\"[{','.join(context_list)}]\"\n",
    "                self.search_query = srch_qries\n",
    "            if print_level>1:\n",
    "                print(f'***********{self.search_query}************')\n",
    "\n",
    "        # Augment the LLM prompt query with the context, if any\n",
    "        if len(context)>0:\n",
    "            query_string = f\"{query_string}\\nContext: ##`{context}`##\"\n",
    "\n",
    "        # Call the LLM to generate a response to the query\n",
    "        if print_level > 1:\n",
    "            print(f\"\\t.... **computing query** for '{self.question}' ({model}) ...\")\n",
    "        \n",
    "        with init_context({'client': client, 'model': model}):\n",
    "            resp = ComputeQuery(task_id=self.id, query=query_string)\n",
    "        await asyncio.sleep(sleep_time)\n",
    "        \n",
    "        if print_level > 2:\n",
    "            pprint(resp.model_dump())\n",
    "        if print_level > 0:\n",
    "            print(f\"... done with '{self.question}' ({model}) ******\")\n",
    "        \n",
    "        return resp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea80be9-d2ae-4031-bb0e-f57124f15186",
   "metadata": {},
   "source": [
    "### Query Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d0b13-206e-40d8-8bad-f0ef953cf26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query0:\n",
    "    def __init__(self, id, dependencies):\n",
    "        self.id = id\n",
    "        self.dependencies = dependencies\n",
    "\n",
    "def dfs(query, visited, all_queries):\n",
    "    visited.add(query.id)\n",
    "    for dep_id in query.dependencies:\n",
    "        if dep_id in all_queries and dep_id not in visited:\n",
    "            dfs(all_queries[dep_id], visited, all_queries)\n",
    "\n",
    "def find_unreachable_queries(queries):\n",
    "    if not queries:\n",
    "        return []\n",
    "\n",
    "    all_queries = {query.id: query for query in queries}\n",
    "    visited = set()\n",
    "\n",
    "    # Start DFS from the first query\n",
    "    dfs(queries[0], visited, all_queries)\n",
    "\n",
    "    # Find queries that were not visited\n",
    "    unreachable = [query for query in queries if query.id not in visited]\n",
    "\n",
    "    return unreachable\n",
    "\n",
    "def dfs_detect_loop(query, visited, rec_stack, all_queries):\n",
    "    visited.add(query.id)\n",
    "    rec_stack.add(query.id)\n",
    "\n",
    "    for dep_id in query.dependencies:\n",
    "        if dep_id in all_queries:\n",
    "            if dep_id not in visited:\n",
    "                if dfs_detect_loop(all_queries[dep_id], visited, rec_stack, all_queries):\n",
    "                    return True\n",
    "            elif dep_id in rec_stack:\n",
    "                # Found a node in the recursion stack: cycle detected\n",
    "                return True\n",
    "\n",
    "    rec_stack.remove(query.id)\n",
    "    return False\n",
    "\n",
    "def detect_loops(queries):\n",
    "    all_queries = {query.id: query for query in queries}\n",
    "    visited = set()\n",
    "    rec_stack = set()\n",
    "\n",
    "    for query in queries:\n",
    "        if query.id not in visited:\n",
    "            if dfs_detect_loop(query, visited, rec_stack, all_queries):\n",
    "                return True  # Loop detected\n",
    "\n",
    "    return False  # No loop found\n",
    "\n",
    "# Example usage\n",
    "queries = [\n",
    "    Query0(\"A\", [\"B\", \"C\"]),\n",
    "    Query0(\"B\", [\"C\"]),\n",
    "    Query0(\"C\", [\"A\"]),  # This creates a loop\n",
    "    Query0(\"D\", [\"A\"]),\n",
    "    Query0(\"E\", [])\n",
    "]\n",
    "\n",
    "print(\"Loop detected:\", detect_loops(queries))\n",
    "\n",
    "# Example usage\n",
    "queries = [\n",
    "    Query0(\"A\", [\"B\", \"C\"]),\n",
    "    Query0(\"B\", [\"C\"]),\n",
    "    Query0(\"C\", []),\n",
    "    Query0(\"D\", [\"A\"]),\n",
    "    Query0(\"E\", [])\n",
    "]\n",
    "\n",
    "unreachable = find_unreachable_queries(queries)\n",
    "for query in unreachable:\n",
    "    print(query.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b14e5-c177-46b0-a9e1-8a4ade142845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_graph = assessment_orient_express2['plan'].query_graph\n",
    "#unreachable = find_unreachable_queries(query_graph)\n",
    "#for query in unreachable:\n",
    "#    print(query.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a4212-e673-4eee-b335-1b47fd7cfe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatusReport(BaseModel):\n",
    "    \"\"\"\n",
    "    Status report of the current plan under execution.\n",
    "    Given the JSON of the current plan, perform these 3 steps:\n",
    "    * `status` -- Assess how well the plan is progressing towards the root question.\n",
    "    * `reasoning` -- Explain your reasoning behind your `status` response, and list suggested improvements after a heading '**Suggestions**:', assuming queued up queries will work fine when run. \n",
    "    * `suggestions` -- Convert any improvements in '**Suggestions**:' section of `reasoning` into a list of new `Query` nodes; or `None`, if no improvements necessary.\n",
    "      - DO NOT include `Query` nodes in improvements that are already in the plan (executed or queued up).\n",
    "    \"\"\"\n",
    "    status: str = Field(...,description=\"Assessment of adequacy of the current plan (executed and queued up queries) to answer the root question. Judge how well the plan is executing towards its goal. Consider whether the `response` fields include sufficient data & information in general to fulfill the purposes of the questions for whom they are one of the dependencies.\")\n",
    "    reasoning: str = Field(...,description=\"Detailed explanation of the step-by-step logical reasoning used to arrive at the `status`. If make suggestions, include '**Suggestions**:...' and list them.\")\n",
    "    suggestions:  Optional[List[Query]] = Field(...,description=\"If necessary, a list of new `Query` nodes (with appropriate `dependencies` and `search_query` fields) to implement `reasoning` field '**Suggestions**'. Else, `None`.\")\n",
    "\n",
    "    @field_validator('suggestions')\n",
    "    @classmethod\n",
    "    def generate_suggestions(cls, v, validation_info, **kwargs):\n",
    "        # 'values' is a dictionary containing the field values of the model\n",
    "        values = validation_info.data\n",
    "        reasoning = values.get('reasoning')\n",
    "        #print(f'\\n------------\\n{reasoning}\\n------------\\n')\n",
    "        \n",
    "        if (v is None):\n",
    "            if reasoning:\n",
    "                search_substring = \"**Suggestions**\"\n",
    "                if search_substring.lower() in reasoning.lower():\n",
    "                    raise ValueError(f\"The `reasoning` field proposes '**Suggestions**', so the `suggestions` field cannot be `None`. It must include `Query` nodes to implement them:\\n`reasoning`={reasoning}\")\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e68abb-3bef-43fe-801e-8a66d391a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryPlan(BaseModel):\n",
    "    \"\"\"\n",
    "    A query plan to answer a main 'root' question. Make sure every question is asked only once. Capture the plan as a textual outline in `query_plan_outline`.   \n",
    "    First, define `search_query_for_context`, and use its automatically generated search results in `context` as you build the plan.\n",
    "    The 'root' of the query tree and must appear as the first item in `query_graph` and in the 'query_plan_outline`.\n",
    "    For all nodes requiring current data and the latest information, use `search_query` fields as lists of necessary internet search queries. Tabulate entities by metrics for comparisons. \n",
    "    \n",
    "    It is better to have few `Query` nodes, each with long `search_query` lists and a \"Tabulate...\" statement for `question`, than to have many `Query` nodes, each with a 1-element `search_query` list.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    search_query_for_context: str = Field(...,description=\"An internet search query to provide context for the plan.\")\n",
    "    context:  Optional[str] = Field(default=None, validate_default=True,description=\"Use this context as you build the plan. First, set search_query_for_context. Then this is set automatically as the result of searching the internet with query in search_query_for_context. \")\n",
    "    query_plan_outline: str = Field(...,description=\"An indented outline of the query plan matching the query tree, where each entry is the query to be posed.\")\n",
    "    query_graph: List[Query] = Field(..., description=\"A list of `Query` nodes: First, the original question we are asking, its dependencies, and so on, as a query graph. Refer to `context` for background context in generating the query graph.\")\n",
    "\n",
    "    @field_validator('context')\n",
    "    @classmethod\n",
    "    def generate_context(cls,v,validation_info,**kwargs):\n",
    "        if v is None:\n",
    "            context = v\n",
    "        else:\n",
    "            try:\n",
    "                eval(v)\n",
    "                context = v # Must be a valid python expression (actually, a raw search result from search_GoogleSerperAPI()\n",
    "            except Exception:\n",
    "                context = None\n",
    "\n",
    "        values = validation_info.data\n",
    "        #if context is None:\n",
    "        search_query_for_context = values.get('search_query_for_context')\n",
    "        if context is None:\n",
    "            if search_query_for_context:\n",
    "                context = str(search_GoogleSerperAPI(topic = search_query_for_context,raw_results=True))\n",
    "            else:\n",
    "                raise ValueError(\"Field `context` cannot be set until `search_query_for_context` is specified. Then `context` is set automatically by searching the internet with search_query_for_context as the search query string.\")\n",
    "        #else:\n",
    "            #raise ValueError(\"Field `context` must be set by specifying `search_query_for_context` first. Then `context` is set automatically by searching the internet with search_query_for_context as the search query string.\") \n",
    "\n",
    "        return context\n",
    "    \n",
    "    @field_validator('query_graph')\n",
    "    @classmethod\n",
    "    def validate_query_graph(cls, v, validation_info, **kwargs):\n",
    "        query_graph = v\n",
    "        # 'values' is a dictionary containing the field values of the model\n",
    "        values = validation_info.data\n",
    "        if query_graph:\n",
    "            no_path = find_unreachable_queries(query_graph)\n",
    "            if len(no_path) > 0:\n",
    "                raise ValueError(f\"\"\"All `Query` nodes in field `query_graph` must be on the `dependencies` path from the `root` question `Query` node, which should be the first node in the `query_graph`. Currently, these nodes are not: {no_path}.\\n\n",
    "    They do not have to be direct dependencies of the `root` question but, if not, they must be among the dependencies of the dependencies of the dependencies, etc.\"\"\")\n",
    "            if detect_loops(query_graph):\n",
    "                raise ValueError(f\"\"\"Field `query_graph` should NOT contain loops. A loop exists if at least one `Query` node has a path through its `dependencies` and their `dependencies`, and so on, that leads back to that node.\"\"\")\n",
    "        return query_graph\n",
    "\n",
    "    \n",
    "    def _get_execution_order(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        Returns the order in which the tasks should be executed using topological sort.\n",
    "        Copied from: https://github.com/jxnl/instructor/blob/main/examples/task_planner/task_planner_topological_sort.py\n",
    "        Inspired by https://gitlab.com/ericvsmith/toposort/-/blob/master/src/toposort.py\n",
    "        \"\"\"\n",
    "        tmp_dep_graph = {item.id: set(item.dependencies) for item in self.query_graph}\n",
    "\n",
    "        def topological_sort(\n",
    "            dep_graph: dict[int, set[int]],\n",
    "        ) -> Generator[set[int], None, None]:\n",
    "            while True:\n",
    "                ordered = set(item for item, dep in dep_graph.items() if len(dep) == 0)\n",
    "                if not ordered:\n",
    "                    break\n",
    "                yield ordered\n",
    "                dep_graph = {\n",
    "                    item: (dep - ordered)\n",
    "                    for item, dep in dep_graph.items()\n",
    "                    if item not in ordered\n",
    "                }\n",
    "            if len(dep_graph) != 0:\n",
    "                raise ValueError(\n",
    "                    f\"Circular dependencies exist among these items: {{{', '.join(f'{key}:{value}' for key, value in dep_graph.items())}}}\"\n",
    "                )\n",
    "\n",
    "        result = []\n",
    "        for d in topological_sort(tmp_dep_graph):\n",
    "            result.extend(sorted(d))\n",
    "        return result\n",
    "\n",
    "\n",
    "    \n",
    "    def strip_context(cq):\n",
    "        cq.response = re.sub(r\"Context: ##`.+?`##\",\"\",cq.response,flags=re.DOTALL)\n",
    "        return cq\n",
    "\n",
    "\n",
    "    async def execute_dynamic(self,client, model = \"gpt-3.5-turbo\",status_updates=False,print_level=0) -> dict[int, ComputeQuery]:\n",
    "        \"\"\"\n",
    "        Executes the queries in the query plan in the correct order using asyncio and chunks with answered dependencies.\n",
    "        Copied from: https://github.com/jxnl/instructor/blob/main/examples/task_planner/task_planner_topological_sort.py\n",
    "        Dynamically generate the query string for any node that has at least 1 dependency and\n",
    "        if the question contains any unknowns enclosed by '[]'.\n",
    "        Dynamically generate the search query for any node that has both a search_query string and at least 1 dependency and\n",
    "        if the search query contains any unknowns enclosed by '[]'.\n",
    "        \"\"\"\n",
    "        execution_order = self._get_execution_order()\n",
    "        tasks = {q.id: q for q in self.query_graph}\n",
    "        task_results = {}\n",
    "        status = None\n",
    "        while True:\n",
    "            ready_to_execute = [\n",
    "                tasks[task_id]\n",
    "                for task_id in execution_order\n",
    "                if task_id not in task_results\n",
    "                and all(\n",
    "                    subtask_id in task_results for subtask_id in tasks[task_id].dependencies\n",
    "                )\n",
    "            ]\n",
    "            if print_level > 1:\n",
    "                # prints chunks to visualize execution order\n",
    "                print(f'*** QUEUE: {len(ready_to_execute)} item(s)')\n",
    "                print(ready_to_execute)\n",
    "                print('***')\n",
    "\n",
    "            computed_answers = await asyncio.gather(\n",
    "                *[\n",
    "                    q.aexecute_dynamic(\n",
    "                        sub_answers=MergedResponses(\n",
    "                            responses=[\n",
    "                                result #strip_context(result)\n",
    "                                for result in task_results.values()\n",
    "                                if result.task_id in q.dependencies\n",
    "                            ]\n",
    "                        ),\n",
    "                        client=client,\n",
    "                        model = model,\n",
    "                        print_level = print_level\n",
    "                    )\n",
    "                    for q in ready_to_execute\n",
    "                ]\n",
    "            )\n",
    "            for answer in computed_answers:\n",
    "                task_results[answer.task_id] = answer\n",
    "\n",
    "            if status_updates:\n",
    "                if status:\n",
    "                    prior_suggestions = f\"\\n\\n::PRIOR SUGGESTED NODES::\\n{status.suggestions}\\n\\n\"\n",
    "                else:\n",
    "                    prior_suggestions = \"\"\n",
    "                # Determine progress towards goal.\n",
    "                plan_dict = {id: \n",
    "                             {'question': re.sub(r'Context: ##`.+?`##',\"\",task_results[id].query,flags=re.DOTALL) if id in task_results else tasks[id].question, \n",
    "                              'search_query': tasks[id].search_query, \n",
    "                              'response': (\"**DONE**:\\n\" + task_results[id].response) if id in task_results else '**IN THE PROCESSING QUEUE**',\n",
    "                              'dependencies': tasks[id].dependencies} \n",
    "                             for id in execution_order}\n",
    "                plan_string = json.dumps(plan_dict,indent=4)\n",
    "                query=f\"\"\"\n",
    "::PLAN::\n",
    "{plan_string}\n",
    "{prior_suggestions}\n",
    "\"\"\"\n",
    "                status: StatusReport = client.chat.completions.create(\n",
    "                    model=model, \n",
    "                    temperature=0, \n",
    "                    messages=[\n",
    "                         {'role':'system',\n",
    "                         'content':f\"\"\"You are an expert at logical and probabilistic reasoning. \n",
    "Use step-by-step logical reasoning to assess how well the given ::PLAN:: query graph is progressing towards its goal of resolving the 'root' question.\n",
    "Consider how comprehensive, thorough, balanced, and intellectually honest the ::PLAN:: is.  \n",
    "For example, does it comprehensively consider all alternatives?\n",
    "Does it collect evidence for and against propositions?\n",
    "Also, note how successful the completed and queued portions of ::PLAN:: are at meeting their goals. \n",
    "For example, are there many pieces of missing information in tables?\n",
    "If there are any gaps or shortcomings, suggest potential new `Query` nodes that could fill in these gaps, and describe\n",
    "them as a '**Suggestions**' list in the 'reasoning' field and generate them as a list of `Query` nodes in the `suggestions` field. \n",
    "Be sure to assign appropriate `dependencies` and `search_query` fields for any suggested `Query` nodes.\n",
    "DO NOT suggest nodes that currently exist in the plan as completed or queued.\n",
    "DO NOT suggest nodes that are tangential or irrelevant to the central issue: '{prop}'.\"\"\"},\n",
    "                        {'role':'user', 'content': query}\n",
    "                    ], \n",
    "                    response_model=StatusReport,\n",
    "                    max_retries=2,\n",
    "                    max_tokens=3000\n",
    "                )\n",
    "                #print(status.suggestions)\n",
    "                suggestion_string = \"None\" if status.suggestions is None else json.dumps({q.id: q.model_dump() for q in status.suggestions},indent=4)                \n",
    "                status_report_string = '\\n\\n'+('-'*150)+'\\n\\n' + f\"**Status**: {status.status}\\n\\n**Reasoning**: {status.reasoning}\\n\\n**Suggested Nodes**:\\n{suggestion_string}\" + '\\n\\n'+('-'*150)+'\\n\\n'\n",
    "                print(status_report_string)\n",
    "            \n",
    "            if len(task_results) == len(execution_order):\n",
    "                break\n",
    "        return {k: task_results[k] for k in execution_order}\n",
    "\n",
    "    def dependencies(self, idz: List[int]) -> List[Query]:\n",
    "        \"\"\"\n",
    "        Returns the dependencies of the query with the given id.\n",
    "        \"\"\"\n",
    "        return [q for q in self.query_graph if q.id in idz]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04942e-8c7f-48f8-97ea-aab8f45be256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query.model_rebuild()\n",
    "#QueryPlan.model_rebuild()\n",
    "\n",
    "\n",
    "def query_planner(question: str,client) -> QueryPlan:\n",
    "    PLANNING_MODEL = \"gpt-4-turbo-preview\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a world-class query planning algorithm capable of breaking apart a question into its dependency queries and search queries for current data such that the answers can be used\n",
    "to inform the parent question. Do not answer the questions, simply provide a query plan as a correct compute graph with a Query node for the parent question at the top and all other Query nodes connecting to\n",
    "it directly as its dependencies or indirectly as the dependencies of its dependencies, and so on.\n",
    "\n",
    "When building the query plan, carefully consider whether a question requires an internet search for the latest information and current data. If so, then define a Query node with a search_query field\n",
    "that has an appropriate list of internet search queries to obtain the information. If searching for a set of entities or concepts and statistics or metrics for each, tabulate the results.\n",
    "\n",
    "Before you define Query nodes, think step-by-step using logical reasoning to get a better understanding of the problem.\n",
    "Be sure to  define the `search_query` field for any steps in the query plan that require the latest information obtainable by internet searching before answering its question.\n",
    "Review the `query_graph` and make sure that the `dependencies` of each Query node are truly just those nodes that directly inform that node's `question`, i.e. consider conditional independence properties.\n",
    "Fix the dependencies throughout the graph to ensure that this is indeed the case.\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Consider: {question}\\n\\nGenerate the correct query plan.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    root = client.chat.completions.create(\n",
    "        model=PLANNING_MODEL,\n",
    "        temperature=0,\n",
    "        response_model=QueryPlan,\n",
    "        messages=messages,\n",
    "        max_retries = 2,\n",
    "        max_tokens=4000,\n",
    "    )\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27064d2-b26b-4b75-ab55-5a2822b65f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_upon_plan(plan) -> QueryPlan:\n",
    "    PLANNING_MODEL = \"gpt-4-turbo-preview\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a world-class query-planning algorithm. Your task is to take a previously generated query plan identified by the JSON provided to you as ::PLAN:: and to improve upon it.\n",
    "The improved plan must have the exact same \"root\" question (i.e., top proposition) as does the original plan. However, other `Query` nodes in the plan may change if you think it is absolutely necessary for an improved plan.\n",
    "If the query plan is satisfactory as presented in ::PLAN::, then merely copy it.\n",
    "\n",
    "Use step-by-step logical reasoning to apply the IMPROVEMENT STRATEGY below to build up an improved query plan.\n",
    "\n",
    "IMPROVEMENT STRATEGY:\n",
    "* Improve and apply the search context.\n",
    "  * Make sure that the `search_query_for_context` field is written so as to obtain the most relevant background information needed to best define the `question` and `search_query` fields\n",
    "    of the `Query` nodes supplying the most definitional concepts and entities for the context of the entire reasoning behind the query plan.\n",
    "* Consolidate `Query` nodes on parallel topics (e.g., collecting similar statistics for comparable entities) and improve search query lists.\n",
    "  * Make sure that rather than having many separate `Query` nodes, each with a singleton list as its `search_query` field, that instead such comparable `Query` nodes as a set are consolidated\n",
    "    into a single `Query` node with a list of all of those search queries as its `search_query` field, and that as its `question` field it has a \"Tabulate...\" statement (or \"Enumerate...\" statement) \n",
    "    to consolidate the information into a single response.\n",
    "  * DO NOT consolidate `Query` nodes on disimilar, non-parallel topics.\n",
    "* Improve dependency structure.\n",
    "  * Make sure that the `Query` node `dependencies` fields are arranged such that the `Query` nodes that identify the most basic, definitional concepts, and entities\n",
    "    appear in the `dependencies` field of `Query` nodes  that tabulate statistics or metrics on them and/or assess them.\n",
    "  * Make sure that the `Query` node `dependencies` fields are arranged such that the `Query` nodes that identify the critical comparative statistics, metrics and indicators of the entities\n",
    "    supplying evidence appear in the `dependencies` field of `Query` nodes that consolidate, tabulate and compare them.\n",
    "  * Make sure that the `Query` nodes that are entity-comparing or statistics-tabulating appear in the `dependencies` fields of `Query` nodes that propose or critique propositions.\n",
    "  * Apply the concept of conditional independence so that `Query` nodes need not include basic/primitive/evidential nodes in their `dependencies` list if such nodes appear as `dependencies` of the\n",
    "    `dependencies`, and so on.  As long as ALL nodes are on a `dependencies` pathway from the top proposition (first) `Query` node in the `query_graph`.\n",
    "* Rewrite the outline, if necessary.\n",
    "  * Make sure that the `query_plan_outline` is rewritten to reflect the new improved dependency structure.\n",
    "\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "--------------------------\n",
    "Consider ::PLAN::\n",
    "{plan.model_dump_json()}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Improve upon the query plan.\"\"\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    root = client.chat.completions.create(\n",
    "        model=PLANNING_MODEL,\n",
    "        temperature=0,\n",
    "        response_model=QueryPlan,\n",
    "        messages=messages,\n",
    "        max_retries = 2,\n",
    "        max_tokens=4000,\n",
    "    )\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9f14a7-978a-4564-aaf1-4cb3ef4e7973",
   "metadata": {},
   "source": [
    "## Report Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937d298-1a15-4861-a172-08e2c2de83ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(input) -> KnowledgeGraph:\n",
    "    \"\"\"\n",
    "    Use step-by-step logical reasoning to explain the input topic in detail.\n",
    "    Deeply explore the central propositions & principles, the pre-suppositions & axiomatic principles that lead to them, and the consequences & conclusions that result from them.\n",
    "    Iteratively recurse through the multiple chains or branches of these cause-effect representations of the causal mechanisms behind the topic.\n",
    "    Repeat this logical reasoning at each level of relationships along the multiple branches/chains until the most thorough, complete, and accurate\n",
    "    knowledge graph you can conceive of has been built.\n",
    "    Respond, ultimately, with a detailed knowledge graph of nodes, representing the concepts, and edges, capturing the relationships between the concepts.\n",
    "    \"\"\"\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-preview\", #\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Help me understand the following by describing it as a detailed knowledge graph: {input}\",\n",
    "            }\n",
    "        ],\n",
    "        response_model=KnowledgeGraph,\n",
    "    )  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9694d9d-daa7-4c27-8f78-be9cacea3d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def visualize_knowledge_graph(kg: KnowledgeGraph,filename_stem='knowledge_graph',aspect_ratio: str ='auto',flip=False):\n",
    "    if flip:\n",
    "        dot = Digraph(comment=\"Knowledge Graph\", graph_attr={'rankdir': 'BT','ratio': aspect_ratio})\n",
    "    else:\n",
    "         dot = Digraph(comment=\"Knowledge Graph\", graph_attr={'rankdir': 'BT','ratio': aspect_ratio})\n",
    "\n",
    "    # Add nodes\n",
    "    for node in kg.nodes:\n",
    "        dot.node(str(node.id), node.label, color=node.color)\n",
    "\n",
    "    # Add edges\n",
    "    for edge in kg.edges:\n",
    "        dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n",
    "\n",
    "    # Render the graph\n",
    "    dot.render(f\"{filename_stem}_gv\", view=False)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2e2a7-bb21-4a6d-8c6a-ada76c1ae1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def convert_to_graph(plan: QueryPlan,proposition: str,label_wrap=20,label_edges=False) -> KnowledgeGraph:\n",
    "    \"\"\"\n",
    "    Convert a QueryPlan object into an equivalent KnowledgeGraph object.\n",
    "    \"\"\"\n",
    "    \n",
    "    kb_node_ids = {}\n",
    "    def create_kb_node(kb_id,qp_id,query_node):\n",
    "        \"\"\"\n",
    "        Create a new Node object for the knowledge graph from the QueryNode object from the query plan.\n",
    "        kb_id is the new node id; qp_id is the given query node id within the query plan.\n",
    "        \"\"\"\n",
    "        kb_node_ids[qp_id] = kb_id\n",
    "        if len(query_node.search_query) == 0:\n",
    "            color='black'\n",
    "            description = 'proposition'\n",
    "        else:\n",
    "            color='red'\n",
    "            description = f'search query: \"{query_node.search_query}\"'\n",
    "\n",
    "        node_label = textwrap.fill(query_node.question,width=label_wrap).replace('\\n','\\\\n')\n",
    "        kb_node = Node(id=kb_id,label=node_label,description=description,color=color)\n",
    "        return kb_node\n",
    "\n",
    "    # Create a dictionary so that we can retrieve query nodes by their query plan id\n",
    "    queries = {q.id: q for q in plan.query_graph}\n",
    "\n",
    "    # Optionally, set edge labels to 'depends on'\n",
    "    if label_edges:\n",
    "        edge_label = 'depends on'\n",
    "    else:\n",
    "        edge_label = ''\n",
    "\n",
    "    # Initialize tracking of knowledge graph nodes and edges\n",
    "    kb_node_id = 0\n",
    "    kb_edge_id = 0\n",
    "    kb_nodes = []\n",
    "    kb_edges = []\n",
    "\n",
    "    # Convert each query node to a knowledge graph node.\n",
    "    for q_id,q in queries.items():\n",
    "        q_id = q.id\n",
    "        q_kb_id = kb_node_ids.get(q_id,None)\n",
    "        if q_kb_id is None: # if not seen this node yet, create a knowledge graph node\n",
    "            kb_node_id += 1\n",
    "            q_kb_id = kb_node_id\n",
    "            q_kb_node = create_kb_node(q_kb_id, q_id, q)\n",
    "            kb_nodes.append(q_kb_node)\n",
    "\n",
    "        # Add an edge to the knowledge graph for each of the dependencies of the query node\n",
    "        if q.dependencies is not None:\n",
    "            for d_id in q.dependencies:\n",
    "                d_kb_id = kb_node_ids.get(d_id,None)\n",
    "                if d_kb_id is None: # if not seen this node yet, create a knowledge graph node\n",
    "                    kb_node_id += 1\n",
    "                    d_kb_id = kb_node_id\n",
    "                    d_kb_node = create_kb_node(d_kb_id, d_id, queries[d_id])\n",
    "                    kb_nodes.append(d_kb_node)\n",
    "                # Create the new edge from the dependency to the query node\n",
    "                d_edge = Edge(target=q_kb_id,source=d_kb_id,label=edge_label,description='')\n",
    "                kb_edges.append(d_edge)\n",
    "                \n",
    "    # Create the knowledge graph from the nodes and edges\n",
    "    kb = KnowledgeGraph(topic=proposition,nodes=kb_nodes,edges=kb_edges)\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051f964-d8ca-49d3-9701-017574051d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_report(computed_query,search_dict={}) -> str:\n",
    "    \"\"\"\n",
    "    Create the text of an R Markdown file (i.e., markdown plus YAML header) representing a report\n",
    "    of the results of the `computed_query` and incorporating the search results, if any, given by\n",
    "    `search_dict`.\n",
    "    \"\"\"\n",
    "    \n",
    "    query_with_context = computed_query.query\n",
    "    response = computed_query.response\n",
    "    query = re.sub(r\"(.+)\\nContext: ##`.+?`##\",r\"\\1\",query_with_context,flags=re.DOTALL)\n",
    "    query = re.sub(r\"Is '(.+)' the truth\\?\",r\"\\1\",query,flags=re.DOTALL)\n",
    "    context = eval(re.sub(r\".+Context: ##`(.+?)`##\",r\"\\1\",query_with_context,flags=re.DOTALL))['responses']\n",
    "\n",
    "    section_text = \"# Sub-Propositions\\n\\n\"\n",
    "    for r in context:\n",
    "        section_text += f\"## {r['query']}\\n\\n{r['response']}\\n\\n\"\n",
    "        \n",
    "    markdown_text = f\"\"\"---\n",
    "title: \"Proposition: '{query}'\"\n",
    "author: \"GPT-4, with Michael L. Thompson\"\n",
    "date: \"`r Sys.Date()`\"\n",
    "output:\n",
    "  pdf_document:\n",
    "    toc: yes\n",
    "    toc_depth: 4\n",
    "linkcolor: red\n",
    "urlcolor: blue\n",
    "---\n",
    "\n",
    "```{{r setup, include=FALSE}}\n",
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "```\n",
    "\\\\newpage\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    markdown_text += f\"# Proposition: \\\"{query}\\\"\\n\\n{section_text}\\n\\n# Conclusion\\n\\n{response}\"\n",
    "    \n",
    "    if len(search_dict)>0:\n",
    "        search_text = \"\\n\\n# Search Results\\n\\n\"\n",
    "        for k,search_result in search_dict.items():\n",
    "            if len(search_result)>0:\n",
    "                search_text += f\"## \\\"{k}\\\"\\n\\n{search_result}\\n\\n\"\n",
    "        markdown_text += search_text\n",
    "        \n",
    "    return markdown_text\n",
    "\n",
    "#print(make_report(computed_query = plan_college2a_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0243f-904d-429f-bc86-5d1148343a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz\n",
    "\n",
    "def outline_processing(plan: QueryPlan, verbose: bool = False):\n",
    "    return [q.id for q in plan.query_graph]\n",
    "\n",
    "def outline_processing0(plan: QueryPlan, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Sort the id's of the plan.query_graph Query objects according to their logical order as dictated by the plan.query_outline.\n",
    "    Returns a list of the re-ordered id's of the associated Query object in the plan query_graph.\n",
    "    \"\"\"\n",
    "\n",
    "    #plan = assessment_explain_wt_evidence['plan']\n",
    "    # strip off the context info associated with each query.\n",
    "    q_dict = {q.id: q.question for q in plan.query_graph}\n",
    "    outline = plan.query_plan_outline.split('\\n')\n",
    "    \n",
    "    # Split the plan query_plan_outline into separate lines and process each one.\n",
    "    res = {}\n",
    "    best_score = {}\n",
    "    for id,qry in q_dict.items():\n",
    "        #print((id,qry))\n",
    "        # Use similarity scoring between the outline's line and the query nodes' queries, ignoring order of words.\n",
    "        scores = [ fuzz.token_sort_ratio(qry, line) for line in outline ]\n",
    "        #print(scores)\n",
    "        # Find the highest scoring query\n",
    "        max_score = max(scores)\n",
    "        #if max_score > 60:\n",
    "        i_best_match = scores.index(max_score)\n",
    "        res[id] = i_best_match\n",
    "        best_score[id] = max_score\n",
    "        #else:\n",
    "            #res[id] = None\n",
    "            #best_score[id] = None\n",
    "    \n",
    "    if verbose:\n",
    "        i_lines_matched = list(res.values())\n",
    "        for i,line in enumerate(outline):\n",
    "            if i in i_lines_matched:\n",
    "                id = list(res.keys())[i_lines_matched.index(i)]\n",
    "                print(f\"\\\"{line}\\\" == \\\"{q_dict[id]}\\\" -- [{id}; {best_score[id]}]\")\n",
    "            else:\n",
    "                print(f\"\\\"{line}\\\" == ****\")\n",
    "                \n",
    "    #print(res)\n",
    "    nmax = len(plan.query_graph)\n",
    "    node_dict = {q.id: q for q in plan.query_graph}\n",
    "    res_frac = {id: (i + 0.1*(1 - len(node_dict[id].dependencies)/nmax)) for id,i in res.items()}\n",
    "    #print(res_frac)\n",
    "    \n",
    "    num_list = list(res_frac.values())\n",
    "    # Sorting the list and getting indices\n",
    "    ordering = [index for index, value in sorted(enumerate(num_list), key=lambda x: x[1])]\n",
    "    \n",
    "    # [num_list[o] for o in ordering] will now give the sorted list\n",
    "    sorted_list = [num_list[o] for o in ordering]\n",
    "    #print(ordering)  # This will print the indices in sorted order\n",
    "    #print(sorted_list)  # This will print the sorted values\n",
    "    res_frac_sorted = {id: round(v,2) for id,v in zip([ list(res_frac.keys())[o] for o in ordering], sorted_list)}\n",
    "    #print(res_frac_sorted)\n",
    "    \n",
    "    if verbose:\n",
    "        for id in list(res_frac_sorted.keys()):\n",
    "            print(node_dict[id].question)\n",
    "\n",
    "    return list(res_frac_sorted.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87535cde-b4b1-4f20-a5fe-80973885f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outline_processing(assessment_explain_wt_evidence['plan'],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f154de70-4374-4ff9-9ea3-c5e7b78777f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_deps(id,node_dict,lvl=0):\n",
    "    node = node_dict[id]\n",
    "    print((lvl*'\\t') + f'{node.question} -- [{node.id}]')\n",
    "    for did in node.dependencies:\n",
    "        list_deps(did,node_dict,lvl+1)\n",
    "\n",
    "#id_sorted = outline_processing(assessment_explain_wt_evidence['plan'],verbose=False)\n",
    "#print(id_sorted)\n",
    "#list_deps('main',{q.id: q for q in assessment_explain_wt_evidence['plan'].query_graph})\n",
    "##print(response_outline(task_id='main',computed_queries={id: assessment_explain_wt_evidence['plan_result'][id] for id in id_sorted,level=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c85b13-ee98-44ce-880b-89a1960657ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(plan_civic2_result.keys()))\n",
    "\n",
    "def response_outline(task_id,computed_queries: dict,dependencies, level=0, done_already=[],verbose=False):\n",
    "    computed_query = computed_queries[task_id]\n",
    "    query_with_context = computed_query.query\n",
    "    response = computed_query.response\n",
    "    query = re.sub(r\"(.+)\\nContext: ##`.+`##\",r\"\\1\",query_with_context,flags=re.DOTALL)\n",
    "    query = re.sub(r\"Is '(.+)' the truth\\?\",r\"\\1\",query)\n",
    "    if verbose:\n",
    "        print(\"report: query = \" + query )\n",
    "    #print(query_with_context)\n",
    "    \n",
    "    if ('Context' in query_with_context):\n",
    "        # As set by the Query.aexecute() treatment of context, which may be only merged_responses or may be both merged_responses & search_results.\n",
    "        # We ignore search_results because they're handled by a separate 'Searches' section in the report.\n",
    "        dict_str = re.sub(r\".*Context: ##`(.+)`##\",r\"\\1\",query_with_context,flags=re.DOTALL) #.replace('\\\\\"','\"')\n",
    "        try:\n",
    "            full_context = eval(dict_str)\n",
    "        except:\n",
    "            dict_str2 = re.sub('`##\".+','',dict_str,flags=re.DOTALL)\n",
    "            dict_str2 = dict_str2.replace('\\\\\\\"','\\\"').replace('\\\\\\n','\\n')\n",
    "            dict_str2 = f\"\"\"{dict_str2}\"\"\"\n",
    "            full_context = eval(dict_str2)\n",
    "\n",
    "        if isinstance(full_context,list):\n",
    "            #print(full_context)\n",
    "            search_results = None\n",
    "            full_context = {}\n",
    "        else:\n",
    "            search_results = full_context.get('search_results',None) # check if merged_responses & search_results in context\n",
    "            \n",
    "        if search_results is None: # no search_results, implies only merged_responses\n",
    "            merged_responses = full_context\n",
    "        else: # must get merged_responses from full_context\n",
    "            merged_responses = full_context.get('merged_responses',None)\n",
    "            \n",
    "        context = merged_responses.get('responses',None) # just to be sure, see if empty\n",
    "    else:\n",
    "        context=None\n",
    "\n",
    "    hashes = \"#\"*(level+1)\n",
    "    if context is None:\n",
    "        markdown_text = f\"{hashes} \\\"{query}\\\"\\n\\n{response}\"\n",
    "    else: # When context is given, this means that the node had sub-propositions that we need to report\n",
    "        if level==0:\n",
    "            section_text = hashes + \" Sub-Propositions\\n\\n\"\n",
    "        else:\n",
    "            section_text = '\\n\\n'\n",
    "\n",
    "        if verbose:\n",
    "            print('\\t\\tContext:')\n",
    "            for r in context:\n",
    "                print('\\t'*3 + r['task_id'])\n",
    "                \n",
    "        context_dict = {r['task_id']: r for r in context if r['task_id'] not in done_already }\n",
    "        context_dict = {id: context_dict[id] for id,cq in computed_queries.items() if id in context_dict}\n",
    "        for r in list(context_dict.values()):\n",
    "            #print(r)\n",
    "            # Recurse: treat the sub-proposition, handling sub-sub-propositions, and so on...\n",
    "            section_text += response_outline(r['task_id'],computed_queries,dependencies,level=level+1,done_already=done_already,verbose=verbose) + \"\\n\\n\" #f\"## {r['query']}\\n\\n{r['response']}\\n\\n\"\n",
    "            done_already.append(r['task_id'])\n",
    "            done_already.extend(dependencies[r['task_id']])\n",
    "        if level==0:\n",
    "            markdown_text = f\"{hashes} Proposition: \\\"{query}\\\"\\n\\n{hashes} Summary\\n\\n{response}\\n\\n{section_text}\"\n",
    "        else:\n",
    "            markdown_text = f\"{hashes} {query}\\n\\n{section_text}\\n\\n{hashes} Conclusion\\n\\n{response}\"\n",
    "    \n",
    "    return markdown_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc659c-dc80-413f-a3fa-88be8914e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_report(plan_outline,lastnode,computed_queries,dependencies,search_dict, filename_stem, added_instructions=None, return_Rmd=False, write_to_file=True):\n",
    "    #lastnode = (list(plan_2020election_result2_gpt4.values())[-1])\n",
    "\n",
    "    #report_markdown_gpt4a = response_outline(task_id=lastnode.task_id,computed_queries=plan_2020election_result2_gpt4,level=0)\n",
    "    report_markdown = response_outline(task_id=lastnode.task_id,computed_queries=computed_queries,dependencies=dependencies,level=0,done_already=[])\n",
    "    figure_txt = f'\\n\\n![Query plan graph.]({filename_stem}_gv.pdf)\\n\\n'\n",
    "    #print(report_markdown)\n",
    "    \n",
    "    if added_instructions is None:\n",
    "        added_instructions_section = ''\n",
    "    else:\n",
    "        added_instructions_section = f\"\"\"\n",
    "        \n",
    "## Added Instructions\n",
    "\n",
    "{added_instructions}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    plan_outline = re.sub(r'(([0-9]+|[A-Za-z]+)\\.)+ ','* ',plan_outline,flags=re.DOTALL)\n",
    "\n",
    "    report_markdown = re.sub(r'(# Proposition: \"[^\"]+\"\\n)',r'\\1'+ figure_txt + added_instructions_section + '## Query Plan Outline\\n\\n'+plan_outline+'\\n\\n',report_markdown)\n",
    "    print('='*150)\n",
    "    #print(report_markdown)\n",
    "    \n",
    "    query = re.sub(r\"(.+)\\nContext: ##`.+?`##\",r\"\\1\",lastnode.query,flags=re.DOTALL)\n",
    "    query = re.sub(r\"Is '(.+)' the truth\\?\",r\"\\1\",query,flags=re.DOTALL)\n",
    "\n",
    "    \n",
    "    mrkdwn = f\"\"\"---\n",
    "title: \"Proposition: '{query}'\"\n",
    "author: \"GPT-4, with Michael L. Thompson\"\n",
    "date: \"`r Sys.Date()`\"\n",
    "output:\n",
    "  pdf_document:\n",
    "    toc: yes\n",
    "    toc_depth: 4\n",
    "linkcolor: red\n",
    "urlcolor: blue\n",
    "---\n",
    "\n",
    "\n",
    "^[Proposition: '{query}' [{filename_stem}.pdf] Â© 2024 by Michael L. Thompson is licensed under CC BY-SA 4.0. To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/4.0/]\n",
    "\n",
    "\\\\newpage\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    how_done_section = f\"\"\"\n",
    "\n",
    "# How Was This Done? \n",
    "\n",
    "Generated by [Michael L. Thompson](https://www.linkedin.com/in/mlthomps) with OpenAI's GPT-4 (`gpt-4-turbo-preview`).\n",
    "\n",
    "* **Python-based Jupyter Lab Notebook:** The contents of this document were generated through coded prompting of **OpenAI's GPT-4** using the Python package **[`instructor`](https://github.com/jxnl/instructor)** by Jason Liu.\n",
    "  - Input: the text string \"{query}\".\n",
    "  - Output: this report as an R Markdown text file.\n",
    "\n",
    "* **GPT-4 Plan Generation:** GPT-4 autonomously generated the query plan (using code based off of an `instructor` example). \n",
    "\n",
    "* **GPT-4 Plan Execution:** GPT-4 then autonomously executed the plan, which involved the following:\n",
    "  - writing search queries and initiating calls to **[Serper (Google Search API)](https://serper.dev/)** for searching, \n",
    "  - generating responses to the questions in the plan using the searches as context, and \n",
    "  - creating all text in the Outline, Summary, and Sub-Propositions sections. \n",
    "  \n",
    "* **Python Report Generation:** The Python code then consolidated GPT-4 responses into an R Markdown text file. \n",
    "\n",
    "* **Manual Rendering:** Manually, this section was written, the Query Plan Outline was re-formatted, and, via **RStudio**, the PDF document was rendered.  A few miscellaneous manual formatting corrections were also done. No other content was manually generated or edited.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    if len(search_dict)>0:\n",
    "        search_text = \"\\n\\n# Search Results\\n\\n\"\n",
    "        for k,search_result in search_dict.items():\n",
    "            if len(search_result)>0:\n",
    "                search_text += f\"## \\\"{k}\\\"\\n\\n{search_result}\\n\\n\"\n",
    "        report_markdown += search_text\n",
    "\n",
    "    report_markdown += how_done_section\n",
    "    \n",
    "    if write_to_file:\n",
    "        mrkdwn_file_path = f'{filename_stem}.Rmd'\n",
    "        with open(mrkdwn_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(mrkdwn + to_ascii(report_markdown))\n",
    "\n",
    "    if return_Rmd:\n",
    "        report_markdown = mrkdwn + to_ascii(report_markdown)\n",
    "\n",
    "    return report_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac929b-cb88-4ebb-a230-9b28c14ada79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from graphviz import Digraph\n",
    "\n",
    "def pickle_it(assessment_results: dict, save = False) -> dict:\n",
    "    \"\"\"\n",
    "    Save to disk or load from disk a full proposition assessment analysis, as generated by function `assess_proposition()`,\n",
    "    based upon the `short_name` saved in the `assessment_results` dictionary, which must contain a key name `short_name`.\n",
    "    If `save` is True, then saves `assessment_results` to disk with name `f'results_{short_name}.pkl'` -- overwriting it if it already exists -- and returns `assessment_results` as passed in.\n",
    "    Else, ignores what was passed in as `assessment_results`, and \n",
    "    if file of that name exists in current directory, loads and returns the dictionary from that file; else returns None.\n",
    "    \"\"\"\n",
    "      \n",
    "    short_name = assessment_results['short_name']\n",
    "    results_filename = f'results_{short_name}.pkl'\n",
    "    file_path = './' + results_filename\n",
    "    \n",
    "    if save:\n",
    "        results = {}\n",
    "        results.update(assessment_results)\n",
    "        dump_dict = {}\n",
    "        dump_dict.update(assessment_results)\n",
    "        \n",
    "        plan_result = assessment_results.get('plan_result',{}) # a dictionary of task_id strings as keys and ComputeQueryGPT4 as values\n",
    "        if plan_result:\n",
    "            serialized_plan_result = {task_id: value.model_dump() for task_id,value in plan_result.items() if isinstance(value,BaseModel)}\n",
    "        else:\n",
    "            serialized_plan_result = None\n",
    "        dump_dict['plan_result'] = serialized_plan_result\n",
    "\n",
    "        dump_dict['kb_dot'] = None # Drop the Digraph object\n",
    "        \n",
    "        # get a JSON representation\n",
    "        # Convert Pydantic models to dictionaries and serialize the entire dictionary\n",
    "        serialized_dict = {\n",
    "            key: value.model_dump() if isinstance(value, BaseModel) else value\n",
    "            for key, value in dump_dict.items()\n",
    "        }\n",
    "        \n",
    "        # Dump the serialized dictionary to a JSON string\n",
    "        json_string = json.dumps(serialized_dict)\n",
    "        \n",
    "        # dump to file\n",
    "        with open(file_path, 'wb') as state_file:\n",
    "            pickle.dump(json_string, state_file)\n",
    "    else:\n",
    "        exist_file = os.path.isfile(file_path)\n",
    "        if exist_file:\n",
    "            # get from file\n",
    "            with open(file_path, 'rb') as state_file:\n",
    "                json_string = pickle.load(state_file)\n",
    "            # Convert the JSON string back into a dictionary\n",
    "            parsed_dict = json.loads(json_string)\n",
    "            \n",
    "            results = parsed_dict\n",
    "            # should have form: {'short_name': short_name, 'plan': plan, 'plan_result': plan_result, 'search_dict': search_dict,'kb_graph': graph, 'kb_dot': kb_dot, 'report': report_rmarkdown, 'model': model }\n",
    "            # Reconstruct the Pydantic models\n",
    "            results['plan'] = QueryPlan(**parsed_dict['plan'])\n",
    "            plan0 = parsed_dict.get('plan0')\n",
    "            if plan0:\n",
    "                results['plan0'] = plan0\n",
    "            plan_result = parsed_dict.get('plan_result')\n",
    "            if plan_result:\n",
    "                results['plan_result'] = { task_id: ComputeQuery(**compute_query_dict) for task_id,compute_query_dict in plan_result.items() }\n",
    "\n",
    "            kb_graph = KnowledgeGraph(**parsed_dict['kb_graph'])\n",
    "            results['kb_dot'] = visualize_knowledge_graph(kb_graph,filename_stem=short_name,flip=True, aspect_ratio= 'auto')\n",
    "        else:\n",
    "            results = None\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0312c3-e8a3-4b71-bcf8-1b368b88b477",
   "metadata": {},
   "source": [
    "## Proposition Assessment Functions\n",
    "\n",
    "\"Snout to tail....\" -- this section takes the entirety of the proposition assessment code and wraps it into a single function `assess_proposition()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf71c9e-7c43-4ce6-9bf1-3aac1cec2fb4",
   "metadata": {},
   "source": [
    "### Definition of function `assess_proposition2()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3364794-316b-4723-86ad-807c30b8c0a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "async def assess_proposition2(proposition: str, short_name: str, search_cache: dict,client, use_plan: QueryPlan = None,plan_only: bool = False, added_instructions: str = '', \n",
    "                              model: str =\"gpt-3.5-turbo\",improvement_round=False, report_to_file: bool = False,kb_label_wrap: int = 20, force_run: bool = False, save_pickle: bool = True,status_updates=False,print_level: int = 0):\n",
    "    \"\"\"\n",
    "    Performs an assessment of the proposition using logical reasoning and evidence evaluation.\n",
    "    Is asynchronous, so must execute with the `await` keyword.\n",
    "    If `use_plan` is given, then it `proposition` must agree with whatever proposition is specified within the given query plan as `use_plan` for the results to make sense.\n",
    "    \n",
    "    Usage example:\n",
    "    assessment_tesla_y =  await assess_proposition(\n",
    "        proposition = \"Tesla Model Y is the best electric vehicle currently available.\", \n",
    "        short_name = 'tesla_y', \n",
    "        search_cache = search_cache, \n",
    "        use_plan = None,\n",
    "        added_instructions = '', \n",
    "        model =\"gpt-3.5-turbo\", # 'gpt-4-turbo-preview'\n",
    "        report_to_file = False,\n",
    "        kb_label_wrap = 20,\n",
    "        force_run = False,\n",
    "        save_pickle = True\n",
    "        \n",
    "    )\n",
    "    # The results are dictionary with all of the work generated.\n",
    "    print(list(assessment_tesla_y.keys()))\n",
    "    # A report is generated in R Markdown format and returned -- it is also saved to file if `write_to_file` is True.\n",
    "    print(assessment_tesla_y['report'])\n",
    "    \"\"\"\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "    plan0 = None\n",
    "    if print_level > 0:\n",
    "        print(\"\\n\\n================================= *** BEGIN *** =================================\\n\")\n",
    "    \n",
    "    if force_run:\n",
    "        assessment_results = None\n",
    "        search_dict = None\n",
    "        report_rmarkdown = None\n",
    "        plan_result = None\n",
    "    else:\n",
    "        assessment_results = pickle_it(assessment_results = {'short_name': short_name}, save = False)\n",
    "        if assessment_results is None:\n",
    "            if print_level > 1:\n",
    "                print(f\"*** NO RESULTS FOUND ('results_{short_name}.pkl' not found) -- WILL GENERATE RESULTS ***\\n\")\n",
    "            assessment_results = {'short_name': short_name}\n",
    "        else:\n",
    "            if print_level > 1:\n",
    "                print(f\"*** RETRIEVED RESULTS FROM 'results_{short_name}.pkl' ***\\n\")\n",
    "            \n",
    "        use_plan = assessment_results.get('plan',use_plan)\n",
    "        plan_result = assessment_results.get('plan_result',None)\n",
    "        if plan_result is not None: # make sure model is same as from saved results rather than input argument\n",
    "            model = assessment_results.get('model',None)\n",
    "        search_dict = assessment_results.get('search_dict',None)\n",
    "        report_rmarkdown = assessment_results.get('report_rmarkdown',None)\n",
    "        \n",
    "    if use_plan is None: # No plan given and none retrieved from results so erase all results\n",
    "        plan_result = None\n",
    "        search_dict = None\n",
    "        report_markdown = None            \n",
    "    \n",
    "        key_question = f\"\"\"# Mandate:\n",
    "Given proposition '{proposition}'. \n",
    "1. Enumerate all sub-propositions, and their sub-propositions and so on, that need to be true to support it.\n",
    "2. Enumerate all factual axiomatic statements as evidence that support or refute each and every one of the propositions. \n",
    "3. Record these propositions and evidence as a textual outline (in 'query_plan_outline') describing the query plan.\n",
    "3. Generate the query plan as a tree of query nodes (in 'query_graph').\n",
    "   - Assign a separate query node for each specific sub-proposition.\n",
    "   - Assign a separate query node for each specific piece of evidence needed to confirm and refute each sub-proposition. \n",
    "   - Be sure to specify search_flag=True for a query node when needed to gather the latest factual information as evidence.\n",
    "   - Demand that actual specific data and statistics are gathered as evidence and that the answers to the propositions are stated in terms of these data.\n",
    "\"\"\"\n",
    "        mandate = f\"\"\"\n",
    "# The Proposition for which we require a Query Plan:\n",
    "'{proposition}'\"\"\"\n",
    "    \n",
    "        # Generate the query plan (always uses gpt-4-turbo-preview)\n",
    "        if len(added_instructions)>0:\n",
    "            added_text = f'\\n\\nAdditional Instructions: {added_instructions}\\n\\n'\n",
    "        else:\n",
    "            added_text = ''\n",
    "            \n",
    "        if print_level > 0:\n",
    "            print(\"*** GENERATING PLAN ***\")\n",
    "        plan = query_planner( key_question + f'{added_text}' + mandate, client )\n",
    "        if improvement_round:\n",
    "            if print_level > 0:\n",
    "                print(\"*** IMPROVING  PLAN ***\")\n",
    "            plan0 = plan\n",
    "            plan = improve_upon_plan(plan)\n",
    "        else:\n",
    "            plan0 = None\n",
    "    else:\n",
    "        plan = use_plan\n",
    "        dids = []\n",
    "        for q in plan.query_graph:\n",
    "            dids.extend(q.dependencies)\n",
    "        top_q = [q for q in plan.query_graph if q.id not in dids]\n",
    "        provided_prop = re.sub(r\"Is '(.+)' the truth\",r\"\\1\",top_q[0].question)\n",
    "        if print_level > 0:\n",
    "            print(f\"Given `proposition` = '{proposition}'.\\nProvided proposition from `use_plan` = '{provided_prop}'.\\n\")\n",
    "    \n",
    "    if print_level > 2:\n",
    "        pprint(plan.model_dump())\n",
    "\n",
    "    # Visualize the query graph -- always saves it to disc as a PDF\n",
    "    graph: KnowledgeGraph = convert_to_graph(plan,proposition,label_wrap=kb_label_wrap)\n",
    "    kb_dot = visualize_knowledge_graph(graph,filename_stem=short_name,flip=True, aspect_ratio= 'auto')\n",
    "\n",
    "    # added this because without it sometimes errors later on and fails without saving any results!\n",
    "    assessment_results = {'short_name': short_name, 'plan': plan,'plan0': plan0, 'search_dict': search_dict,'kb_graph': graph, 'kb_dot': kb_dot, 'report': report_rmarkdown, 'model': model }\n",
    "    if save_pickle:\n",
    "        _ = pickle_it(assessment_results = assessment_results, save = True)\n",
    "        if print_level > 1:\n",
    "            print(f\"... saving results (plan & kb graph) to 'results_{short_name}.pkl' ...\")\n",
    "\n",
    "    if plan_only:\n",
    "        # Just skip generating results or returning retrieved results, if plan_only is True.\n",
    "        plan_result = None\n",
    "        search_dict = None\n",
    "        report_markdown = None    \n",
    "    else:\n",
    "        if plan_result is None:\n",
    "            # Execute the plan if the results aren't already gotten from file.\n",
    "            if search_dict is not None:\n",
    "                searche_cache.update(search_dict)\n",
    "                \n",
    "            if print_level > 0:\n",
    "                print(\"*** EXECUTING PLAN ***\")\n",
    "            # Execute the plan using the LLM given by `model`\n",
    "            plan_result = await plan.execute_dynamic(client=client,model=model,status_updates=status_updates,print_level=print_level)\n",
    "    \n",
    "        \n",
    "        last_node_result = list(plan_result.values())[-1]\n",
    "        # added this because without it sometimes errors occur during report generation and fails without saving any results!\n",
    "        assessment_results = {'short_name': short_name, 'plan': plan,'plan0': plan0, 'plan_result': plan_result, 'kb_graph': graph, 'kb_dot': kb_dot,  'model': model }\n",
    "        if save_pickle:\n",
    "            _ = pickle_it(assessment_results = assessment_results, save = True)\n",
    "            if print_level > 1:\n",
    "                print(f\"... saving results (without search dictionary and report) to 'results_{short_name}.pkl' ...\")\n",
    "            \n",
    "        if print_level > 0:\n",
    "            # Display the top-level query response\n",
    "            print(\"\\n\\n================================= *** RESULTS *** =================================\\n\")\n",
    "            print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',last_node_result.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "            print('RESPONSE: ' + last_node_result.response)\n",
    "    \n",
    "        if search_dict is None:\n",
    "            # Gather up the results of all searches performed\n",
    "            #search_keys = [q.search_query for q in plan.query_graph if len(q.search_query)>0]\n",
    "            if plan.search_query_for_context:\n",
    "                try:\n",
    "                    search_dict = { plan.search_query_for_context : summarize_raw_search(eval(plan.context)) }\n",
    "                except:\n",
    "                    search_dict = {}\n",
    "                    warnings.warn(\"Could not get `plan.context`. Will not include it in search results.\", UserWarning)\n",
    "            else:\n",
    "                search_dict = {}\n",
    "            search_keys = [search_query for q in plan.query_graph for search_query in q.search_query]\n",
    "            if print_level > 1:\n",
    "                print(search_keys)\n",
    "            search_dict.update( {key: summarize_raw_search(eval(search_cache[key])) for key in search_keys if search_cache.get(key,None) is not None} )\n",
    "\n",
    "        # added this because without it sometimes errors occur during report generation and fails without saving any results!\n",
    "        assessment_results = {'short_name': short_name, 'plan': plan,'plan0': plan0, 'plan_result': plan_result, 'search_dict': search_dict,'kb_graph': graph, 'kb_dot': kb_dot, 'report': report_rmarkdown, 'model': model }\n",
    "        if save_pickle:\n",
    "            _ = pickle_it(assessment_results = assessment_results, save = True)\n",
    "            if print_level > 1:\n",
    "                print(f\"... saving results (without report) to 'results_{short_name}.pkl' ...\")\n",
    "            \n",
    "        if report_rmarkdown is None:\n",
    "            if print_level > 0:\n",
    "                print(\"\\n*** GENERATING REPORT ***\")\n",
    "            # Generate the report as an R Markdown (.Rmd) document\n",
    "            dependencies = {q.id: q.dependencies for q in plan.query_graph}\n",
    "            try:\n",
    "                # Put the computed queries of the plan_result in the same order that their questions appear in the plan's query outline text.\n",
    "                node_ids =  outline_processing(plan)\n",
    "                computed_queries = {id: plan_result[id] for id in node_ids}\n",
    "\n",
    "                report_rmarkdown = generate_report(\n",
    "                    plan_outline = plan.query_plan_outline,\n",
    "                    lastnode=last_node_result,\n",
    "                    computed_queries=computed_queries,                    \n",
    "                    dependencies = dependencies,\n",
    "                    search_dict=search_dict, \n",
    "                    filename_stem=short_name, \n",
    "                    write_to_file=report_to_file,\n",
    "                    added_instructions = added_instructions if len(added_instructions)>0 else None\n",
    "                )\n",
    "            except:\n",
    "                warnings.warn(\"Could not generate report. Results will have report_markdown = None.\", UserWarning)\n",
    "\n",
    "\n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    assessment_results = {'short_name': short_name, 'plan': plan,'plan0': plan0, 'plan_result': plan_result, 'search_dict': search_dict,'kb_graph': graph, 'kb_dot': kb_dot, 'report': report_rmarkdown, 'model': model, \n",
    "                          'start_time': start_time, 'elapsed_time': elapsed_time }\n",
    "\n",
    "    # Ahh, relief. Got a valid report. Save all results with report.\n",
    "    if save_pickle:\n",
    "        _ = pickle_it(assessment_results = assessment_results, save = True)\n",
    "        if print_level > 1:\n",
    "            print(f\"*** RESULTS SAVED TO 'results_{short_name}.pkl' ***\")\n",
    "        \n",
    "    # Return all the results\n",
    "    if print_level > 0:\n",
    "        print(\"\\n\\n================================= *** DONE *** =================================\\n\")\n",
    "\n",
    "    return assessment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429afc5-fdb7-4dfc-b2c7-f4add10ec93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(assmnt):\n",
    "    summary_response = list(assmnt['plan_result'].values())[-1].response\n",
    "    print(f\"\"\"**PROPOSITION {assmnt['short_name']} **\\n**\"{assmnt['plan'].query_graph[0].question}\"**\\n**Elapsed time: {assmnt['elapsed_time']} sec**\"\"\")\n",
    "    print(f\"\\n**SEARCH FOR CONTEXT:**\\n{assmnt['plan'].search_query_for_context}\")\n",
    "    print(summarize_raw_search(eval(assmnt['plan'].context)))\n",
    "    print(f'\\n**RESPONSE:**\\n{summary_response}')\n",
    "    return(summary_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9d8ee-4161-4eab-9aa8-fbc3181ee49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report(result_dictionary,search_cache=search_cache):    \n",
    "    report_rmarkdown = generate_report(\n",
    "        plan_outline = result_dictionary['plan'].query_plan_outline,\n",
    "        lastnode=list(result_dictionary['plan_result'].values())[-1],\n",
    "        computed_queries={id: result_dictionary['plan_result'][id] for id in outline_processing(result_dictionary['plan'])},                    \n",
    "        dependencies = {q.id: q.dependencies for q in result_dictionary['plan'].query_graph},\n",
    "        search_dict= {key: summarize_raw_search(eval(search_cache[key])) for key in  [search_query for q in result_dictionary['plan'].query_graph for search_query in q.search_query] if search_cache.get(key,None) is not None}, \n",
    "        filename_stem=result_dictionary['short_name'], \n",
    "        write_to_file=True,\n",
    "        added_instructions = None\n",
    "    )\n",
    "    return report_rmarkdown\n",
    "\n",
    "#rpt = get_report(assessment_brady_qb_new)\n",
    "\n",
    "#print(rpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7076bde-7f82-41ea-8c89-a2737da6250e",
   "metadata": {},
   "source": [
    "# Proposition Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eac368-2694-4a3c-a0ce-bbac736303c5",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d67749-59f1-4e96-98de-b4f209790310",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cache_exists = ('search_cache' in globals()) | ('search_cache' in locals())\n",
    "INITIALIZE = False | (not search_cache_exists) # force it by replacing False with True \n",
    "if INITIALIZE: # Set INITIALIZE to True if want to clear search_cache\n",
    "    search_cache = {}\n",
    "\n",
    "# Begin again with a new `client` object\n",
    "client = instructor.patch(OpenAI())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c5868-10e9-44ad-947e-24ba6b87bd64",
   "metadata": {},
   "source": [
    "## Arbitrary Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184cb1e6-3adf-43f5-8db1-7033d0aea185",
   "metadata": {},
   "source": [
    "#### â€œNot once in the twentieth century, ... has a single politician, actor, athlete, or surgeon emerged as a first-rate novelist, despite the dismayingly huge breadth of experience each profession affords.â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3352c-8efb-407b-a8a2-2e3dd7d201f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_novelists = await assess_proposition2(\n",
    "    proposition = \"Not once in the twentieth century,... has a single politician, actor, athlete, or surgeon emerged as a first-rate novelist, despite the dismayingly huge breadth of experience each profession affords.\", \n",
    "    short_name = 'novelists',\n",
    "    added_instructions=\"\",\n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = True, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(visualize_knowledge_graph(convert_to_graph(assessment_novelists['plan0'],proposition='copilotkt_gamechanger')))\n",
    "display(assessment_novelists['kb_dot'])\n",
    "if assessment_novelists.get('plan_result'):\n",
    "    _ = print_summary(assessment_novelists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eecaea-5fde-45c9-9177-e7f691698eb3",
   "metadata": {},
   "source": [
    "#### CopilotKit is a game-changer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1548cb1-984b-42d5-872f-25e777fdf9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_copilotkit_gamechanger = await assess_proposition2(\n",
    "    proposition = \"CopilotKit is a unique game-changer for AI application development.\", \n",
    "    #plan_only = True,\n",
    "    short_name = 'copilotkit_gamechanger',\n",
    "    added_instructions=\"\"\"\n",
    "Accept ::GIVEN_FACTS:: as factual information that does not need to be verified by search but rather supplies clarity and resources to help in posing search queries.\n",
    "\n",
    "::GIVEN_FACTS::\n",
    "\n",
    "* CopilotKit is 'The Open-Source Copilot Platform'.\n",
    "* CopilotKit is available for download at GitHub repository https://github.com/CopilotKit/CopilotKit .\n",
    "* Detailed documentation of CopilotKit features and usage is available at https://docs.copilotkit.ai/getting-started/quickstart-textarea  .\n",
    "\n",
    "\"\"\",\n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = True, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(visualize_knowledge_graph(convert_to_graph(assessment_copilotkit_gamechanger['plan0'],proposition='copilotkt_gamechanger')))\n",
    "display(assessment_copilotkit_gamechanger['kb_dot'])\n",
    "if assessment_copilotkit_gamechanger.get('plan_result'):\n",
    "    _ = print_summary(assessment_copilotkit_gamechanger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fba721-369a-4f0c-ac05-71e5aa8e693d",
   "metadata": {},
   "source": [
    "#### Altman was prophetic (\"Moore's Law for Everything\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efb663-ae81-42f4-80bf-d2abcdc897d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_altman_prophetic = await assess_proposition2(\n",
    "    proposition = \"Sam Altman's essay [*Moore's Law for Everything* (March 16, 2021)](https://moores.samaltman.com/) was prophetic.\", \n",
    "    #plan_only = True,\n",
    "    short_name = 'altman_prophetic',\n",
    "    added_instructions=\"\",\n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "#display(visualize_knowledge_graph(convert_to_graph(assessment_altman_prophetic['plan0'],proposition='Prophetic')))\n",
    "display(assessment_altman_prophetic['kb_dot'])\n",
    "if assessment_altman_prophetic.get('plan_result'):\n",
    "    _ = print_summary(assessment_altman_prophetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632a81a-87e3-4b3f-96be-0ab71ed754d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(visualize_knowledge_graph(convert_to_graph(assessment_altman_prophetic['plan0'],proposition='Prophetic')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aacb5c-298b-48a2-8f13-2454dee7fdad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1c806eb-2a73-40d3-a6eb-71f13a7bba61",
   "metadata": {},
   "source": [
    "#### RAG vs Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8bc017-ceaf-4130-917a-bc18fd14c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_rag_vs_finetuning = await assess_proposition2(\n",
    "    proposition = \"Which gives a Generative AI-based application the greatest capability and differentiation over competitors: RAG or Fine-Tuning.\", \n",
    "    short_name = 'rag_vs_finetuning',\n",
    "    added_instructions=\"\"\"\n",
    "* Identify best use cases for both approaches.\n",
    "* Give a reasoned argument in favor of the better one for a maximally profitable AI-based commercial product intended for professional product formulators in\n",
    "consumer packaged goods (CPG) R&D.\n",
    "\"\"\",\n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = True, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_rag_vs_finetuning['kb_dot'])\n",
    "if assessment_rag_vs_finetuning.get('plan_result'):\n",
    "    _ = print_summary(assessment_rag_vs_finetuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08cf17a-d76a-4b64-a30d-26604c528e2b",
   "metadata": {},
   "source": [
    "#### Cox Arboretum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c619fe-a8d8-4653-b821-3bc5d1165264",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_cox_arboretum = await assess_proposition2(\n",
    "    proposition = \"Cox Arboretum Metropark is the best place to walk for pleasurable exercise in the entire Cincinnati-Dayton metroplex.\", \n",
    "    short_name = 'cox_arboretum',\n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_cox_arboretum['kb_dot'])\n",
    "if assessment_cox_arboretum.get('plan_result'):\n",
    "    _ = print_summary(assessment_cox_arboretum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb9c9f-6b95-4080-a8e7-70320e9b3183",
   "metadata": {},
   "source": [
    "#### U.S.: a Chrisitian nation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c18c5-6c2f-4072-8dff-cd24731910c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_us_christian = await assess_proposition2(\n",
    "    proposition = \"America was conceived and founded with the intent of being a Christian nation, according to the writings of the Nation's forefathers and the Constitution.\", \n",
    "    short_name = 'us_christian',\n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_us_christian['kb_dot'])\n",
    "if assessment_us_christian.get('plan_result'):\n",
    "    _ = print_summary(assessment_us_christian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c063365-77db-4a34-8cc9-299a1ea75bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_us_christian2 = await assess_proposition2(\n",
    "    proposition = \"America was conceived and founded with the intent of being a Christian nation, according to the writings of the Nation's forefathers and the Constitution.\", \n",
    "    short_name = 'us_christian2',\n",
    "    added_instructions=\"\"\"\n",
    "Follow these principles in generating the query plan:\n",
    "\n",
    "* DO NOT make up sources, references, or any information.\n",
    "* Seek out and prioritize the most authoritative and reliable sources when collecting all information as evidence.\n",
    "* Emphasize a contrast of propositions and their alternative propositions (i.e., negations), thoroughly accumulating factual evidence for and against each proposition.\n",
    "* Systematically, lay out the arguments for and against each proposition.\n",
    "* DO NOT blindly accept any proposition as true unless reliable factual evidence in support of it outweighs similarly reliable factual evidence in refutation of it.\n",
    "\"\"\",\n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_us_christian2['kb_dot'])\n",
    "if assessment_us_christian2.get('plan_result'):\n",
    "    _ = print_summary(assessment_us_christian2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e4ed4-df05-4a88-b56c-816f6085ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_results = pickle_it(assessment_results = {'short_name': 'graph_rep_dishwashing2'}, save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35279f-4f68-45d0-8c90-81f914be5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_knowledge_graph(kg= convert_to_graph(plan= QueryPlan(**assessment_results['plan0']),proposition='Dishwashing Liquid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ccff28-d8d8-412b-86e8-b33b5950fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "assessment_graph_rep_dishwashing2a = await assess_proposition2(\n",
    "    proposition = \"Conceive of a graph network representation of commercial dishwashing liquid formulation science.\", \n",
    "    added_instructions =\"\"\"\n",
    "It is critical that in the execution of the plan, you absolutely must explicitly reference the concepts presented below, especially in the search queries.\n",
    "\n",
    "Consider these concepts:\n",
    "\n",
    "* Each formulation space is represented by 5 types of entities:\n",
    "  1. ::INGREDIENTS::,\n",
    "  2. functional ::ROLES::, \n",
    "  3. causal ::MECHANISMS::, \n",
    "  4. delivered ::BENEFITS::, \n",
    "  5. observed ::MANIFESTATIONS::.\n",
    "* Each ::INGREDIENT:: has 1 or more functional ::ROLES:: in the formulation.\n",
    "* Each functional ::ROLE:: in the formulation participates in 1 or more causal ::MECHANSIMS:: resulting in the delivery of an intended ::BENEFIT:: of the finished product.\n",
    "* Each delivered ::BENEFIT:: is the result of 1 or more causal ::MECHANISMS::.\n",
    "* Each ::BENEFIT:: is realized in terms of observed ::MANIFESTATIONS:: of 3 types:\n",
    "    1. laboratory measurements, ::LAB_MEASUREMENT:: (composition, physical properties, performance efficacy relevant to delivery of benefits)\n",
    "    2. sensory panel assessments, ::SENSORY_METRIC:: (organoleptic qualities relevant to benefits)\n",
    "    3. consumer survey responses. ::SURVEY:: (perception of benefits, satisfaction, overall liking)\n",
    "* Each functional ::ROLE:: is a parent node of the ::INGREDIENT:: that have that ::ROLE::.\n",
    "* Each functional ::ROLE:: is also a parent node of every causal ::MECHANISM:: that realizes that functional ::ROLE::.\n",
    "* Each causal ::MECHANISM:: is a parent node of the delivered ::BENEFITS:: that are the direct or indirect result of that ::MECHANISM::.\n",
    "* Each delivered ::BENEFIT:: is a parent node of the directly observed ::MANIFESTATIONS:: (::LAB_MEASUREMENT::, ::SENSORY_METRIC::, and/or ::SURVEY_RESPONSE::) of that ::BENEFIT::.\n",
    "\n",
    "Finally, as an example of realizing this proposition:\n",
    "\n",
    "* Generate a detailed, fine-granularity, comprehensive graph for the formulation space.\n",
    "* Make sure that all of the specific entities you can think of are included. \n",
    "* Present the graph using the Graphviz-compatible DOT syntax.\n",
    "\"\"\",\n",
    "    short_name = 'graph_rep_dishwashing2a', \n",
    "    use_plan = QueryPlan(**assessment_results['plan0']),\n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    #status_updates = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_graph_rep_dishwashing2a['kb_dot'])\n",
    "if assessment_graph_rep_dishwashing2a.get('plan_result'):\n",
    "    _ = print_summary(assessment_graph_rep_dishwashing2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be05c1d5-7553-4889-b2a0-d00f25da622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_graph_rep_dishwashing2 = await assess_proposition2(\n",
    "    proposition = \"Conceive of a graph network representation of commercial dishwashing liquid formulation science.\", \n",
    "    added_instructions =\"\"\"\n",
    "It is critical that in the execution of the plan, you absolutely must explicitly reference the concepts presented below, especially in the search queries.\n",
    "\n",
    "Consider these concepts:\n",
    "\n",
    "* Each formulation space is represented by 5 types of entities:\n",
    "  1. ::INGREDIENTS::,\n",
    "  2. functional ::ROLES::, \n",
    "  3. causal ::MECHANISMS::, \n",
    "  4. delivered ::BENEFITS::, \n",
    "  5. observed ::MANIFESTATIONS::.\n",
    "* Each ::INGREDIENT:: has 1 or more functional ::ROLES:: in the formulation.\n",
    "* Each functional ::ROLE:: in the formulation participates in 1 or more causal ::MECHANSIMS:: resulting in the delivery of an intended ::BENEFIT:: of the finished product.\n",
    "* Each delivered ::BENEFIT:: is the result of 1 or more causal ::MECHANISMS::.\n",
    "* Each ::BENEFIT:: is realized in terms of observed ::MANIFESTATIONS:: of 3 types:\n",
    "    1. laboratory measurements, ::LAB_MEASUREMENT:: (composition, physical properties, performance efficacy relevant to delivery of benefits)\n",
    "    2. sensory panel assessments, ::SENSORY_METRIC:: (organoleptic qualities relevant to benefits)\n",
    "    3. consumer survey responses. ::SURVEY:: (perception of benefits, satisfaction, overall liking)\n",
    "* Each functional ::ROLE:: is a parent node of the ::INGREDIENT:: that have that ::ROLE::.\n",
    "* Each functional ::ROLE:: is also a parent node of every causal ::MECHANISM:: that realizes that functional ::ROLE::.\n",
    "* Each causal ::MECHANISM:: is a parent node of the delivered ::BENEFITS:: that are the direct or indirect result of that ::MECHANISM::.\n",
    "* Each delivered ::BENEFIT:: is a parent node of the directly observed ::MANIFESTATIONS:: (::LAB_MEASUREMENT::, ::SENSORY_METRIC::, and/or ::SURVEY_RESPONSE::) of that ::BENEFIT::.\n",
    "\n",
    "Finally, as an example of realizing this proposition:\n",
    "\n",
    "* Generate a detailed, fine-granularity, comprehensive graph for the formulation space.\n",
    "* Make sure that all of the specific entities you can think of are included. \n",
    "* Present the graph using the Graphviz-compatible DOT syntax.\n",
    "\"\"\",\n",
    "    short_name = 'graph_rep_dishwashing2', \n",
    "    plan_only = False,\n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = True, \n",
    "    save_pickle = True,\n",
    "    #status_updates = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_graph_rep_dishwashing2['kb_dot'])\n",
    "if assessment_graph_rep_dishwashing2.get('plan_result'):\n",
    "    _ = print_summary(assessment_graph_rep_dishwashing2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf23c6-0759-4f4e-a30d-7934358218d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_graph_rep_dishwashing2['plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15170eb-fd32-4106-9eb1-b629b4e40620",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_graph_rep_formulations = await assess_proposition2(\n",
    "    proposition = \"Conceive of a graph network representation of chocolate chip recipes.\", \n",
    "    added_instructions =\"\"\"\n",
    "Consider these concepts:\n",
    "\n",
    "* Each formulation space is represented by 5 types of entities:\n",
    "  1. ::INGREDIENTS::,\n",
    "  2. functional ::ROLES::, \n",
    "  3. causal ::MECHANISMS::, \n",
    "  4. delivered ::BENEFITS::, \n",
    "  5. observed ::MANIFESTATIONS::.\n",
    "* Each ::INGREDIENT:: has 1 or more functional ::ROLES:: in the formulation.\n",
    "* Each functional ::ROLE:: in the formulation participates in 1 or more causal ::MECHANSIMS:: resulting in the delivery of an intended ::BENEFIT:: of the finished product.\n",
    "* Each delivered ::BENEFIT:: is the result of 1 or more causal ::MECHANISMS::, and is realized in terms of observed ::MANIFESTATIONS::.\n",
    "* ::MANIFESTATIONS:: are observations of 3 types: \n",
    "  1. laboratory measurements, ::LAB_MEASURE::\n",
    "  2. sensory panel assessments, ::SENSORY::\n",
    "  3. consumer survey responses. ::SURVEY::\n",
    "* Each functional ::ROLE:: is a parent node of the ::INGREDIENT:: that have that ::ROLE::.\n",
    "* Each functional ::ROLE:: is also a parent node of every causal ::MECHANISM:: that realizes that functional ::ROLE::.\n",
    "* Each causal ::MECHANISM:: is a parent node of the delivered ::BENEFITS:: that are the direct or indirect result of that ::MECHANISM::.\n",
    "* Each delivered ::BENEFIT:: is a parent node of the directly observed ::MANIFESTATIONS:: of that ::BENEFIT::.\n",
    "\n",
    "\"\"\",\n",
    "    short_name = 'graph_rep_formulations', \n",
    "    plan_only = False,\n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    #status_updates = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_graph_rep_formulations['kb_dot'])\n",
    "if assessment_graph_rep_formulations.get('plan_result'):\n",
    "    _ = print_summary(assessment_graph_rep_formulations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248d7b0-7c2f-44fb-8b02-bbf2fe5e3f65",
   "metadata": {},
   "source": [
    "#### Perception of time as you age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab6f18-87fd-4cb2-a7ae-7a7fdd615a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_perception_of_time = await assess_proposition2(\n",
    "    proposition = \"Time seems to pass faster as you age.\", \n",
    "    added_instructions =\"\"\"\n",
    "Consider these musings:\n",
    "\n",
    "* Each year you live is a smaller and smaller proportion of your life as you age: '1 yr as % of age' = 1/Age.\n",
    "* Therefore, the proportion of your life you've lived before that latest year becomes larger: '% of Life lived before latest year' = 1 - 1/Age\n",
    "* As you go through early adulthood, you become accustomed to life experiences/events at a certain pace, which is proportional to 1/Age.\n",
    "* For many people, an effective 'Reference Age' at which you benchmark this pace is 'Reference Age' = 30 years old.\n",
    "* Then, the pace of life at any age, is basically 'Pace of Life' = (1/'Reference Age')/(1/Age).\n",
    "* And, you eventually become acclimated to this perception as 'Perceived Speed of Life' = 1/(1 + exp(-6*('Pace of Life'-1))) = 1/(1 + exp(-6*(Age - 'Reference Age')/'Reference Age')) .\n",
    "* In other words, as you age, years ago seem to be as if they were just yesterday because of this 'Perceived Speed of Life'.\n",
    "\n",
    "\"\"\",\n",
    "    short_name = 'perception_of_time', \n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    #status_updates = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_perception_of_time['kb_dot'])\n",
    "if assessment_perception_of_time.get('plan_result'):\n",
    "    _ = print_summary(assessment_perception_of_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244de2d9-c8ff-4026-9b7a-067a2e30f6f9",
   "metadata": {},
   "source": [
    "#### Gary, Indiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe1302-3711-4c60-9cc5-cc5f358a1080",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_gary_indiana_test = await assess_proposition2(\n",
    "    proposition = \"Gary, Indiana is a failed city.\", \n",
    "    short_name = 'gary_indiana_test', \n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = True, \n",
    "    save_pickle = True,\n",
    "    status_updates = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_gary_indiana_test['kb_dot'])\n",
    "if assessment_gary_indiana_test.get('plan_result'):\n",
    "    _ = print_summary(assessment_gary_indiana_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5232c-ebf6-4a06-83bc-26acd03341a8",
   "metadata": {},
   "source": [
    "#### Brady query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8193343b-4af3-4baf-9036-0396a9fef628",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tom Brady is the NFL's greatest quarterback of all time.\"\n",
    "assessment_brady_qb_reflect = await assess_proposition2(\n",
    "    #plan_only = True,\n",
    "    proposition = question, \n",
    "    short_name = 'brady_qb_reflect', \n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = True,  # Always set True to force the generation of a new plan for this test run.\n",
    "    save_pickle = True,\n",
    "    status_updates = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_brady_qb_reflect['kb_dot'])\n",
    "if assessment_brady_qb_reflect.get('plan_result'):\n",
    "    _ = print_summary(assessment_brady_qb_reflect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2bb1f2-7fb3-49f3-a952-5faffa3ebeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tom Brady is the NFL's greatest quarterback of all time.\"\n",
    "assessment_brady_qb_test_improve = await assess_proposition2(\n",
    "    #plan_only = True,\n",
    "    proposition = question, \n",
    "    short_name = 'brady_qb_test_improve', \n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = True,  # Always set True to force the generation of a new plan for this test run.\n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_brady_qb_test_improve['kb_dot'])\n",
    "if assessment_brady_qb_test_improve.get('plan_result'):\n",
    "    _ = print_summary(assessment_brady_qb_test_improve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450cfe9-30d8-4775-ae35-1ba355dbd035",
   "metadata": {},
   "source": [
    "#### Mahomes, NFL GOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fcdffa-33bd-4033-8f81-2881ad18e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_mahomes_goat = await assess_proposition2(\n",
    "    #plan_only = True,\n",
    "    proposition = \"Patrick Mahomes is the NFL's greatest quarterback of all time.\", \n",
    "    short_name = 'mahomes_goat', \n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_mahomes_goat['kb_dot'])\n",
    "if assessment_mahomes_goat.get('plan_result'):\n",
    "    _ = print_summary(assessment_mahomes_goat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e41b7-5a04-45bf-a080-099f7c789b74",
   "metadata": {},
   "source": [
    "#### Economies of Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d7258-3f53-4c1d-ba87-ca2af1262673",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the best country, economically, in Africa?\"\n",
    "assessment_econ_africa = await assess_proposition2(\n",
    "    proposition = question, \n",
    "    short_name = 'econ_africa', \n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_econ_africa['kb_dot'])\n",
    "if assessment_econ_africa.get('plan_result'):\n",
    "    _ = print_summary(assessment_econ_africa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c3fb6-6cf2-4f13-b556-9a3ad0bf087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the best country, economically, in Africa?\"\n",
    "assessment_econ_africa_new = await assess_proposition2(\n",
    "    #plan_only = True,\n",
    "    proposition = question, \n",
    "    short_name = 'econ_africa_new', \n",
    "    added_instructions = \"\"\"\n",
    "* Focus on only the largest 5 African economies. \n",
    "* Perform searches to provide the identity of these countries as context for defining subsequent `Query` nodes by putting the node with the country names in the `dependencies` fields of other nodes.\n",
    "* If necessary, consider using dynamic search queries for [Country 1], [Country 2], etc., \n",
    "  being sure to supply the `Query` nodes with ample context to identify these countries at time of search.\n",
    "\"\"\",\n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20, \n",
    "    force_run = True, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_econ_africa_new['kb_dot'])\n",
    "if assessment_econ_africa_new.get('plan_result'):\n",
    "    _ = print_summary(assessment_econ_africa_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e30c97-7521-4a7c-a735-ce7b51866410",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the best country, economically, in Africa?\"\n",
    "assessment_econ_africa_new_improve = await assess_proposition2(\n",
    "    #plan_only = True,\n",
    "    proposition = question, \n",
    "    short_name = 'econ_africa_new_improve', \n",
    "    added_instructions = \"\"\"\n",
    "* Focus on only the largest 5 African economies. \n",
    "* Perform searches to provide the identity of these countries as context for defining subsequent `Query` nodes by putting the node with the country names in the `dependencies` fields of other nodes.\n",
    "* If necessary, consider using dynamic search queries for [Country 1], [Country 2], etc., \n",
    "  being sure to supply the `Query` nodes with ample context to identify these countries at time of search.\n",
    "\"\"\",\n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    improvement_round=True,\n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20, \n",
    "    force_run = True, \n",
    "    save_pickle = True,\n",
    "    status_updates = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_econ_africa_new_improve['kb_dot'])\n",
    "if assessment_econ_africa_new_improve.get('plan_result'):\n",
    "    _ = print_summary(assessment_econ_africa_new_improve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430478a7-89b3-42cd-8652-82f4dd768d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "[search_query for q in assessment_econ_africa_new['plan'].query_graph for search_query in q.search_query] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3542cd8e-bebd-42b7-9603-944df14041d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpt = get_report(assessment_econ_africa_new)\n",
    "\n",
    "print(rpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90677da8-aace-4aa3-ba38-fd2d9c9ab166",
   "metadata": {},
   "source": [
    "#### Difference in Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4f0f8-12f2-4045-9637-e88a6c1f2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the difference in populations between the country where Toronto is and Barack Obama's father's home country?\"\n",
    "assessment_pop_diff_reflect = await assess_proposition2(\n",
    "    proposition = question, \n",
    "    short_name = 'pop_diff_reflect', \n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    improvement_round=True,\n",
    "    added_instructions = '', \n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    report_to_file = False,\n",
    "    kb_label_wrap = 20, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_pop_diff_reflect['kb_dot'])\n",
    "if assessment_pop_diff_reflect.get('plan_result'):\n",
    "    _ = print_summary(assessment_pop_diff_reflect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db6c9f-cd6a-4850-97e2-831dec3f91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the difference in populations between the country where Toronto is and Barack Obama's father's home country?\"\n",
    "assessment_pop_diff = await assess_proposition2(\n",
    "    proposition = question, \n",
    "    short_name = 'pop_diff', \n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    use_plan = None,\n",
    "    plan_only = False, \n",
    "    added_instructions = '', \n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    report_to_file = False,\n",
    "    kb_label_wrap = 20, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "assessment_pop_diff['kb_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b724c-06c9-4190-a08a-2513e77abdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the difference in populations between the country where Toronto is and Barack Obama's father's home country?\"\n",
    "assessment_pop_diff_improve = await assess_proposition2(\n",
    "    proposition = question, \n",
    "    short_name = 'pop_diff_improve', \n",
    "    search_cache = search_cache,\n",
    "    improvement_round=True,\n",
    "    client = client, \n",
    "    use_plan = None,\n",
    "    plan_only = False, \n",
    "    added_instructions = '', \n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    report_to_file = False,\n",
    "    kb_label_wrap = 20, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "assessment_pop_diff_improve['kb_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76d3ae-0b4b-4026-88e5-8d224b61d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same proposition and plan, execute the assessment using GPT-4 instead of GPT-3.5-Turbo\n",
    "assessment_pop_diff_gpt4 = await assess_proposition2(\n",
    "    proposition = question, \n",
    "    short_name = 'pop_diff_gpt4', \n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    use_plan = assessment_pop_diff['plan'],\n",
    "    plan_only = False, \n",
    "    added_instructions = '', \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = False,\n",
    "    kb_label_wrap = 20, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 0\n",
    ")\n",
    "_=print_summary(assessment_pop_diff_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf788ca-6dd2-4392-b41a-edaf8d03691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_pop_diff_gpt4.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758cca67-f9c9-4565-8df7-40c5c5298b28",
   "metadata": {},
   "source": [
    "#### Long, complex proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa65674-336e-422b-971e-1953e16c738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_proposition = \"The eastern neighbor of the country where the father of the 44th U.S. president was born is similar in size to the total population of the U.S. state that has the largest population at risk of earthquakes.\"\n",
    "assessment_long_proposition_reflect =  await assess_proposition2(\n",
    "    proposition = long_proposition, \n",
    "    short_name = 'long_proposition_reflect', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = True,\n",
    "    save_pickle = True,\n",
    "    status_updates = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_long_proposition_reflect['kb_dot'])\n",
    "if assessment_long_proposition_reflect.get('plan_result'):\n",
    "    _=print_summary(assessment_long_proposition_reflect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8231c-4fd7-442e-919a-04ef21b3a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_proposition = \"The eastern neighbor of the country where the father of the 44th U.S. president was born is similar in size to the total population of the U.S. state that has the largest population at risk of earthquakes.\"\n",
    "assessment_long_proposition =  await assess_proposition2(\n",
    "    proposition = long_proposition, \n",
    "    short_name = 'long_proposition', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = True,\n",
    "    save_pickle = True,\n",
    "    print_level = 0\n",
    ")\n",
    "display(assessment_long_proposition['kb_dot'])\n",
    "_=print_summary(assessment_long_proposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262f01ca-603d-4bd5-8394-46c48b280f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_proposition = \"The eastern neighbor of the country where the father of the 44th U.S. president was born is similar in size to the total population of the U.S. state that has the largest population at risk of earthquakes.\"\n",
    "assessment_long_proposition0 =  await assess_proposition2(\n",
    "    proposition = long_proposition, \n",
    "    short_name = 'long_proposition0', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "    Use this key:\n",
    "    * <<A>> = the 44th U.S. president\n",
    "    * <<B>> = the father of <<A>>\n",
    "    * <<C>> = the country where <<B>> was born\n",
    "    * <<D>> = the eastern neighbor of <<C>>\n",
    "    * <<E>> = the total population of <<D>>\n",
    "    * <<F>> = the U.S. state that has the largest population at risk of earthquakes\n",
    "    * <<G>> = the total population of <<F>>\n",
    "    \"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 35,\n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    print_level=0\n",
    ")\n",
    "display(assessment_long_proposition0['kb_dot'])\n",
    "_=print_summary(assessment_long_proposition0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a2677-59b6-4a43-88a1-5d7362b9f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_proposition0 = \"The eastern neighbor of the country where the father of the 44th U.S. president was born is similar in size to the total population of the U.S. state that has the largest population at risk of earthquakes.\"\n",
    "assessment_long_proposition0_dynamic =  await assess_proposition2(\n",
    "    proposition = long_proposition0, \n",
    "    short_name = 'long_proposition0_dynamic', \n",
    "    use_plan = assessment_long_proposition0['plan'],\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "    Use this key:\n",
    "    * <<A>> = the 44th U.S. president\n",
    "    * <<B>> = the father of <<A>>\n",
    "    * <<C>> = the country where <<B>> was born\n",
    "    * <<D>> = the eastern neighbor of <<C>>\n",
    "    * <<E>> = the total population of <<D>>\n",
    "    * <<F>> = the U.S. state that has the largest population at risk of earthquakes\n",
    "    * <<G>> = the total population of <<F>>\n",
    "    \"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 35,\n",
    "    force_run = True,\n",
    "    save_pickle = True,\n",
    "    print_level = 1\n",
    ")\n",
    "#display(assessment_long_proposition0_dynamic['kb_dot'])\n",
    "_=print_summary(assessment_long_proposition0_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454608c-877c-426f-bacf-f3bd3bc9f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_proposition0 = \"The eastern neighbor of the country where the father of the 44th U.S. president was born is similar in size to the total population of the U.S. state that has the largest population at risk of earthquakes.\"\n",
    "assessment_long_proposition0_dynamic_nokey =  await assess_proposition2(\n",
    "    proposition = long_proposition0, \n",
    "    short_name = 'long_proposition0_dynamic_nokey', \n",
    "    use_plan = None,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = False,\n",
    "    save_pickle = True\n",
    ")\n",
    "display(assessment_long_proposition0_dynamic_nokey['kb_dot'])\n",
    "_=print_summary(assessment_long_proposition0_dynamic_nokey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511307a5-2cf3-4117-9651-e63bee5163a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_proposition = \"<<D>> is similar in size to <<G>>.\"\n",
    "assessment_long_proposition =  await assess_proposition2(\n",
    "    proposition = long_proposition, \n",
    "    short_name = 'long_proposition', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "    Use this key:\n",
    "    * <<A>> = the 44th U.S. president\n",
    "    * <<B>> = the father of <<A>>\n",
    "    * <<C>> = the country where <<B>> was born\n",
    "    * <<D>> = the eastern neighbor of <<C>>\n",
    "    * <<E>> = the total population of <<D>>\n",
    "    * <<F>> = the U.S. state that has the largest population at risk of earthquakes\n",
    "    * <<G>> = the total population of <<F>>\n",
    "    \"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = True,\n",
    "    save_pickle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4615eac0-5b9d-4b7b-b3a0-cdb8c5ca8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_long_proposition_dynamic =  await assess_proposition2(\n",
    "    proposition = long_proposition, \n",
    "    short_name = 'long_proposition_dynamic', \n",
    "    use_plan = assessment_long_proposition['plan'],\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "    Use this key:\n",
    "    * <<A>> = the 44th U.S. president\n",
    "    * <<B>> = the father of <<A>>\n",
    "    * <<C>> = the country where <<B>> was born\n",
    "    * <<D>> = the eastern neighbor of <<C>>\n",
    "    * <<E>> = the total population of <<D>>\n",
    "    * <<F>> = the U.S. state that has the largest population at risk of earthquakes\n",
    "    * <<G>> = the total population of <<F>>\n",
    "    \"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = True,\n",
    "    save_pickle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee1d84-8084-42c6-bd00-6ac5158361e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = assessment_long_proposition_dynamic['plan']\n",
    "plan_result = assessment_long_proposition_dynamic['plan_result']\n",
    "last_node_result = list(plan_result.values())[-1]\n",
    "search_dict = assessment_long_proposition_dynamic['search_dict']\n",
    "short_name = assessment_long_proposition_dynamic['short_name']\n",
    "report_rmarkdown = generate_report(\n",
    "    plan_outline = plan.query_plan_outline,\n",
    "    lastnode=last_node_result,\n",
    "    computed_queries=plan_result,\n",
    "    search_dict=search_dict, \n",
    "    filename_stem=short_name, \n",
    "    write_to_file=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f78125-5882-4332-b8ce-23579663c3ca",
   "metadata": {},
   "source": [
    "#### Arbitrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ea5b1-4514-422f-8f07-e154cff015c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_prop2 = \"What is the tallest building in the 3rd most populous country in Asia?\"\n",
    "assessment_long_prop2 =  await assess_proposition2(\n",
    "    proposition = long_prop2, \n",
    "    short_name = 'long_prop2', \n",
    "    use_plan = None,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    dynamic = True,\n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    print_level = 0\n",
    ")\n",
    "display(assessment_long_prop2['kb_dot'])\n",
    "_=print_summary(assessment_long_prop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b554874-9083-4747-bf67-527706a81e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_prop2a = \"The Autograph Tower of Indonesia is taller than the Pertamina Energy Tower.\"\n",
    "assessment_long_prop2a =  await assess_proposition2(\n",
    "    proposition = long_prop2a, \n",
    "    short_name = 'long_prop2a', \n",
    "    use_plan = None,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    dynamic = True,\n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = True,\n",
    "    save_pickle = True\n",
    ")\n",
    "display(assessment_long_prop2a['kb_dot'])\n",
    "_=print_summary(assessment_long_prop2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4faa1c4-4ee8-4a44-855e-11b5b3cb60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_proposition_result_dynamic = await assessment_long_proposition['plan'].execute_dynamic(client,model='gpt-4-turbo-preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074bf1d5-3bf4-4e9d-9c52-46db7196b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_long_proposition_result_dynamic = list(long_proposition_result_dynamic.values())[-1]\n",
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',last_long_proposition_result_dynamic.query.replace(\"\\\\n\",\"\\n\")) + '\\n\\n')\n",
    "print('RESPONSE: ' + last_long_proposition_result_dynamic.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e67636-e527-4b67-8e7a-74710da14e56",
   "metadata": {},
   "source": [
    "#### Baboons vs. Hippos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6de7151-7f90-42d0-afd8-2818f53c1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_baboon_hippo2 =  await assess_proposition2(\n",
    "    proposition = \"A baboon would beat a hippo in a 40-meter race.\", \n",
    "    short_name = 'baboon_hippo2', \n",
    "    search_cache = search_cache, \n",
    "    improvement_round=True,\n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, # Don't bother saving a report to file\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_baboon_hippo2['kb_dot'])\n",
    "_=print_summary(assessment_baboon_hippo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd62442-be00-4dfc-b15c-f69c8bfb9994",
   "metadata": {},
   "source": [
    "#### Grease Fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2175348-751b-4b90-9c60-64e99b057849",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_grease_fire =  await assess_proposition2(\n",
    "    proposition =  \"Covering it with a pan lid is the best way to handle a grease fire in the kitchen.\", \n",
    "    short_name = 'grease_fire', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_grease_fire['kb_dot'])\n",
    "if assessment_grease_fire.get('plan_result'):\n",
    "    print_summary(assessment_grease_fire)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe54f10-dd09-47f6-93f5-f8ab5e925bc2",
   "metadata": {},
   "source": [
    "#### Smoke detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70138ac5-16a6-414d-a2de-d3a2f894dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_smoke_detectors_reflect =  await assess_proposition2(\n",
    "    proposition =  \"The best smoke detectors and locations in the home.\", \n",
    "    short_name = 'smoke_detectors_reflect', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    status_updates=True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_smoke_detectors_reflect['kb_dot'])\n",
    "if assessment_smoke_detectors_reflect.get('plan_result'):\n",
    "    print_summary(assessment_smoke_detectors_reflect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8e7d2-61fe-4909-89b5-4b2a093469e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_smoke_detectors =  await assess_proposition2(\n",
    "    proposition =  \"The best smoke detectors and locations in the home.\", \n",
    "    short_name = 'smoke_detectors', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_smoke_detectors['kb_dot'])\n",
    "if assessment_smoke_detectors.get('plan_result'):\n",
    "    print_summary(assessment_smoke_detectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf3373e-0c29-4702-80f7-9ad457df515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_smoke_detectors2 =  await assess_proposition2(\n",
    "    proposition =  \"The best smoke detectors, locations in the home, and highly recommended models (with URLs).\", \n",
    "    short_name = 'smoke_detectors2', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    force_run = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_smoke_detectors2['kb_dot'])\n",
    "if assessment_smoke_detectors2.get('plan_result'):\n",
    "    print_summary(assessment_smoke_detectors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96dbc2-2075-455e-b1ac-53014b08513e",
   "metadata": {},
   "source": [
    "## Sports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb73ab2-b6c9-4814-a3e9-909c1055455d",
   "metadata": {},
   "source": [
    "#### Tom Brady, NFL GOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5b99d-eabf-40e0-9eca-4581a5c846a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = instructor.patch(OpenAI())\n",
    "proposition = \"Tom Brady is the GOAT of the NFL\"\n",
    "#key_question = f\"\"\"Given proposition '{proposition}'. First identify all sub-propositions and their sub-propositions, and so on, that need to be true to support it.\n",
    "#Then identify all factual axiomatic statements as evidence that support or refute each and every one of the propositions. \n",
    "#Finally, answer this question: Is '{proposition}' the truth?\"\"\"\n",
    "\n",
    "key_question = f\"\"\"# Mandate:\n",
    "Given proposition '{proposition}'. \n",
    "1. Enumerate all sub-propositions, and their sub-propositions and so on, that need to be true to support it.\n",
    "2. Enumerate all factual axiomatic statements as evidence that support or refute each and every one of the propositions. \n",
    "3. Record these propositions and evidence as a textual outline (in 'query_plan_outline') describing the query plan.\n",
    "3. Generate the query plan as a tree of query nodes (in 'query_graph').\n",
    "   - Assign a separate query node for each specific sub-proposition.\n",
    "   - Assign a separate query node for each specific piece of evidence needed to confirm and refute each sub-proposition. \n",
    "   - Be sure to specify search_flag=True for a query node when needed to gather the latest factual information as evidence.\n",
    "   - Demand that actual specific data and statistics are gathered as evidence and that the answers to the propositions are stated in terms of these data.\n",
    "\"\"\"\n",
    "\n",
    "mandate = f\"\"\"\n",
    "# The Question for which we require a Query Plan:\n",
    "Is '{proposition}' the truth?\"\"\"\n",
    "\n",
    "plan_brady = query_planner( key_question + mandate, client )\n",
    "pprint(plan_brady.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d15278-888f-46cf-b2fc-c83efa7d51df",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph: KnowledgeGraph = convert_to_graph(plan_brady,proposition,label_wrap=20)\n",
    "kb_dot = visualize_knowledge_graph(graph,flip=True, aspect_ratio= 'auto')\n",
    "kb_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fbae9-3155-4991-8bc8-98dc7f4d657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_brady2_result = await plan_brady.execute(client,model='gpt-4-turbo-preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42d75d-4852-4e61-878e-3caa9935fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_node_brady2_result_gpt4 = list(plan_brady2_result.values())[-1]\n",
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',last_node_brady2_result_gpt4.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + last_node_brady2_result_gpt4.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cfa140-4c64-47c5-86fa-fda0f4181bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_brady_goat =  await assess_proposition2(\n",
    "    proposition = \"Tom Brady is the GOAT of the NFL\", \n",
    "    short_name = 'brady_goat', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level = 1\n",
    ")\n",
    "display(assessment_brady_goat['kb_dot'])\n",
    "_=print_summary(assessment_brady_goat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7669ae4-8393-4416-9952-241ffafa7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"Tom Brady is the GOAT of the NFL\"\n",
    "assessment_brady_new = await assess_proposition2(\n",
    "    proposition = question, \n",
    "    short_name = 'brady_new', \n",
    "    #use_plan = assessment_nfl_playoffs_new['plan'],\n",
    "    #plan_only = True,\n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = False,\n",
    "    kb_label_wrap = 20, \n",
    "    force_run = True, \n",
    "    save_pickle = True,\n",
    "    print_level = 1\n",
    ")\n",
    "display(assessment_brady_new['kb_dot'])\n",
    "if assessment_brady_new.get('plan_result'):\n",
    "    _ = print_summary(assessment_brady_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c623d7eb-536f-404f-be90-fd4d8bf20c5c",
   "metadata": {},
   "source": [
    "#### Michael Jordan, NBA GOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c145fd-a62a-45c6-9751-deecd8c17740",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.patch(OpenAI())\n",
    "assessment_jordan_goat2 =  await assess_proposition2(\n",
    "    proposition = \"Michael Jordan is the GOAT of the NBA\", \n",
    "    short_name = 'jordan_goat2', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level = 1\n",
    ")\n",
    "display(assessment_jordan_goat2['kb_dot'])\n",
    "_=print_summary(assessment_jordan_goat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f43678-1ae9-4911-88c9-fa5b9a5d5fa5",
   "metadata": {},
   "source": [
    "#### Lebron James, NBA GOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80475734-afd7-4153-979e-190dbbf80d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_lebron_goat =  await assess_proposition2(\n",
    "    proposition = \"Lebron James is the GOAT of the NBA\", \n",
    "    short_name = 'lebron_goat', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce0c5b6-367a-4c07-8b54-f0d2d324aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_lebron_goat['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a363d39-3785-4539-b851-e51742bff15d",
   "metadata": {},
   "source": [
    "#### Lewis Hamilton, Formula 1 GOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f35b8a-110a-433e-bedc-4d89e730e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_hamilton_goat_reflect =  await assess_proposition2(\n",
    "    proposition = \"Lewis Hamilton is the GOAT of Formula 1 racing.\", \n",
    "    short_name = 'hamilton_goat_reflect', \n",
    "    plan_only=False,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = 'Consider the top 5 Formula 1 drivers of all-time.', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\",\n",
    "    force_run = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    status_updates=True,\n",
    "    print_level=2\n",
    ")\n",
    "display(assessment_hamilton_goat_reflect['kb_dot'])\n",
    "if assessment_hamilton_goat_reflect.get('plan_result'):\n",
    "    _=print_summary(assessment_hamilton_goat_reflect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54500b2e-979a-4b70-9fbd-4a8467f61a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_hamilton_goat_new =  await assess_proposition2(\n",
    "    proposition = \"Lewis Hamilton is the GOAT of Formula 1 racing.\", \n",
    "    short_name = 'hamilton_goat_new', \n",
    "    plan_only=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = 'Consider the top 5 Formula 1 drivers of all-time.', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\",\n",
    "    force_run = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level=2\n",
    ")\n",
    "display(assessment_hamilton_goat_new['kb_dot'])\n",
    "if assessment_hamilton_goat_new.get('plan_result'):\n",
    "    _=print_summary(assessment_hamilton_goat_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44774ca-c1e0-4f9f-a60a-4df26d43f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_hamilton_goat_improved =  await assess_proposition2(\n",
    "    proposition = \"Lewis Hamilton is the GOAT of Formula 1 racing.\", \n",
    "    short_name = 'hamilton_goat_improved', \n",
    "    improvement_round=True,\n",
    "    #plan_only=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = 'Consider the top 5 Formula 1 drivers of all-time.', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\",\n",
    "    force_run = False,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level=2\n",
    ")\n",
    "display(assessment_hamilton_goat_improved['kb_dot'])\n",
    "if assessment_hamilton_goat_improved.get('plan_result'):\n",
    "    _=print_summary(assessment_hamilton_goat_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0724c02-532c-47bd-849a-78206cad3580",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(assessment_hamilton_goat_new['plan'].query_graph[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce01bf8-7c37-4649-8d5a-87f9c69b6c0c",
   "metadata": {},
   "source": [
    "#### Novak Djokovic, Tennis GOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163397e-2c08-416d-a75d-b032cd56122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"Novak Djokovic is the GOAT of professional tennis.\"\n",
    "assessment_djokovic_goat = await assess_proposition2(\n",
    "    proposition = question, \n",
    "    short_name = 'djokovic_goat', \n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_djokovic_goat['kb_dot'])\n",
    "if assessment_djokovic_goat.get('plan_result'):\n",
    "    _ = print_summary(assessment_djokovic_goat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e10bf-9acc-4341-9206-8bb493cb957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"Novak Djokovic is the GOAT of professional tennis.\"\n",
    "assessment_djokovic_goat_new = await assess_proposition2(\n",
    "    proposition = question, \n",
    "    short_name = 'djokovic_goat_new', \n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 24, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_djokovic_goat_new['kb_dot'])\n",
    "if assessment_djokovic_goat_new.get('plan_result'):\n",
    "    _ = print_summary(assessment_djokovic_goat_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b3490-a7fb-4911-911b-517e8de5252b",
   "metadata": {},
   "source": [
    "#### Sports Prediction: Kansas City defeats San Francisco in Super Bowl LVIII, February 11, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a8030-6504-4526-9a9e-dd15c1bdc37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_super_bowl_lviii_new =  await assess_proposition2(\n",
    "    proposition =  \"Predict if San Francisco 49ers defeat Kansas City Chiefs in Super Bowl LVIII on February 11,2024\", \n",
    "    short_name = 'super_bowl_lviii_new', \n",
    "    improvement_round=True,\n",
    "    use_plan = None,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Accept the \\<\\<GIVEN FACTS\\>\\> as truths that needn't be questioned. DO NOT create Query nodes for these given facts.\n",
    "\n",
    "=========\n",
    "\n",
    "\\<\\<GIVEN FACTS\\>\\>:\n",
    "* Super Bowl LVIII will be played on February 11,2024.\n",
    "* San Francisco 49ers will play the Kansas City Chiefs in Super Bowl LVIII.\n",
    "\n",
    "==========\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 24,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_super_bowl_lviii_new['kb_dot'])\n",
    "if assessment_super_bowl_lviii_new.get('plan_result'):\n",
    "    _ = print_summary(assessment_super_bowl_lviii_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e5a49-6f6d-4f0d-ab3c-29154f7e8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_super_bowl_lviii_score =  await assess_proposition2(\n",
    "    proposition =  \"Prediction of the final score of the San Francisco 49ers versus the Kansas City Chiefs in Super Bowl LVIII on February 11,2024.\", \n",
    "    short_name = 'super_bowl_lviii_score', \n",
    "    improvement_round=True,\n",
    "    use_plan = None,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Devise a query plan to predict the final score of Super Bowl LVIII.  Just find a plan that works.\n",
    "\n",
    "Guarantee that when the plan is executed it adheres to the following principles:\n",
    "\n",
    "* A prediction of the final score will be produced for each team. \n",
    "* Focus on deriving the narrative that leads to the score rather than external experts and media predictions of final scores or point spreads.\n",
    "* The plan explains how the factors logically accumulate over the course of the game to result in the final score for each team.\n",
    "* DO NOT make any statements of caveats about the unpredictability and uncertainty of predicting scores. \n",
    "* Each team's strengths and weaknesses are carefully, systematically, and meticulously analyzed to arrive at a reasoned rationale for the predictions.\n",
    "  * Each team's coaching staffs going head-to-head against their respective coaching adversary: SF offensive coordinator vs. KC defensive coordinator; KC offensive coordinator vs. SF defensive coordinator.\n",
    "  * Each team's offensive talent paired against each team's defensive talent: SF offense vs. KC defense; KC offense vs. SF defense.\n",
    "  * Each team's special teams going both ways: SF kicking vs KC receiving/defending; KC kicking vs. SF receiving/defending.\n",
    "* Provided a reasoned explanation of factors such as fast or slow start by the respective teams and the impact on their odds of scoring quarter-by-quarter.\n",
    "    \n",
    "Accept the \\<\\<GIVEN FACTS\\>\\> as truths that needn't be questioned. DO NOT create Query nodes for these given facts.\n",
    "\n",
    "=========\n",
    "\n",
    "\\<\\<GIVEN FACTS\\>\\>:\n",
    "* Super Bowl LVIII will be played on February 11,2024.\n",
    "* San Francisco 49ers and Kansas City Chiefs will play against each other in Super Bowl LVIII.\n",
    "\n",
    "==========\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = True,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 24,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_super_bowl_lviii_score['kb_dot'])\n",
    "if assessment_super_bowl_lviii_score.get('plan_result'):\n",
    "    _ = print_summary(assessment_super_bowl_lviii_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7650e7f-781d-4704-af5b-b75d02d1f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_super_bowl_lviii =  await assess_proposition2(\n",
    "    proposition =  \"Predict if Baltimore Ravens will (1) meet and (2) defeat Dallas Cowboys in Super Bowl LVIII on February 11,2024\", \n",
    "    short_name = 'super_bowl_lviii', \n",
    "    use_plan = None,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes either lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these given facts.\n",
    "\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* Super Bowl LVIII will be played on February 11,2024.\n",
    "\n",
    "==========\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = True,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f4975-ae8c-43b8-bf65-fc69cc02fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_super_bowl_lviii['kb_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88777fee-724b-436f-a3d1-a6bf66ea2617",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_super_bowl_lviii2 =  await assess_proposition2(\n",
    "    proposition =  \"Predict if Baltimore Ravens will (1) meet and (2) defeat Dallas Cowboys in Super Bowl LVIII on February 11,2024\", \n",
    "    short_name = 'super_bowl_lviii2', \n",
    "    use_plan = None,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes either lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\n",
    "Accept the <<GIVEN FACTS>> as truths that are not to be questioned. DO NOT create Query nodes for any of the <<GIVEN FACTS>>.\n",
    "\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* Super Bowl LVIII will be played on February 11,2024.\n",
    "* Both Baltimore and Dallas have qualified for the playoffs this year (2024).\n",
    "* The Baltimore Ravens are the #1 seed in the AFC and will have a bye in the Wildcard Round.\n",
    "* The Dallas Cowboys are the #2 seed in the NFC and will host the #7 seed Green Bay Packers in the Wildcard Round on January 15, 2024. \n",
    "\n",
    "==========\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = True,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d47a3d-ff57-4877-af5a-360e4243b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_super_bowl_lviii2['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92627cdc-da3e-438f-ad8a-e4fb32a7a7df",
   "metadata": {},
   "source": [
    "#### Buffalo Bills at Miami Dolphins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb5490e-b5ef-438f-aa77-eca5fd33e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_bills_dolphins =  await assess_proposition2(\n",
    "    proposition =  \"Who will win Buffalo Bills vs Miami Dolphins today (January 7, 2024)?\", \n",
    "    short_name = 'bills_dolphins', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes either lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these given facts.\n",
    "\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* The game is being played today, January 7, 2024.\n",
    "\n",
    "==========\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = True,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca14a89-a675-4fe1-b913-9362df9ae940",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_bills_dolphins['kb_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb912bd-8153-4343-98ca-dcef13da1538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def replace_teams(query_plan : QueryPlan,home: str, away: str, date_string: str = None, verbose: bool = False):\n",
    "    home_city = re.sub(r'(.+) [^ ]+$',r'\\1',home).lower()\n",
    "    away_city = re.sub(r'(.+) [^ ]+$',r'\\1',away).lower()\n",
    "    nfl_game_txt = assessment_bills_dolphins['plan'].model_dump_json()\n",
    "    nfl_game_txt = nfl_game_txt.replace(\"Miami Dolphins\",home).replace(\"Buffalo Bills\",away)\n",
    "    nfl_game_txt = nfl_game_txt.replace(\"miami\",home_city).replace(\"buffalo\",away_city)\n",
    "    if date_string is not None:\n",
    "        nfl_game_txt = nfl_game_txt.replace(\" today\",\"\").replace(\"January 7, 2024\",date_string)\n",
    "    if verbose:\n",
    "        print(nfl_game_txt)\n",
    "    new_plan = QueryPlan.model_validate_json(nfl_game_txt)\n",
    "    return new_plan\n",
    "\n",
    "plan_browns_bengals = replace_teams(query_plan=assessment_bills_dolphins['plan'],home ='Cincinnati Bengals', away = 'Cleveland Browns', date_string = None, verbose = False)\n",
    "kb_browns_bengals = convert_to_graph(plan = plan_browns_bengals, proposition=plan_browns_bengals.query_graph[0].question)\n",
    "visualize_knowledge_graph(kb_browns_bengals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf8e82-4c3a-414d-a1c4-ebf109863f17",
   "metadata": {},
   "source": [
    "#### Cleveland Browns at Cincinnati Bengals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9def7e-20c2-4e9e-80aa-23aab2eaae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_browns_bengals_dynamic =  await assess_proposition2(\n",
    "    proposition =  \"Who will win Cleveland Browns vs Cincinnati Bengals today (January 7, 2024)?\", \n",
    "    short_name = 'browns_bengals_dynamic', \n",
    "    use_plan = plan_browns_bengals,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes either lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these given facts.\n",
    "\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* The game is being played today, January 7, 2024.\n",
    "\n",
    "==========\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = True,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55734bfc-7e5b-418c-bbf4-dbc2d782d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_browns_bengals_dynamic =  generate_report(\n",
    "    plan_outline = assessment_browns_bengals_dynamic['plan'].query_plan_outline,\n",
    "    lastnode=list(assessment_browns_bengals_dynamic['plan_result'].values())[-1],\n",
    "    computed_queries=assessment_browns_bengals_dynamic['plan_result'],\n",
    "    search_dict=assessment_browns_bengals_dynamic['search_dict'], \n",
    "    filename_stem=assessment_browns_bengals_dynamic['short_name'], \n",
    "    write_to_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62161fda-1bd3-4c64-ad21-b2f134330498",
   "metadata": {},
   "source": [
    "#### Roles of Ingredients in Salad Dressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd981b09-2bf9-4cb3-8c6f-09c0876673c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.patch(OpenAI())\n",
    "plan_dressing = query_planner( \"What are the functional roles of the ingredients in a commercially manufactured salad dressing? Be meticulous, thorough, accurate, and complete in enumerating all of the types of ingredients.\",client)\n",
    "pprint(plan_dressing.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fdf96e-8f5e-47d1-b719-d4b3c53eaa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph: KnowledgeGraph = convert_to_graph(plan_dressing,question,label_wrap=20)\n",
    "kb_dot = visualize_knowledge_graph(graph,filename_stem='plan_dressing_gpt4',flip=True, aspect_ratio= 'auto')\n",
    "kb_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b834b0-b2f8-4f07-ad97-c9f808a1b584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plan_dressing_result = await plan_dressing.execute(client,model=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c290491-7706-4e80-a362-fb9a8e2d13c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import re\n",
    "last_node_dressing_result = list(plan_dressing_result.values())[-1]\n",
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',last_node_dressing_result.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + last_node_dressing_result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f808ad-5558-40cc-ac31-acee3efa5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(search_cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc37a2-a8f8-4fbd-8db9-7ffb90ba1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "dressing_search_keys = [q.search_query for q in plan_dressing.query_graph if len(q.search_query)>0]\n",
    "dressing_search = {key: summarize_raw_search(eval(search_cache[key])) for key in dressing_search_keys if search_cache.get(key,None) is not None}\n",
    "\n",
    "report_markdown_dressing_gpt4 = generate_report(\n",
    "    plan_outline = plan_dressing.query_plan_outline,\n",
    "    lastnode=last_node_dressing_result,\n",
    "    computed_queries=plan_dressing_result,\n",
    "    search_dict=dressing_search, \n",
    "    filename_stem='plan_dressing_gpt4_again', \n",
    "    write_to_file=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c8893-4106-42d9-8272-3e1f102c4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_dressing =  await assess_proposition2(\n",
    "    proposition =  \"What are the functional roles of the ingredients in a commercially manufactured salad dressing?\", \n",
    "    short_name = 'dressing', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"Be meticulous, thorough, accurate, and complete in enumerating all of the types of ingredients.\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_dressing['kb_dot'])\n",
    "if assessment_dressing.get('plan_result'):\n",
    "    print_summary(assessment_dressing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed817c-1a4b-4022-b861-1aaff09bba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_dressing_scm =  await assess_proposition2(\n",
    "    proposition =  \"The fundamental basis of formulating and assessing commercially manufactured salad dressing can be represented in a detailed structural causal model (SCM).\", \n",
    "    short_name = 'dressing_scm', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_dressing_scm['kb_dot'])\n",
    "if assessment_dressing_scm.get('plan_result'):\n",
    "    print_summary(assessment_dressing_scm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e13d8-dc5d-4af9-87e9-1dd7aa81a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_dressing_scm2 =  await assess_proposition2(\n",
    "    proposition =  \"The fundamental basis of formulating and assessing commercially manufactured salad dressing can be represented in a detailed structural causal model (SCM).\", \n",
    "    short_name = 'dressing_scm2', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "* Generate a SCM for formulation and assessment of commercially manufactured salad dressing to provide proof of the proposition in the form of a fully executed example.\n",
    "* Capture the full causal mechanisms from ingredients through all of the subsequent stages of latent variables:\n",
    "  * composition\n",
    "  * morphology\n",
    "  * physical & chemical phenomena\n",
    "  * product behavior\n",
    "  * product performance\n",
    "* Manifest the latent variable at each stage with the appropriate observable outcomes:\n",
    "  * analytical laboratory measurements\n",
    "  * human sensory panel assessments\n",
    "  * consumer survey perception of product benefits\n",
    "  * consumer survey assessment of product benefits and, ultimately, overall liking\n",
    "  \n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_dressing_scm2['kb_dot'])\n",
    "if assessment_dressing_scm2.get('plan_result'):\n",
    "    print_summary(assessment_dressing_scm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6842e173-cbc7-425a-88d8-8dce2074438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(assessment_dressing_scm2['plan_result'].values())[-1].response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6599949-b10b-4a8f-a8a2-6f320bdbc8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_dressing_scm3 =  await assess_proposition2(\n",
    "    proposition =  \"The fundamental basis of formulating and assessing commercially manufactured salad dressing can be represented in a detailed structural causal model (SCM).\", \n",
    "    short_name = 'dressing_scm3', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "* Generate a SCM for formulation and assessment of commercially manufactured salad dressing to provide proof of the proposition in the form of a fully executed example.\n",
    "  * Capture the full causal mechanisms from ingredients through all of the subsequent stages of latent variables:\n",
    "    * composition\n",
    "    * morphology\n",
    "    * physical & chemical phenomena\n",
    "    * product behavior\n",
    "    * product performance\n",
    "  * Manifest the latent variable at each stage with the appropriate observable outcomes:\n",
    "    * analytical laboratory measurements\n",
    "    * human sensory panel assessments\n",
    "    * consumer survey perception of product benefits\n",
    "    * consumer survey assessment of product benefits and, ultimately, overall liking\n",
    "  * Generate the resulting directed acyclic graph (DAG)\n",
    "    * create distinct nodes for all entities\n",
    "      * functional ingredient roles as inputs, latent variables within each stage, and manifest outcomes of all types (lab, sensory, survey)\n",
    "      * capture as a high-resolution, fine-grain representation of the important entities for reliable causal inference\n",
    "    * represent all of the direct causal relationships between the entities as edges\n",
    "    * represent the DAG in DOT syntax suitable for display in Graphviz\n",
    "  \n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_dressing_scm3['kb_dot'])\n",
    "if assessment_dressing_scm3.get('plan_result'):\n",
    "    print_summary(assessment_dressing_scm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847b23e-8ff6-4ecd-80b4-fa510f7b282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_dressing_scm4 =  await assess_proposition2(\n",
    "    proposition =  \"The fundamental basis of formulating and assessing commercially manufactured salad dressing can be represented in a detailed Structural Causal Model.\", \n",
    "    short_name = 'dressing_scm4', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that the query plan includes `Query` nodes to accomplish the following task:\n",
    "\n",
    "* Generate a Structural Causal Model for the formulation and assessment of commercially manufactured salad dressing to provide proof of the proposition in the form of a fully executed example.\n",
    "  * Capture the full causal mechanisms from ingredients through all of the subsequent stages of latent variables:\n",
    "    * composition\n",
    "    * morphology\n",
    "    * physical & chemical phenomena\n",
    "    * product behavior\n",
    "    * product performance\n",
    "  * Manifest the latent variable at each stage with the appropriate observable outcomes:\n",
    "    * analytical laboratory measurements\n",
    "    * human sensory panel assessments\n",
    "    * consumer survey perception of product benefits\n",
    "    * consumer survey assessment of product benefits and, ultimately, overall liking\n",
    "  * Generate the resulting directed acyclic graph (DAG)\n",
    "    * create distinct nodes for all entities at the following granularity:\n",
    "      * functional ingredient roles as inputs, latent variables within each stage of causal mechanisms, and manifest outcomes of all types (lab, sensory, survey)\n",
    "      * capture as a high-resolution, fine-grain representation of the important entities for reliable causal inference\n",
    "    * represent all of the direct causal relationships between the entities as edges\n",
    "    * represent the DAG in DOT syntax suitable for display in Graphviz\n",
    "  \n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_dressing_scm4['kb_dot'])\n",
    "if assessment_dressing_scm4.get('plan_result'):\n",
    "    print_summary(assessment_dressing_scm4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad4342-0d22-4440-82b4-98c8fcc55be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_dressing_scm4_reflect =  await assess_proposition2(\n",
    "    proposition =  \"The fundamental basis of formulating and assessing commercially manufactured salad dressing can be represented in a detailed Structural Causal Model.\", \n",
    "    short_name = 'dressing_scm4_reflect', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that the query plan includes `Query` nodes to accomplish the following task:\n",
    "\n",
    "* Generate a Structural Causal Model for the formulation and assessment of commercially manufactured salad dressing to provide proof of the proposition in the form of a fully executed example.\n",
    "  * Capture the full causal mechanisms from ingredients through all of the subsequent stages of latent variables:\n",
    "    * composition\n",
    "    * morphology\n",
    "    * physical & chemical phenomena\n",
    "    * product behavior\n",
    "    * product performance\n",
    "  * Manifest the latent variable at each stage with the appropriate observable outcomes:\n",
    "    * analytical laboratory measurements\n",
    "    * human sensory panel assessments\n",
    "    * consumer survey perception of product benefits\n",
    "    * consumer survey assessment of product benefits and, ultimately, overall liking\n",
    "  * Generate the resulting directed acyclic graph (DAG)\n",
    "    * create distinct nodes for all entities at the following granularity:\n",
    "      * functional ingredient roles as inputs, latent variables within each stage of causal mechanisms, and manifest outcomes of all types (lab, sensory, survey)\n",
    "      * capture as a high-resolution, fine-grain representation of the important entities for reliable causal inference\n",
    "    * represent all of the direct causal relationships between the entities as edges\n",
    "    * represent the DAG in DOT syntax suitable for display in Graphviz\n",
    "  \n",
    "\"\"\", \n",
    "    client = client,\n",
    "    force_run=True,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    status_updates=True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_dressing_scm4_reflect['kb_dot'])\n",
    "if assessment_dressing_scm4_reflect.get('plan_result'):\n",
    "    print_summary(assessment_dressing_scm4_reflect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79f6cb3-126f-45db-b318-74240aa8f433",
   "metadata": {},
   "source": [
    "#### Roles of Ingredients in Sunscreen Lotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e8434-73c2-4ef7-b49e-7b99739440ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_sunscreen = query_planner( \"What are the functional roles of the ingredients in a commercially manufactured sunscreen lotion?\",client)\n",
    "pprint(plan_sunscreen.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b716e3-67da-4542-8806-35d1742ca987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plan_sunscreen_result = await plan_sunscreen.execute(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8308c-eba9-4c38-85c1-0ace460932cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k,v in plan_dressing_result.items():\n",
    "#    print({k: v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec32813-2c84-401f-81cd-7f6a804f615f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import re\n",
    "last_node_result = list(plan_sunscreen_result.values())[-1]\n",
    "#last_node_result = plan_dressing_result['1']\n",
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',last_node_result.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + last_node_result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d69d57-4a2f-4977-9a1d-ce7775239771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(search_cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b734d0-cc0a-4f9f-abaa-360cfed783d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunscreen_search_keys = [q.search_query for q in plan_dressing.query_graph if len(q.search_query)>0]\n",
    "sunscreen_search = {key: summarize_raw_search(eval(search_cache[key])) for key in sunscreen_search_keys if search_cache.get(key,None) is not None}\n",
    "\n",
    "#report_markdown = make_report(computed_query = plan_sunscreen_result,search_dict=sunscreen_search)\n",
    "report_markdown = make_report(computed_query = last_node_result,search_dict=sunscreen_search)\n",
    "mrkdwn_file_path = 'plan_sunscreen.Rmd'\n",
    "with open(mrkdwn_file_path, 'w') as file:\n",
    "    file.write(to_ascii(report_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d91009-663a-47d9-819b-dd7fc6848384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8037a876-2dd2-48ba-b5c8-4adcdbcc3746",
   "metadata": {},
   "source": [
    "#### Salad Dressing, again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c05fa8-049b-4a3a-9b08-4e8edbea5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('What are the functional roles of the ingredients in a commercially manufactured salad dressing?\\nContext: {\"responses\":[{\"query\":\"What is the functional role of emulsifiers in salad dressing?\",\"response\":\"The functional role of emulsifiers in salad dressing is to stabilize and maintain the emulsion between oil and water-based ingredients. Salad dressings typically consist of oil, vinegar or other acidic ingredients, and water-based ingredients like herbs, spices, and flavorings. Since oil and water do not naturally mix, emulsifiers are added to prevent separation and create a smooth and uniform texture.\\\\n\\\\nEmulsifiers work by reducing the surface tension between oil and water, allowing them to mix together more easily. They form a protective layer around the oil droplets, preventing them from coalescing and separating from the water-based ingredients. This helps to create a stable and homogeneous mixture, ensuring that the dressing remains well-blended and does not separate over time.\\\\n\\\\nAdditionally, emulsifiers can enhance the mouthfeel and texture of salad dressings by providing a creamy and smooth consistency. They can also improve the overall sensory experience by enhancing flavor release and preventing the formation of undesirable textures, such as oiliness or grittiness.\\\\n\\\\nCommon emulsifiers used in salad dressings include lecithin, which is derived from soybeans or egg yolks, and xanthan gum, a natural thickening agent. These emulsifiers help to create a visually appealing and palatable salad dressing that is easy to pour, spread, and mix with salad ingredients.\"},{\"query\":\"What is the functional role of preservatives in salad dressing?\",\"response\":\"The functional role of preservatives in salad dressing is to inhibit the growth of microorganisms, such as bacteria, yeast, and mold, thereby extending the shelf life of the product. Preservatives help maintain the freshness, quality, and safety of the salad dressing by preventing spoilage and the growth of harmful bacteria that could cause foodborne illnesses. They also help preserve the flavor, texture, and appearance of the dressing over time.\"},{\"query\":\"What is the functional role of flavor enhancers in salad dressing?\",\"response\":\"The functional role of flavor enhancers in salad dressing is to enhance and intensify the taste and aroma of the dressing. Flavor enhancers are typically added to salad dressings to improve the overall flavor profile, making them more appealing and enjoyable to consume. They work by stimulating the taste buds and enhancing the perception of flavors, making the dressing more flavorful and satisfying. Flavor enhancers can include ingredients such as monosodium glutamate (MSG), yeast extracts, natural or artificial flavorings, herbs, spices, or other flavor-enhancing compounds.\"},{\"query\":\"What is the functional role of thickeners in salad dressing?\",\"response\":\"Thickeners play a crucial functional role in salad dressings by providing the desired texture and consistency. They help to create a smooth and creamy texture, preventing the dressing from separating or becoming watery. Thickeners also enhance the mouthfeel of the dressing, giving it a more luxurious and satisfying feel when consumed. Additionally, they help to suspend and distribute the various ingredients evenly throughout the dressing, ensuring a consistent flavor and appearance.\"},{\"query\":\"What is the functional role of acids in salad dressing?\",\"response\":\"Acids play several functional roles in salad dressing:\\\\n\\\\n1. Flavor enhancement: Acids, such as vinegar or citrus juices, add a tangy and refreshing taste to salad dressings. They help balance the flavors by providing a contrast to the richness of oils and other ingredients.\\\\n\\\\n2. pH adjustment: Acids help in adjusting the pH of the dressing. Most salad dressings are slightly acidic, which helps to inhibit the growth of bacteria and increase the shelf life of the dressing.\\\\n\\\\n3. Emulsification: Acids aid in the emulsification process, which is the process of combining oil and water-based ingredients to create a stable and homogeneous mixture. The acid helps to break down the oil into smaller droplets, allowing it to mix more easily with other ingredients.\\\\n\\\\n4. Tenderizing: Some acids, like lemon juice or vinegar, can help tenderize certain ingredients in the salad, such as raw vegetables or proteins. The acid breaks down the fibers and proteins, making them more tender and easier to eat.\\\\n\\\\n5. Preservation: Acids act as natural preservatives by inhibiting the growth of bacteria and other microorganisms. They create an environment that is less favorable for spoilage, thus extending the shelf life of the salad dressing.\\\\n\\\\nOverall, acids in salad dressing contribute to the taste, texture, preservation, and overall quality of the dressing.\"},{\"query\":\"What is the functional role of colorants in salad dressing?\",\"response\":\"The functional role of colorants in salad dressing is to enhance the visual appeal of the product. Colorants are added to give the dressing an attractive and appetizing appearance, making it more visually appealing to consumers. This can help in attracting attention and influencing purchasing decisions. Colorants are used to achieve a desired color or shade that is visually appealing and consistent with the flavor or type of dressing. Additionally, colorants can also be used to differentiate between different varieties or flavors of salad dressings.\"}]}')\n",
    "print('-'*100)\n",
    "print('The functional roles of the ingredients in a commercially manufactured salad dressing are as follows:\\n\\n1. Emulsifiers: Emulsifiers stabilize and maintain the emulsion between oil and water-based ingredients. They prevent separation and create a smooth and uniform texture in the dressing.\\n\\n2. Preservatives: Preservatives inhibit the growth of microorganisms, extending the shelf life of the dressing. They help maintain freshness, quality, and safety by preventing spoilage and the growth of harmful bacteria.\\n\\n3. Flavor enhancers: Flavor enhancers intensify the taste and aroma of the dressing, making it more appealing and enjoyable to consume. They stimulate the taste buds and enhance the perception of flavors.\\n\\n4. Thickeners: Thickeners provide the desired texture and consistency to the dressing. They create a smooth and creamy texture, prevent separation, and enhance the mouthfeel of the dressing.\\n\\n5. Acids: Acids add flavor, adjust the pH, aid in emulsification, tenderize ingredients, and act as natural preservatives. They enhance the taste, texture, preservation, and overall quality of the dressing.\\n\\n6. Colorants: Colorants enhance the visual appeal of the dressing. They give the dressing an attractive and appetizing appearance, making it visually appealing to consumers and helping to differentiate between different varieties or flavors of salad dressings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63268825-4730-4fe7-8f37-709040ccdf01",
   "metadata": {},
   "source": [
    "## Book Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd79908-614e-4322-a13f-d99deea794a9",
   "metadata": {},
   "source": [
    "#### Assess Annie Duke's book \"Thinking in Bets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260f1f4-ae6d-4dbf-952b-92607e3cf4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_duke_bets =  await assess_proposition2(\n",
    "    proposition =  \"Explain the premise of and assess the importance of the book 'Thinking in Bets' by Annie Duke.\", \n",
    "    short_name = 'duke_bets', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_duke_bets['kb_dot'])\n",
    "if assessment_duke_bets.get('plan_result'):\n",
    "    print_summary(assessment_duke_bets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2b85a-0366-4c68-a093-0bef30dbf904",
   "metadata": {},
   "source": [
    "#### Assess Annie Duke's book \"Quit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9087a-c839-4fd1-a969-8f00118613e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_duke_quit_dict = eval(re.sub(\"Thinking in Bets\",\"Quit\",assessment_duke_bets['plan'].model_dump_json()))\n",
    "plan_duke_quit = QueryPlan(**plan_duke_quit_dict)\n",
    "visualize_knowledge_graph(convert_to_graph(plan_duke_quit,\"Explain the premise of and assess the importance of the book 'Quit' by Annie Duke.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b82eb-fcbb-404c-b0f0-a8420493d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_duke_quit =  await assess_proposition2(\n",
    "    proposition =  proposition, \n",
    "    short_name = 'duke_quit', \n",
    "    use_plan = plan_duke_quit,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = False,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    print_level = 2\n",
    ")\n",
    "#display(assessment_duke_quit['kb_dot'])\n",
    "if assessment_duke_quit.get('plan_result'):\n",
    "    print_summary(assessment_duke_quit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11bfd4-cde0-4560-86c0-34f088e89037",
   "metadata": {},
   "source": [
    "#### Assess Andy Grove's book \"Only the Paranoid Survive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad51293-2f88-4108-a585-5d9811c861fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposition_duke_quit = \"Explain the premise of and assess the importance of the book 'Quit' by Annie Duke.\"\n",
    "proposition = re.sub(\"Annie Duke\",\"Andrew S. Grove\",re.sub(\"Quit\",\"Only the Paranoid Survive\",proposition_duke_quit))\n",
    "plan_json = re.sub(\"Annie Duke\",\"Andrew S. Grove\",re.sub(\"Quit\",\"Only the Paranoid Survive\",assessment_duke_quit['plan'].model_dump_json()))\n",
    "plan_dict = eval(plan_json)\n",
    "plan = QueryPlan(**plan_dict)\n",
    "visualize_knowledge_graph(convert_to_graph(plan,proposition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83272680-46a0-46f3-b2f3-2b88c1defaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_grove_paranoid =  await assess_proposition2(\n",
    "    proposition =  proposition, \n",
    "    short_name = 'grove_paranoid', \n",
    "    use_plan = plan,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = False,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    print_level = 2\n",
    ")\n",
    "#display(assessment_grove_paranoid['kb_dot'])\n",
    "if assessment_grove_paranoid.get('plan_result'):\n",
    "    print_summary(assessment_grove_paranoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68abe28-0e43-4d4d-a5fc-d5d71efb98e2",
   "metadata": {},
   "source": [
    "#### Murder on the Orient Express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33bf41-1082-4f9d-b57a-9bd3614da629",
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"Agatha Christie's 'Murder on the Orient Express' has a great plot.\"\n",
    "assessment_orient_express = await assess_proposition2(\n",
    "    proposition = question,\n",
    "    short_name = 'orient_express', \n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25, \n",
    "    force_run = True, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_orient_express['kb_dot'])\n",
    "if assessment_orient_express.get('plan_result'):\n",
    "    _ = print_summary(assessment_orient_express)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc773f9f-974c-4183-97c9-cfdd94beed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in assessment_orient_express['plan'].query_graph:\n",
    "    print(f\"{q.id}: {q.question}\\n-- {q.dependencies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a619dc-5914-410a-919e-c6aaa41f426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"Agatha Christie's book 'Murder on the Orient Express' has a great plot.\"\n",
    "assessment_orient_express2 = await assess_proposition2(\n",
    "    proposition = question,\n",
    "    short_name = 'orient_express2', \n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_orient_express2['kb_dot'])\n",
    "if assessment_orient_express2.get('plan_result'):\n",
    "    _ = print_summary(assessment_orient_express2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f89115-0ecb-4ab6-b226-3485accbf2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in assessment_orient_express2['plan'].query_graph:\n",
    "    print(f\"{q.id}: {q.question}\\n-- {q.dependencies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beff60d0-a981-4462-bbd9-c9e9c9f95bb9",
   "metadata": {},
   "source": [
    "## Culture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372a75c-3a3f-47a5-8359-a830408188fe",
   "metadata": {},
   "source": [
    "#### Culture wars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe5e39-6579-4db8-90c0-57b566ecfeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_culture_wars = await assess_proposition2(\n",
    "    proposition = \"ESPN is the epicenter of the American 'culture wars'.\", \n",
    "    short_name = 'culture_wars', \n",
    "    added_instructions = \"\"\"\n",
    "Especially, consider tensions between the hosts, commentators, guests, and audiences of the following ESPN shows:\n",
    "\n",
    "* First Take\n",
    "* The Pat McAfee Show\n",
    "* Get Up\n",
    "* SportsCenter\n",
    "\n",
    "And the tensions in the following sports and their relationships with stars in pop, rap, and country music and the movie industry:\n",
    "\n",
    "* NFL\n",
    "* NBA\n",
    "* WNBA\n",
    "* NCAA Mens Football\n",
    "* NCAA Mens Basketball\n",
    "* NCAA Womens Basketball\n",
    "* NHL\n",
    "* MSL\n",
    "\"\"\",\n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True, \n",
    "    kb_label_wrap = 24, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_culture_wars['kb_dot'])\n",
    "if assessment_culture_wars.get('plan_result'):\n",
    "    _ = print_summary(assessment_culture_wars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b312e-034a-482b-a9b2-db881df14dcc",
   "metadata": {},
   "source": [
    "#### \"Chill soundtrack for a generation\"\n",
    "\n",
    "According to this [article at Fatherly](https://www.fatherly.com/entertainment/air-talkie-walkie-retrospective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a390bd3-fdb5-418a-92ce-6fb1711e5f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"What album, 20 years ago, became the 'chill soundtrack of a generation'.\"\n",
    "assessment_album_chill = await assess_proposition2(\n",
    "    proposition = question,\n",
    "    short_name = 'album_chill', \n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 28, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_album_chill['kb_dot'])\n",
    "if assessment_album_chill.get('plan_result'):\n",
    "    _ = print_summary(assessment_album_chill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f88bb9-a3d4-44d9-be7a-e93b0b613fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"What album of 2004 became the 'chill soundtrack for a generation'.\"\n",
    "assessment_album_chill2 = await assess_proposition2(\n",
    "    proposition = question,\n",
    "    short_name = 'album_chill2', \n",
    "    added_instructions = \"\"\"\n",
    "    \n",
    "* Identify candidate albums of 2004.\n",
    "* Enumerate criteria to be 'chill soundtrack of a generation'.\n",
    "* Evaluate candidates by the criteria.\n",
    "* Answer the top-level question.\n",
    "\n",
    "\"\"\",\n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 28, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_album_chill2['kb_dot'])\n",
    "if assessment_album_chill2.get('plan_result'):\n",
    "    _ = print_summary(assessment_album_chill2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57748cbf-617a-4490-a467-37cb5d4ca363",
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"The 2004 album 'Talkie Walkie' by Air has become the 'chill soundtrack for a generation'.\"\n",
    "assessment_album_chill2_improved = await assess_proposition2(\n",
    "    proposition = question,\n",
    "    short_name = 'album_chill2_improved', \n",
    "    added_instructions = \"\"\"\n",
    "Consider the following information.\n",
    "\n",
    "The essence of this proposition is posed as a question in The Fatherly Turntable article\n",
    "[\"20 Years Ago, One Album Became The Chill Soundtrack For A Generation: Why Airâ€™s *Talkie Walkie* is more than that one *Lost in Translation* song.\"](https://www.fatherly.com/entertainment/air-talkie-walkie-retrospective).\n",
    "\n",
    "After noting the popularity of the tracks *Run*, used in the TV show *Veronica Mars*, and *Alone in Kyoto*, used in the movie *Lost in Translation*,\n",
    "the article poses the following quintessential question:\n",
    "\n",
    "* \"Are there Air songs left on *Talkie Walkie* that can become part of the movie of your life, today, two decades later?\"\n",
    "\n",
    "And answered within that article with a resounding \"Yes\".\n",
    "\n",
    "\"\"\",\n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 28, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_album_chill2_improved['kb_dot'])\n",
    "if assessment_album_chill2_improved.get('plan_result'):\n",
    "    _ = print_summary(assessment_album_chill2_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0140d923-d61c-4549-977d-85a21d29adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"The 2004 album 'Talkie Walkie' by Air has become the 'chill soundtrack for a generation'.\"\n",
    "assessment_album_chill2_improved_a = await assess_proposition2(\n",
    "    proposition = question,\n",
    "    short_name = 'album_chill2_improved_a', \n",
    "    added_instructions = \"\"\"\n",
    "Make sure that all search queries unambiguously reference \"the Air album 'Talkie Walkie'\" to avoid confusion with the walkie-talkie communication device.\n",
    "DO NOT discuss the walkie-talkie communication device.\n",
    "\n",
    "Consider the following information.\n",
    "\n",
    "The essence of this proposition is posed as a question in The Fatherly Turntable article\n",
    "[\"20 Years Ago, One Album Became The Chill Soundtrack For A Generation: Why Airâ€™s *Talkie Walkie* is more than that one *Lost in Translation* song.\"](https://www.fatherly.com/entertainment/air-talkie-walkie-retrospective).\n",
    "\n",
    "After noting the popularity of the tracks *Run*, used in the TV show *Veronica Mars*, and *Alone in Kyoto*, used in the movie *Lost in Translation*,\n",
    "the article poses the following quintessential question:\n",
    "\n",
    "* \"Are there Air songs left on *Talkie Walkie* that can become part of the movie of your life, today, two decades later?\"\n",
    "\n",
    "And the article answers that question with a resounding \"Yes\".\n",
    "\n",
    "\"\"\",\n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 28, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_album_chill2_improved_a['kb_dot'])\n",
    "if assessment_album_chill2_improved_a.get('plan_result'):\n",
    "    _ = print_summary(assessment_album_chill2_improved_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712b16a-3df7-4dea-9d6d-f5fbe14086a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"The 2004 album 'Talkie Walkie' by Air has become the 'chill soundtrack for a generation'.\"\n",
    "assessment_album_chill2_improved_b = await assess_proposition2(\n",
    "    proposition = question,\n",
    "    short_name = 'album_chill2_improved_b', \n",
    "    added_instructions = \"\"\"\n",
    "Make sure that all search queries unambiguously reference \"the Air album 'Talkie Walkie'\" to avoid confusion with the walkie-talkie communication device.\n",
    "DO NOT discuss the walkie-talkie communication device.\n",
    "\n",
    "Consider the following information.\n",
    "\n",
    "The essence of this proposition is posed as a question in The Fatherly Turntable article\n",
    "[\"20 Years Ago, One Album Became The Chill Soundtrack For A Generation: Why Airâ€™s *Talkie Walkie* is more than that one *Lost in Translation* song.\"](https://www.fatherly.com/entertainment/air-talkie-walkie-retrospective).\n",
    "\n",
    "After noting the popularity of the tracks *Run*, used in the TV show *Veronica Mars*, and *Alone in Kyoto*, used in the movie *Lost in Translation*,\n",
    "the article poses the following question:\n",
    "\n",
    "* \"Are there Air songs left on *Talkie Walkie* that can become part of the movie of your life, today, two decades later?\"\n",
    "\n",
    "And the article answers that question with a resounding \"Yes\".\n",
    "\n",
    "Discuss that central concept: \"songs... that can become part of the movie of your life, today, two decades later\" -- a soundtrack of the lives of a generation.\n",
    "\n",
    "\"\"\",\n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 28, \n",
    "    force_run = False, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_album_chill2_improved_b['kb_dot'])\n",
    "if assessment_album_chill2_improved_b.get('plan_result'):\n",
    "    _ = print_summary(assessment_album_chill2_improved_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3dea64-4c50-4b3b-b265-f87e872f8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "question ='For those who were young adults in 2004, the Air album *Talkie Walkie* has songs \"... that can become part of the movie of your life, today, two decades later\" -- a soundtrack of the lives of a generation.'\n",
    "assessment_album_chill2_improved_c = await assess_proposition2(\n",
    "    proposition = question,\n",
    "    short_name = 'album_chill2_improved_c', \n",
    "    added_instructions = \"\"\"\n",
    "Make sure that all search queries unambiguously reference \"the Air album 'Talkie Walkie'\" to avoid confusion with the walkie-talkie communication device.\n",
    "DO NOT discuss the walkie-talkie communication device.\n",
    "\n",
    "Consider the following information:\n",
    "\n",
    "* The essence of this proposition is posed as a question in The Fatherly Turntable article\n",
    "[\"20 Years Ago, One Album Became The Chill Soundtrack For A Generation: Why Airâ€™s *Talkie Walkie* is more than that one *Lost in Translation* song.\"](https://www.fatherly.com/entertainment/air-talkie-walkie-retrospective).\n",
    "After noting the popularity of the tracks *Run*, used in the TV show *Veronica Mars*, and *Alone in Kyoto*, used in the movie *Lost in Translation*,\n",
    "the article poses the following question:\n",
    "   * \"Are there Air songs left on *Talkie Walkie* that can become part of the movie of your life, today, two decades later?\"\n",
    "   * And the article answers that question with a resounding \"Yes\".\n",
    "\n",
    "\"\"\",\n",
    "    search_cache = search_cache,\n",
    "    client = client, \n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 28, \n",
    "    force_run = True, \n",
    "    save_pickle = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_album_chill2_improved_c['kb_dot'])\n",
    "if assessment_album_chill2_improved_c.get('plan_result'):\n",
    "    _ = print_summary(assessment_album_chill2_improved_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7599e3c-019c-4226-b02c-162179095aea",
   "metadata": {},
   "source": [
    "#### TwoSetViolin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143fab3-4edc-4046-a4db-ac15a00aac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_twosetviolin =  await assess_proposition2(\n",
    "    proposition =  \"TwoSetViolin is revolutionizing the classical music fan experience.\", \n",
    "    short_name = 'twosetviolin', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "Be sure to leverage internet searches while investigating the entities, concepts, ideas, and propositions.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = False,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4fb718-d453-4706-af45-702a740ea10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_twosetviolin_improved =  await assess_proposition2(\n",
    "    proposition =  \"TwoSetViolin is revolutionizing the classical music fan experience.\", \n",
    "    short_name = 'twosetviolin_improved', \n",
    "    search_cache = search_cache, \n",
    "    improvement_round=True,\n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = False,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level=2\n",
    ")\n",
    "display(assessment_twosetviolin_improved['kb_dot'])\n",
    "if assessment_twosetviolin_improved.get('plan_result'):\n",
    "    _ = print_summary(assessment_twosetviolin_improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbdeb36-89f4-4257-ac1f-e701b501d5ac",
   "metadata": {},
   "source": [
    "#### Taylor Swift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea745b4-ac68-42d3-aadf-df1b2d6016c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_taylor_swift =  await assess_proposition2(\n",
    "    proposition =  \"Taylor Swift is revolutionizing the pop music fan experience.\", \n",
    "    short_name = 'taylor_swift', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "Be sure to leverage internet searches while investigating the entities, concepts, ideas, and propositions.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = False,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960ba0c-66ac-4420-a1e6-2ade2b1b5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_processing(assessment_taylor_swift['plan'],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94f446-fca4-4ca3-9277-b77432fc5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_taylor_swift['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfdf028-dc2c-4eff-b063-7b4f5ae18b3e",
   "metadata": {},
   "source": [
    "#### TwoSetViolin can learn from Taylor Swift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab7737-4e03-4bd7-8016-3e58d3469736",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_twoset_swift =  await assess_proposition2(\n",
    "    proposition =  \"TwoSetViolin can learn from Taylor Swift about career maturation as they, their careers, and their fans mature.\", \n",
    "    short_name = 'twoset_swift', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "Be sure to leverage internet searches while investigating the entities, concepts, ideas, and propositions.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = False,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b72ab-3279-43a4-ae7a-c7825842f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_processing(assessment_twoset_swift['plan'],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4a05b-b609-4e77-8a2f-ddad561c6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_twoset_swift['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de38adf-65a8-46dd-8a4c-4aebd23e5b22",
   "metadata": {},
   "source": [
    "#### Netflix 'The Diplomat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008caec1-dee2-4d0a-ab0c-44b68608e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_netflix_diplomat =  await assess_proposition(\n",
    "    proposition = \"In the Netflix drama 'The Diplomat', the portrayal of diversity in the highest ranks of government, esp. the diplomatic corp, does not accurately reflect reality.\", \n",
    "    short_name = 'netflix_diplomat', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions =  \"\"\"\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these facts.\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* Netflix has a drama called 'The Diplomat'.\n",
    "* Season 1 of 'The Diplomat' has a racially & ethnically diverse cast.\n",
    "* 'The Diplomat' portrays many people of color in the highest ranks of the U.S. government, esp. the diplomatic corp -- e.g. ambassadors, diplomats, foreign ministers, etc.\n",
    "* 'The Diplomat' portrays many people of color in the highest ranks of the U.K. government, esp. the diplomatic corp -- e.g. ambassadors, diplomats, foreign ministers, etc.\n",
    "* 'The Diplomat' portrays many women in the highest ranks of the U.S. government, esp. the diplomatic corp -- e.g. ambassadors, diplomats, foreign ministers, etc.\n",
    "* 'The Diplomat' portrays many women in the highest ranks of the U.K. government, esp. the diplomatic corp -- e.g. ambassadors, diplomats, foreign ministers, etc.\n",
    "\n",
    "==========\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\",\n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292d70a-66a0-49e8-bdd4-2ce0bf3a05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qg[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2cb594-94f4-4fce-acc9-d4a233557996",
   "metadata": {},
   "outputs": [],
   "source": [
    "qp = QueryPlan(**(assessment_netflix_diplomat['plan'].model_dump()))\n",
    "qg = qp.query_graph\n",
    "\n",
    "qnodes = {q.id : q for q in qg}\n",
    "\n",
    "qnodes['main_proposition'].dependencies.extend(['racial_ethnic_diversity_cast','gender_diversity_cast'])\n",
    "qnodes['racial_ethnic_diversity_us'].dependencies.extend(['racial_ethnic_diversity_cast'])\n",
    "qnodes['racial_ethnic_diversity_uk'].dependencies.extend(['racial_ethnic_diversity_cast'])\n",
    "qnodes['gender_diversity_us'].dependencies.extend(['gender_diversity_cast'])\n",
    "qnodes['gender_diversity_uk'].dependencies.extend(['gender_diversity_cast'])\n",
    "\n",
    "qg = list(qnodes.values())\n",
    "\n",
    "qg_add = [\n",
    "    Query(\n",
    "        id='racial_ethnic_diversity_cast',\n",
    "        question='What is the racial and ethnic composition of the cast of the Netflix drama \"The Diplomat\", esp. roles portraying the highest ranks of the U.K. government, esp. the diplomatic corp?',\n",
    "        dependencies =[],\n",
    "        search_query='racial ethnic composition of the cast of Netflix \"The Diplomat\" (season 1) with roles'\n",
    "    ),\n",
    "    Query(id='gender_diversity_cast',\n",
    "        question='What is the gender composition of the cast of the Netflix drama \"The Diplomat\", esp. roles portraying the highest ranks of the U.K. government, esp. the diplomatic corp?',\n",
    "        dependencies =[],\n",
    "        search_query='gender composition of the cast of Netflix \"The Diplomat\" (season 1) with roles'\n",
    "    )\n",
    "]\n",
    "qg.extend(qg_add)\n",
    "\n",
    "qp.query_graph = qg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d83dd8-399a-4ddb-a962-fc32d483dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_netflix_diplomat2 =  await assess_proposition(\n",
    "    proposition = \"In the Netflix drama 'The Diplomat', the portrayal of diversity in the highest ranks of government, esp. the diplomatic corp, does not accurately reflect reality.\", \n",
    "    short_name = 'netflix_diplomat2', \n",
    "    use_plan = qp,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions =  \"\"\"\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these facts.\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* Netflix has a drama called 'The Diplomat'.\n",
    "* Season 1 of 'The Diplomat' has a racially & ethnically diverse cast.\n",
    "* 'The Diplomat' portrays many people of color in the highest ranks of the U.S. government, esp. the diplomatic corp -- e.g. ambassadors, diplomats, foreign ministers, etc.\n",
    "* 'The Diplomat' portrays many people of color in the highest ranks of the U.K. government, esp. the diplomatic corp -- e.g. ambassadors, diplomats, foreign ministers, etc.\n",
    "* 'The Diplomat' portrays many women in the highest ranks of the U.S. government, esp. the diplomatic corp -- e.g. ambassadors, diplomats, foreign ministers, etc.\n",
    "* 'The Diplomat' portrays many women in the highest ranks of the U.K. government, esp. the diplomatic corp -- e.g. ambassadors, diplomats, foreign ministers, etc.\n",
    "\n",
    "==========\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\",\n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e0de77-abd2-4c56-8b52-0b769214a5d9",
   "metadata": {},
   "source": [
    "#### Rainer Rilke -- 'dragons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d4aa4-0547-4067-bc34-4358c7617c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_rilke_dragons =  await assess_proposition2(\n",
    "    proposition =  \"Rainer Rilke references dragons in a perfect metaphor for life's challenges.\", \n",
    "    short_name = 'rilke_dragons', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these facts.\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* Quote by Rilke: \"Perhaps all the dragons in our lives are princesses who are only waiting to see us act, just once, with beauty and courage. Perhaps everything that frightens us is, in its deepest essence, something helpless that wants our love.\"\n",
    "-- Rainer Maria Rilke, Letters to a Young Poet\n",
    "==========\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = False,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933b7db-b45c-43e7-867c-4487c355405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_rilke_dragons['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef8157a-9e40-47ef-ba93-574272d9f318",
   "metadata": {},
   "source": [
    "#### Rainer Rilke -- Philosopher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b63b37-da93-4c6e-9fde-0f943af2f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_rilke_philosopher2 =  await assess_proposition2(\n",
    "    proposition =  \"Rainer Rilke was a philosopher with great insights into life.\", \n",
    "    short_name = 'rilke_philosopher2', \n",
    "    search_cache = search_cache, \n",
    "    improvement_round=True,\n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level=2\n",
    ")\n",
    "display(assessment_rilke_philosopher2['kb_dot'])\n",
    "if assessment_rilke_philosopher2.get('plan_result'):\n",
    "    _=print_summary(assessment_rilke_philosopher2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c7238-6e2a-4250-82bc-f1fa5cc74e07",
   "metadata": {},
   "source": [
    "#### UK Lottery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5dcd20-d12c-4eea-9e25-2fe60da75991",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_uk_lottery =  await assess_proposition(\n",
    "    proposition =  \"Mathematicians found a guaranteed way to win the UK lottery in 2023.\", \n",
    "    short_name = 'uk_lottery', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af09d633-6068-4611-8721-40f6789e7ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_uk_lottery['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bae84e-f7b5-4cfd-a11b-5d726d4a5d4f",
   "metadata": {},
   "source": [
    "#### 'The Expanse' greatest sci-fi of all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7098e2-6e37-4d61-89f8-bb9bca11afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_expanse_goat_reflect =  await assess_proposition2(\n",
    "    proposition =  \"'The Expanse' sci-fi novel series surpasses all great sci-fi novels other than 'The Expanse'.\", \n",
    "    short_name = 'expanse_goat_reflect',\n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these facts.\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* The Expanse is a series of science fiction novels by James S. A. Corey, the joint pen name of authors Daniel Abraham and Ty Franck.\n",
    "\n",
    "==========\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run=True,\n",
    "    report_to_file = True,\n",
    "    status_updates=True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_expanse_goat_reflect['kb_dot'])\n",
    "_=print_summary(assmnt=assessment_expanse_goat_reflect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a22015-f2ee-4f0a-9939-e760b7a5a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_expanse_goat =  await assess_proposition2(\n",
    "    proposition =  \"'The Expanse' sci-fi novel series surpasses all great sci-fi novels other than 'The Expanse'.\", \n",
    "    short_name = 'expanse_goat', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these facts.\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* The Expanse is a series of science fiction novels by James S. A. Corey, the joint pen name of authors Daniel Abraham and Ty Franck.\n",
    "\n",
    "==========\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    print_level = 0\n",
    ")\n",
    "display(assessment_expanse_goat['kb_dot'])\n",
    "_=print_summary(assmnt=assessment_expanse_goat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23f19d-8e6f-4259-8b8c-ccda33fa7978",
   "metadata": {},
   "source": [
    "## Science & Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc02641-c5ec-45a9-a3d5-4de49ac9babe",
   "metadata": {},
   "source": [
    "#### Low Calorie Density Foods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b2913-bba7-4af7-b925-79c27abfdd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_low_calorie_density =  await assess_proposition2(\n",
    "    proposition =  \"Low calorie density foods are excellent for maintaining a healthy weight.\", \n",
    "    short_name = 'low_calorie_density', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Things to consider:\n",
    "\n",
    "* Provide plenty of examples of low calorie density foods, with explanations for each.\n",
    "* Provide examples of good meal plans with low calorie density foods.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 35,\n",
    "    print_level = 2,\n",
    "    force_run = True\n",
    ")\n",
    "display(assessment_low_calorie_density['kb_dot'])\n",
    "if assessment_low_calorie_density.get('plan_result'):\n",
    "    print_summary(assessment_low_calorie_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948cf22-0bd7-4bd7-8500-73ef9fb1c7ca",
   "metadata": {},
   "source": [
    "#### Fat calorie burn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc1df1e-8d1c-47f3-ab79-4c96b4444057",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_fat_calories =  await assess_proposition2(\n",
    "    proposition =  \"The '500 cal/day Myth' for a 1-pound-a-week weight loss is misleading; so it is better to simply habitually eat low calorie-density foods.\", \n",
    "    short_name = 'fat_calories', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Things to consider:\n",
    "\n",
    "* The 500 cal/day Myth: A pound of fat is equivalent to about 3500 calories, so to lose a pound in a week, you must achieve a caloric deficit of 3500/7 = 500 cal/day relative to the calories calculated for your starting basal metabolic rate.\n",
    "* It is a myth because as you lose weight, your basal metabolic rate (BMR) decreases, so if you set your caloric deficit relative to starting BMR, your rate of weight loss will decrease.\n",
    "* Moreover, counting calories every day is not a sustainable habit for most people; so it is better to establish healthy eating habits rather than counting calories.\n",
    "* What are the best sustainable habits for healthy weight maintenance for a 65-year-old man who is 30-lbs overweight?\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 30,\n",
    "    print_level = 2,\n",
    "    force_run = True\n",
    ")\n",
    "display(assessment_fat_calories['kb_dot'])\n",
    "if assessment_fat_calories.get('plan_result'):\n",
    "    print_summary(assessment_fat_calories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d320e5-7aed-47c1-8ea7-eb9731bbfdc8",
   "metadata": {},
   "source": [
    "#### Deflation due to Generative AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90edb27-1f42-485c-9045-bec08f1cdb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_ai_deflation =  await assess_proposition2(\n",
    "    proposition =  \"Generative AI will lead to significant and widespread deflation in the compensation of white-collar workers.\", \n",
    "    short_name = 'ai_deflation', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 30,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_ai_deflation['kb_dot'])\n",
    "if assessment_ai_deflation.get('plan_result'):\n",
    "    print_summary(assessment_ai_deflation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd83b4-0295-463e-aa15-3ee903efc26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_cheapening_creation =  await assess_proposition2(\n",
    "    proposition =  \"In a capitalist economy, devaluation of the value of creation depresses wages for jobs of all types whether creation-driven (design, engineering, etc.) or production-driven (manufacturing, assembly, service, etc.).\", \n",
    "    short_name = 'cheapening_creation', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Consider the following propositions:\n",
    "\n",
    "* Creative talents set a ceiling and buoy wages for jobs of all types.\n",
    "* Until now, technology across all of history has been a net creator of creation-driven jobs even as it has displaced production-driven jobs.\n",
    "* The new trait of Generative AI as opposed to past technological disruptions is that it is the first technology that can substantially reduce the number of humans needed in creation-driven jobs even moreso than production-driven jobs.\n",
    "* A cheapening of creative assets leads to depression of wages across all jobs.\n",
    "* Downward pressure on wages leads to deflation and, if too rapidly occurring, to economic depression.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 30,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_cheapening_creation['kb_dot'])\n",
    "if assessment_cheapening_creation.get('plan_result'):\n",
    "    print_summary(assessment_cheapening_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868dcb5-4dc8-41aa-8f77-bb4c89e3840b",
   "metadata": {},
   "source": [
    "#### Income inequality exacerbated by Generative AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c6910-7ab4-491c-8a95-3defc1daac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_ai_inequality =  await assess_proposition2(\n",
    "    proposition =  \"Generative AI will exacerbate income inequality.\", \n",
    "    short_name = 'ai_inequality', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 30,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_ai_inequality['kb_dot'])\n",
    "if assessment_ai_inequality.get('plan_result'):\n",
    "    print_summary(assessment_ai_inequality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ce654-ba2b-48e7-bf66-c50f0948f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_ai_inequality2 =  await assess_proposition2(\n",
    "    proposition =  \"Generative AI will exacerbate income inequality.\", \n",
    "    short_name = 'ai_inequality2', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "* Consider changes in wage structure -- not only changes in job distribution.\n",
    "* Attempt to assess degree (quantitative extent) -- not only type (qualitative nature) -- of economic changes.\n",
    "* Consider the rapidity of economic changes relative to the slowness at which governmental policy changes.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 30,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_ai_inequality2['kb_dot'])\n",
    "if assessment_ai_inequality2.get('plan_result'):\n",
    "    print_summary(assessment_ai_inequality2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c20af-1cb0-475c-bc99-0edee765c21e",
   "metadata": {},
   "source": [
    "#### Einstein's theory of relativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ee886-0990-46d1-86f9-b166492f174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_explain_relativity2 =  await assess_proposition2(\n",
    "    proposition =  \"Explain Einstein's theories of relativity.\", \n",
    "    short_name = 'explain_relativity2', \n",
    "    search_cache = search_cache, \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level=2\n",
    ")\n",
    "display(assessment_explain_relativity2['kb_dot'])\n",
    "if assessment_explain_relativity2.get('plan_result'):\n",
    "    print_summary(assessment_explain_relativity2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9048c69a-8e47-450c-9da1-09f9386eda23",
   "metadata": {},
   "source": [
    "#### Richard Feynman -- Physicist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c17433-6831-431d-93ae-4b3307cc2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_feynman_physicist =  await assess_proposition2(\n",
    "    proposition =  \"Richard Feynman was the greatest physicist of his generation.\", \n",
    "    short_name = 'feynman_physicist', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these facts.\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* Richard Feynman was a famous physicist.\n",
    "\n",
    "==========\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9b8c1-a49d-418a-9eb2-b6140e549300",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_feynman_physicist['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29855305-8ebf-46c0-9e0e-413848120e3d",
   "metadata": {},
   "source": [
    "#### Einstein and Hilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47c04b-f27e-4df6-8a73-6499d7746e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_einstein_hilbert =  await assess_proposition2(\n",
    "    proposition =  \"Einstein needed Hilbert to help him understand the mathematics that led to the Theory of General Relativity.\", \n",
    "    short_name = 'einstein_hilbert', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94105c4-fd51-4d8e-8ce5-3a07748366e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_einstein_hilbert['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b179c1-8181-49a1-bbfb-4cf92e80c0ab",
   "metadata": {},
   "source": [
    "#### Explain Branden Fitelsen Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa99dc-95c7-46e9-a6c0-4ded5c8c2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_explain_fitelsen =  await assess_proposition2(\n",
    "    proposition =  \"Explain Bayesian Confirmation Theory, per Branden Fitelsen.\", \n",
    "    short_name = 'explain_fitelsen', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"Especially, thoroughly investigate these topics:\n",
    "    \n",
    "* Bayesian Confirmation Theory in the context of scientific inquiry and the logic of science. \n",
    "* Confirmation as relative to induction and deduction.\n",
    "* Confirmation perspectives with respect to Carnap, Hume, and Howson & Urbach.  \n",
    "* Enumeration and contrast of Bayesian confirmation measures.\n",
    "* Practical application in reasoning and decision-making.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_explain_fitelsen['kb_dot'])\n",
    "if assessment_explain_fitelsen.get('plan_result'):\n",
    "    print_summary(assessment_explain_fitelsen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e450b79-918b-4144-ac7e-1bbe669612a4",
   "metadata": {},
   "source": [
    "#### Explain Homotopy Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f8615-c8d9-43f2-b4ae-0f059909fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_explain_homotopy =  await assess_proposition2(\n",
    "    proposition =  \"Explain homotopy theory.\", \n",
    "    short_name = 'explain_homotopy', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")\n",
    "display(assessment_explain_homotopy['kb_dot'])\n",
    "if assessment_explain_homotopy.get('plan_result'):\n",
    "    print_summary(assessment_explain_homotopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fe722-1ec6-4b40-97fc-0d027981c25f",
   "metadata": {},
   "source": [
    "#### Explain Bayesian belief networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d101fe4-b1bf-43c1-80e4-5ae1d4847064",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_explain_bbn =  await assess_proposition2(\n",
    "    proposition =  \"Explain Bayesian belief networks.\", \n",
    "    short_name = 'explain_bbn', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Do so in a manner that a 7th-grader can understand.\n",
    "Provide many examples in a clear and concise manner.\n",
    "\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")\n",
    "display(assessment_explain_bbn['kb_dot'])\n",
    "_=print_summary(assessment_explain_bbn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8560a0b9-126d-4ddd-907e-e7e70c540d4b",
   "metadata": {},
   "source": [
    "#### Investigate Yuan et al, \"Most Relevant Explanation in Bayesian Networks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab82ea-080b-4616-a16c-44ae9addad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_mre_bbn =  await assess_proposition2(\n",
    "    proposition = 'Investigate \"Most Relevant Explanation in Bayesian Networks\" by Yuan et al.', \n",
    "    short_name = 'mre_bbn', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Do so in a manner that a 7th-grader can understand.\n",
    "Provide many examples in a clear and concise manner.\n",
    "\n",
    "All Query nodes must lead directly or indirectly back to the main proposition (the 'sink' node, which is the only node upon which no other node depends). \n",
    "The query graph must have one and only one \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Consider the work in the context of \n",
    "(a) Explanation - given evidence, find hypotheses that explain it - and \n",
    "(b) Argumentation - given hypothesis, find evidence that supports and refute it.\n",
    "\n",
    "Investigate alternative explanations, i.e., hypotheses, for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\n",
    "Rely upon the <<GIVEN FACTS>> as established truths and resources for further information:\n",
    "==================================\n",
    "<<GIVEN FACTS>>\n",
    "* Yuan et al (Changhe Yuan, Heejin Lim, and Tsai-Ching Lu) published \"Most Relevant Explanation in Bayesian Networks\" at arxiv: https://arxiv.org/abs/1401.3893\n",
    "  - Abstract: \n",
    "    A major inference task in Bayesian networks is explaining why some variables are observed in their particular states using a set of target variables. \n",
    "    Existing methods for solving this problem often generate explanations that are either too simple (underspecified) or too complex (overspecified). In this paper, \n",
    "    we introduce a method called Most Relevant Explanation (MRE) which finds a partial instantiation of the target variables that maximizes the generalized Bayes factor (GBF) \n",
    "    as the best explanation for the given evidence. Our study shows that GBF has several theoretical properties that enable MRE to automatically identify the most relevant \n",
    "    target variables in forming its explanation. In particular, conditional Bayes factor (CBF), defined as the GBF of a new explanation conditioned on an existing explanation, \n",
    "    provides a soft measure on the degree of relevance of the variables in the new explanation in explaining the evidence given the existing explanation. As a result, \n",
    "    MRE is able to automatically prune less relevant variables from its explanation. We also show that CBF is able to capture well the explaining-away phenomenon that is often \n",
    "    represented in Bayesian networks. Moreover, we define two dominance relations between the candidate solutions and use the relations to generalize MRE to find a set of \n",
    "    top explanations that is both diverse and representative. Case studies on several benchmark diagnostic Bayesian networks show that MRE is often able to find explanatory hypotheses \n",
    "    that are not only precise but also concise.\n",
    "==================================\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")\n",
    "display(assessment_mre_bbn['kb_dot'])\n",
    "_=print_summary(assessment_mre_bbn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9795ffe1-e603-4ac7-b7fd-f31fd4e2ee28",
   "metadata": {},
   "source": [
    "#### Bayesian Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf034e-4dfd-4961-9459-f07c8e419583",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_bayesian_taxonomy =  await assess_proposition2(\n",
    "    proposition = 'A taxonomy of abstracted concepts ties together all of the [CONCEPTS] into a cohesive science of Bayesian reasoning under uncertainty.', \n",
    "    short_name = 'bayesian_taxonomy', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Derive a query plan that results in such a Conceptual Taxonomy for the [CONCEPTS] given below.\n",
    "All Query nodes must lead directly or indirectly back to the main proposition (the 'sink' node, which is the only node upon which no other node depends). \n",
    "The query graph must have one and only one \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Consider the work in the context of \n",
    "(a) Explanation - given evidence, find hypotheses that explain it - and \n",
    "(b) Argumentation - given hypothesis, find evidence that supports and refute it.\n",
    "\n",
    "Investigate alternative explanations, i.e., hypotheses, for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\n",
    "Explore the following [CONCEPTS], being sure to cite the literature references that explain them:\n",
    "==================================\n",
    "[CONCEPTS]\n",
    "\n",
    "* Bayesian blocks, e.g., Scargle, with dynamic programming\n",
    "* Bayesian optimal stopping rules with dynamic programming\n",
    "* Bayesian updating in Kalman filters with dynamic programming\n",
    "* Bayesian updating in robotics SLAM (Simultaneous Localization and Mapping) with particle filters\n",
    "* Bayesian optimization for optimal experimental design & search under uncertainty\n",
    "* Bayesian basis of Good-Turing estimation\n",
    "* Bayesian Weight of Evidence in hypothesis confirmation, e.g., I.J. Good, and connection to mutual information by expectation (averaging)\n",
    "* Bayesian hypothesis confirmation theory, e.g., Branden Fitelsen\n",
    "* Bayesian reasoning under uncertainty\n",
    "* Bayesian decision-making under uncertainty\n",
    "\n",
    "==================================\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 30,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_bayesian_taxonomy['kb_dot'])\n",
    "_=print_summary(assessment_bayesian_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a87a51-5f93-4d3b-a76c-9a24fcc0838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dict = ({\"responses\":[{\"task_id\":\"bayesian_blocks\",\"query\":\"Explore Bayesian blocks as an example of dynamic programming and cite literature references.\\n\",\"response\":\"Bayesian blocks is a method for creating histograms with variable-width bins, which is particularly useful when dealing with data that has varying event rates. It is a non-parametric technique that can adaptively determine the optimal binning of data by maximizing a fitness function that is based on a Bayesian prior and likelihood. The method is often used in the analysis of astronomical data, but it can be applied to any type of data that is amenable to histogram representation.\\n\\nThe connection to dynamic programming comes from the algorithm's approach to optimizing the binning. Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems, solving each of these subproblems just once, and storing their solutions - ideally, using a recursive relation. The key feature of dynamic programming is the use of these stored solutions to solve larger subproblems, which avoids the need to recompute the answers to the subproblems multiple times.\\n\\nIn the context of Bayesian blocks, dynamic programming is used to efficiently find the optimal segmentation of the data into blocks (bins). The algorithm iteratively evaluates the fitness of different possible partitions of the data, storing the best solution found for each segment. By building on these solutions, the algorithm can efficiently find the best overall partitioning of the data into blocks.\\n\\nFor literature references, here are some sources that discuss Bayesian blocks and dynamic programming:\\n\\n1. Scargle, J. D., Norris, J. P., Jackson, B., & Chiang, J. (2013). Studies in Astronomical Time Series Analysis. VI. Bayesian Block Representations. The Astrophysical Journal, 764(2), 167. [IOPscience](https://iopscience.iop.org/article/10.1088/0004-637X/764/2/167)\\n   - This paper presents a detailed description of the Bayesian blocks algorithm and its application to astronomical time series analysis.\\n\\n2. VanderPlas, J. T. (2012). Dynamic Programming in Python: Bayesian Blocks. [Jake VanderPlas's blog](https://jakevdp.github.io/blog/2012/09/12/dynamic-programming-in-python/)\\n   - A blog post that explains how dynamic programming is used in the implementation of the Bayesian blocks algorithm in Python.\\n\\n3. AstroML documentation. Bayesian Blocks for Histograms. [astroML 1.0 documentation](https://www.astroml.org/examples/algorithms/plot_bayesian_blocks.html)\\n   - The documentation for the astroML library, which includes an example of using Bayesian blocks for histogramming.\\n\\n4. Astropy documentation. bayesian_blocks â€” Astropy v6.0.0. [Astropy stats documentation](https://docs.astropy.org/en/stable/api/astropy.stats.bayesian_blocks.html)\\n   - The documentation for the Astropy library, which provides an implementation of the Bayesian blocks algorithm.\\n\\n5. Singh, S. P. N., & Moore, A. W. (2005). Bayesian Real-Time Dynamic Programming. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI'05), 3-9. [IJCAI Proceedings](https://www.ijcai.org/Proceedings/09/Papers/297.pdf)\\n   - This paper discusses real-time dynamic programming in the context of Markov decision processes, which is related to the concept of dynamic programming used in Bayesian blocks.\\n\\nThese references provide a starting point for understanding the Bayesian blocks algorithm and its relationship to dynamic programming. The algorithm's efficiency and adaptability make it a powerful tool for data analysis in various fields, particularly when the underlying data structure is complex or unevenly distributed.\"},{\"task_id\":\"bayesian_hypothesis_confirmation\",\"query\":\"Explore Bayesian hypothesis confirmation theory, e.g., Branden Fitelsen and cite literature references.\\n\",\"response\":\"Bayesian confirmation theory is a probabilistic approach to the philosophy of science that deals with how evidence confirms or supports scientific hypotheses. According to this theory, evidence E confirms a hypothesis H if the probability of H given E is higher than the probability of H by itself. This is often expressed using Bayes' theorem, which relates the conditional and marginal probabilities of events.\\n\\nBranden Fitelson is a prominent philosopher who has contributed significantly to the study of Bayesian confirmation theory. His work includes a detailed analysis of how Bayesian principles can be applied to the confirmation of hypotheses and the resolution of paradoxes in the philosophy of science.\\n\\nHere are some key references to Fitelson's work on Bayesian confirmation theory:\\n\\n1. Fitelson, B. (2001). \\\"Studies in Bayesian Confirmation Theory.\\\" PhD thesis, University of Wisconsin--Madison. This thesis is a comprehensive study of Bayesian confirmation theory, discussing various measures of confirmation and addressing issues such as the paradox of the ravens and the problem of old evidence. [Link](https://fitelson.org/thesis.pdf)\\n\\n2. Fitelson, B. (2006). \\\"How Bayesian Confirmation Theory Handles the Paradox of the Ravens.\\\" Philosophy Compass, 1(1), 95â€“113. In this paper, Fitelson discusses the famous paradox of the ravens and how Bayesian confirmation theory can provide a resolution. [Link](https://philpapers.org/rec/HAWHBC)\\n\\n3. Fitelson, B., & Waterman, A. (2005). \\\"Bayesian Confirmation and Auxiliary Hypotheses Revisited.\\\" Journal of Philosophy, 102(7), 298-315. This paper revisits the problem of auxiliary hypotheses in the context of Bayesian confirmation theory. [Link](https://www.jstor.org/stable/3541703)\\n\\n4. Fitelson, B. (1999). \\\"The Plurality of Bayesian Measures of Confirmation and the Problem of Measure Sensitivity.\\\" Philosophy of Science, 66(S1), S362-S378. This paper discusses the issue of having multiple non-equivalent Bayesian measures of confirmation and the problem this poses for the theory. [Link](https://www.jstor.org/stable/188784)\\n\\n5. Fitelson, B. (2001). \\\"Studies in Bayesian Confirmation Theory.\\\" This book provides an in-depth look at Bayesian confirmation theory, including discussions on various measures of confirmation and their implications. [Link](https://books.google.com/books/about/Studies_in_Bayesian_Confirmation_Theory.html?id=M64IAQAAMAAJ)\\n\\nThese references provide a solid foundation for understanding Fitelson's contributions to Bayesian confirmation theory. His work has been influential in clarifying how evidence can incrementally confirm hypotheses and in addressing various philosophical problems through a Bayesian lens.\"},{\"task_id\":\"bayesian_optimization\",\"query\":\"Explore Bayesian Optimization of expensive, uncertain objective functions as applied to experimental design and cite literature references.\\n\",\"response\":\"Bayesian Optimization (BO) is a powerful strategy for the optimization of expensive and uncertain objective functions, which is particularly useful in experimental design where each experiment can be time-consuming and costly. The goal of BO is to find the maximum (or minimum) of an objective function efficiently, which means with as few evaluations as possible. This is achieved by building a probabilistic model of the objective function and using it to make decisions about where to sample next.\\n\\nHere's a step-by-step logical reasoning on how Bayesian Optimization can be applied to experimental design, along with literature references:\\n\\n1. **Define the Objective Function**: In experimental design, the objective function typically represents the outcome of an experiment that depends on certain input parameters. The function is often unknown or partially known, and direct evaluations are expensive (e.g., time, resources).\\n\\n2. **Choose a Prior**: Bayesian Optimization starts with a prior over the objective function, which captures beliefs about the behavior of the function before any data is collected. Gaussian Processes (GPs) are commonly used as priors due to their flexibility and tractability (Rasmussen & Williams, 2006, \\\"Gaussian Processes for Machine Learning\\\").\\n\\n3. **Acquire Data**: Initial data is collected, often through a small number of random experiments, to update the prior and create a posterior distribution.\\n\\n4. **Update the Model**: The GP model is updated with the new data, which involves computing the posterior distribution of the function's values at new points given the observed data.\\n\\n5. **Select Next Sample Point**: A new sample point is chosen by optimizing an acquisition function, which balances exploration (sampling where the model is uncertain) and exploitation (sampling where the model predicts high values). Common acquisition functions include Expected Improvement, Probability of Improvement, and Upper Confidence Bound.\\n\\n6. **Iterate**: Steps 3-5 are repeated, each time updating the model with the new data, until a stopping criterion is met (e.g., a maximum number of iterations, convergence of the objective function value, or exhaustion of resources).\\n\\n7. **Make Decisions**: The results of Bayesian Optimization can inform decisions about the experimental design, such as the optimal settings of the experimental parameters.\\n\\nLiterature references that discuss Bayesian Optimization in the context of experimental design include:\\n\\n- Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & de Freitas, N. (2016). \\\"Taking the human out of the loop: A review of Bayesian optimization.\\\" Proceedings of the IEEE, 104(1), 148-175. This paper provides a comprehensive review of Bayesian optimization, including its application to experimental design.\\n\\n- Frazier, P. I. (2018). \\\"A tutorial on Bayesian optimization.\\\" arXiv preprint arXiv:1807.02811. This tutorial covers the basics of Bayesian optimization and includes discussion on experimental design.\\n\\n- Snoek, J., Larochelle, H., & Adams, R. P. (2012). \\\"Practical Bayesian optimization of machine learning algorithms.\\\" Advances in neural information processing systems, 25. While focused on machine learning, this paper provides insights into the practical application of Bayesian optimization that can be applied to experimental design.\\n\\n- Garnett, R., Osborne, M. A., & Roberts, S. J. (2010). \\\"Bayesian optimization for sensor set selection.\\\" Proceedings of the 9th ACM/IEEE International Conference on Information Processing in Sensor Networks. This paper discusses the use of Bayesian optimization for selecting sensor sets, which is a form of experimental design.\\n\\nThe literature references provided in the context above also offer valuable insights into the application of Bayesian Optimization to experimental design:\\n\\n- \\\"Bayesian Optimization for Adaptive Experimental Design: A Review\\\" (https://ieeexplore.ieee.org/document/8957442) provides a review of Bayesian optimization as a statistical method for modeling and optimizing expensive black-box functions, which is relevant to experimental design.\\n\\n- The arXiv preprint \\\"Bayesian Optimisation for Sequential Experimental Design\\\" (https://arxiv.org/pdf/2107.12809) discusses the approach to globally optimizing black-box objective functions that are expensive to evaluate, which is a key challenge in experimental design.\\n\\n- \\\"Bayesian Experimental Design: A Review\\\" (http://homepage.divms.uiowa.edu/~gwoodwor/AdvancedDesign/Chaloner%20Verdinelli.pdf) reviews the literature on Bayesian experimental design, providing a unified view based on a decision-theoretic approach.\\n\\n- The Wikipedia page on \\\"Bayesian experimental design\\\" (https://en.wikipedia.org/wiki/Bayesian_experimental_design) provides a general overview of the framework and its theoretical basis.\\n\\nThese references collectively provide a solid foundation for understanding the application of Bayesian Optimization to experimental design, highlighting its advantages in efficiently navigating the trade-off between exploration and exploitation to optimize experimental outcomes.\"},{\"task_id\":\"good_turing_estimation\",\"query\":\"Explore Good-Turing estimation with respect to derivation from Bayesian principles and cite literature references.\\n\",\"response\":\"The Good-Turing estimation is a statistical technique originally developed by I.J. Good and Alan Turing during World War II for cryptanalysis purposes. It has since been widely used in various fields, such as ecology, linguistics, and information theory, for estimating the probability of encountering an unseen event given a set of observed data. The method is particularly useful when dealing with sparse data.\\n\\nThe Good-Turing estimator adjusts the observed frequencies of events to better estimate the true frequencies, including those events that have not been observed (i.e., have zero frequency in the sample). The key idea is to reassign some of the probability mass from observed to unobserved events.\\n\\nFrom a Bayesian perspective, the Good-Turing estimation can be derived by considering a nonparametric Bayesian model. In such a model, one assumes a prior distribution over the possible frequencies of different events, and then updates this distribution in light of the observed data. The Good-Turing estimate can be seen as an approximation to the Bayesian posterior predictive distribution of the frequency of an unobserved event.\\n\\nThe literature references provided in the context suggest that there has been work on rediscovering Good-Turing estimators through Bayesian nonparametrics. For example, the paper titled \\\"Rediscovery of Goodâ€“Turing estimators via Bayesian Nonparametrics\\\" investigates the relationship between the Good-Turing approach and Bayesian nonparametric methods. This paper can be found on JSTOR (https://www.jstor.org/stable/24738771), PubMed (https://pubmed.ncbi.nlm.nih.gov/26224325/), arXiv (https://arxiv.org/abs/1401.0303), and the Oxford Academic website (https://academic.oup.com/biometrics/article/72/1/136/7511697).\\n\\nThe Bayesian nonparametric approach typically involves using Dirichlet Process or Pitman-Yor Process priors, which allow for an infinite number of possible events (species, words, etc.). The Good-Turing estimator can be derived as a special case or approximation within these Bayesian nonparametric frameworks.\\n\\nTo summarize, the Good-Turing estimation can be connected to Bayesian principles by considering it as an approximation to the Bayesian posterior predictive distribution for unseen events, derived within a nonparametric Bayesian framework. The literature references provided offer a deeper exploration of this connection and the mathematical derivations involved.\"},{\"task_id\":\"kalman_filters\",\"query\":\"Explore Kalman filters and smoothing with respect to derivation from Bayesian principles and as an example of dynamic programming and cite literature references.\\n\",\"response\":\"Kalman filters are a class of filters that are used for estimating the state of a dynamic system over time. They are based on Bayesian principles and can be seen as an example of dynamic programming. Here's a step-by-step logical reasoning that connects Kalman filters to Bayesian principles and dynamic programming, along with literature references:\\n\\n1. **Bayesian Principles:**\\n   - Kalman filters operate on the Bayesian principle of recursive estimation. At each time step, the filter takes a prediction of the system's state and updates it with new measurement data to produce a posterior estimate.\\n   - The prediction step is based on a prior model of the system's evolution, while the update step incorporates new data using Bayes' theorem to revise the belief about the system's state.\\n   - The Kalman filter assumes that both the system dynamics and the measurements are subject to Gaussian noise, which allows for closed-form expressions for the prediction and update steps.\\n\\n   **Literature Reference:**\\n   - \\\"Kalman filter demystified: from intuition to probabilistic graphical models\\\" (https://arxiv.org/pdf/1811.11618) provides an intuitive explanation of the Kalman filter from a Bayesian perspective.\\n\\n2. **Dynamic Programming:**\\n   - Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems. It is applicable to problems exhibiting the properties of overlapping subproblems and optimal substructure.\\n   - The Kalman filter can be seen as an example of dynamic programming because it solves the filtering problem in a recursive manner. Each step of the filter computes an optimal estimate of the current state based on the previous estimate and the latest measurement, which is analogous to the principle of optimality in dynamic programming.\\n\\n   **Literature Reference:**\\n   - \\\"The Kalman Filter and Related Algorithms: A Literature Review\\\" (https://www.researchgate.net/publication/236897001_The_Kalman_Filter_and_Related_Algorithms_A_Literature_Review) discusses the Kalman filter in the context of various algorithms, which can be related to dynamic programming.\\n\\n3. **Kalman Smoothing:**\\n   - Kalman smoothing refers to the process of improving the state estimates obtained by a Kalman filter by incorporating future measurements. This is typically done in a batch process, where the entire sequence of measurements is processed to refine the state estimates.\\n   - Smoothing can also be derived from Bayesian principles, as it involves computing the posterior distribution of the state over the entire time series of measurements.\\n\\n   **Literature Reference:**\\n   - \\\"BAYESIAN FILTERING AND SMOOTHING\\\" (https://users.aalto.fi/~ssarkka/pub/cup_book_online_20131111.pdf) provides an introduction to non-linear Kalman filtering and smoothing from a Bayesian perspective.\\n\\n4. **Further Reading:**\\n   - For a comprehensive understanding of the Kalman filter's derivation from Bayesian principles, \\\"Advanced Kalman Filtering Approaches to Bayesian State Estimation\\\" (https://www.diva-portal.org/smash/get/diva2:1077486/FULLTEXT01.pdf) can be a valuable resource.\\n   - The Wikipedia page on Kalman filters (https://en.wikipedia.org/wiki/Kalman_filter) provides a broad overview and links to various extensions of the Kalman filter, which can be useful for understanding its relationship to Bayesian principles and dynamic programming.\\n\\nIn summary, Kalman filters are deeply rooted in Bayesian principles, using prior knowledge and observed data to make inferences about the state of a system. The recursive nature of the Kalman filter aligns with the principles of dynamic programming, where each step optimizes the state estimate based on the previous step and new information. The literature provided offers a detailed exploration of these concepts.\"},{\"task_id\":\"optimal_stopping_rules\",\"query\":\"Explore optimal stopping rules with respect to derivation from Bayesian principles and as an example of dynamic programming and cite literature references.\\n\",\"response\":\"Optimal stopping rules are a fundamental concept in decision theory and statistics, particularly in the context of sequential analysis and dynamic programming. The goal of an optimal stopping rule is to determine the best time to take a particular action to maximize an expected payoff or to minimize an expected cost, based on sequentially observed random variables.\\n\\nFrom a Bayesian perspective, optimal stopping involves updating beliefs about an uncertain process as new information becomes available and making a decision about when to stop based on these updated beliefs. Bayesian principles provide a framework for incorporating prior knowledge and evidence into the decision-making process.\\n\\nDynamic programming, on the other hand, is a method for solving complex problems by breaking them down into simpler subproblems. It is particularly useful in the context of optimal stopping because it allows for the problem to be solved recursively, considering the value of stopping at each point in time versus the expected value of continuing and making a decision later.\\n\\nLet's explore some literature references that discuss optimal stopping rules, Bayesian principles, and dynamic programming:\\n\\n1. \\\"Optimal Stopping and Applications\\\" by Thomas S. Ferguson (https://www.math.ucla.edu/~tom/Stopping/sr1.pdf) provides a comprehensive overview of the theory of optimal stopping, including its applications in various fields. The document discusses the connection between stopping rule problems and sequential statistical analysis as developed by Wald.\\n\\n2. \\\"Sequential Analysis and Optimal Stopping\\\" available on JSTOR (https://www.jstor.org/stable/40749860) discusses the development of dynamic programming in conjunction with the theory of sequential analysis, which is closely related to optimal stopping problems.\\n\\n3. The Wikipedia page on \\\"Optimal Stopping\\\" (https://en.wikipedia.org/wiki/Optimal_stopping) provides a general overview of the theory, including its mathematical foundations and applications.\\n\\n4. \\\"Optimal Stopping Rules\\\" on ResearchGate (https://www.researchgate.net/publication/304179768_Optimal_Stopping_Rules) discusses the problem of choosing a time to take a given action based on sequentially observed random variables in the context of optimal control literature.\\n\\n5. \\\"Optimal Stopping Methods for Investment Decisions: A Literature Review\\\" published on MDPI (https://www.mdpi.com/2227-7072/10/4/96) reviews the use of optimal stopping methods in investment decisions, describing the stochastic models used to represent stock market trends.\\n\\n6. \\\"Dynamic Programming Principles for Optimal Stopping Problems\\\" on arXiv (https://arxiv.org/abs/1708.02192) analyzes an optimal stopping problem with a constraint on the expected cost, highlighting the use of dynamic programming principles.\\n\\n7. \\\"Optimal Stopping with Multiple Priors\\\" on JSTOR (https://www.jstor.org/stable/40263845) discusses the classical theory of optimal stopping and its reliance on martingale theory, which is a probabilistic model of a fair game against nature.\\n\\n8. \\\"On optimal stopping rules\\\" in the journal \\\"Probability Theory and Related Fields\\\" (https://link.springer.com/article/10.1007/BF00535296) discusses the Bayesian and minimax solutions of sequential decision problems, which are relevant to optimal stopping.\\n\\n9. \\\"Approximate Dynamic Programming With Correlated Bayesian Beliefs\\\" (https://optimallearning.princeton.edu/Papers/RyzhovPowell-kgdp_AllertonconferenceJune142010.pdf) discusses the representation of uncertainty about the value function using a Bayesian model in the context of approximate dynamic programming.\\n\\n10. \\\"Optimal stopping, incomplete information, and stochastic games\\\" (https://uu.diva-portal.org/smash/get/diva2:1777701/FULLTEXT01.pdf) extends the classical Bayesian sequential decision framework to include optimal stopping and stochastic games.\\n\\nThese references provide a solid foundation for understanding the principles and applications of optimal stopping rules, as well as their derivation from Bayesian principles and their implementation using dynamic programming techniques.\"},{\"task_id\":\"robotics_slam\",\"query\":\"Explore Robotics SLAM with particle filters with respect to derivation from Bayesian principles and cite literature references.\\n\",\"response\":\"Simultaneous Localization and Mapping (SLAM) is a critical problem in robotics, where a robot aims to build a map of an unknown environment while simultaneously determining its location within that map. Particle filters, also known as Sequential Monte Carlo methods, are a popular choice for solving the SLAM problem due to their ability to handle non-linear models and non-Gaussian noise.\\n\\nThe particle filter approach to SLAM is derived from Bayesian principles, which provide a probabilistic framework for state estimation. In Bayesian terms, the goal is to compute the posterior distribution of the robot's path and the map given all the available data (sensor measurements and control inputs). The particle filter approximates this posterior distribution using a set of particles, where each particle represents a hypothesis of the robot's trajectory and the map.\\n\\nHere's a step-by-step logical reasoning of how particle filters are derived from Bayesian principles in the context of SLAM:\\n\\n1. **Bayesian Filtering**: The foundation of particle filters lies in Bayesian filtering, which updates the belief about the state of a system as new data becomes available. The belief is represented as a probability distribution over possible states.\\n\\n2. **Recursive Estimation**: Bayesian filtering is recursive and consists of two main steps: prediction and update. The prediction step uses the system model to propagate the belief over time, while the update step incorporates new measurements to refine the belief.\\n\\n3. **Non-parametric Representation**: Particle filters use a non-parametric representation of the probability distribution, which means they do not assume any specific form (like Gaussian) for the distribution. Instead, they represent the distribution as a set of samples (particles).\\n\\n4. **Importance Sampling**: Particle filters employ importance sampling to approximate the posterior distribution. Each particle is drawn from a proposal distribution and is assigned a weight based on how well it explains the observed data.\\n\\n5. **Resampling**: To avoid the problem of degeneracy, where all but a few particles have negligible weight, particle filters periodically perform resampling. This process generates a new set of particles by sampling with replacement from the current set, biased towards particles with higher weights.\\n\\n6. **Rao-Blackwellization**: In the context of SLAM, particle filters can be combined with Rao-Blackwellization to reduce variance and computational cost. This technique analytically computes the conditional distribution of the map given the trajectory, which is represented by the particles.\\n\\nThe literature provides extensive coverage of these principles and their application to SLAM:\\n\\n- Thrun, S. (2002). Particle Filters in Robotics. In Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence (UAI2002). Retrieved from http://robots.stanford.edu/papers/thrun.pf-in-robotics-uai02.pdf\\n- Doucet, A., de Freitas, N., & Gordon, N. (2001). An Introduction to Sequential Monte Carlo Methods. In Sequential Monte Carlo Methods in Practice. Retrieved from https://www.stats.ox.ac.uk/~doucet/doucet_defreitas_gordon_SMCbookintro.pdf\\n- Montemerlo, M., Thrun, S., Koller, D., & Wegbreit, B. (2002). FastSLAM: A Factored Solution to the Simultaneous Localization and Mapping Problem. In AAAI/IAAI. Retrieved from http://robots.stanford.edu/papers/Montemerlo02a.pdf\\n- Arulampalam, M. S., Maskell, S., Gordon, N., & Clapp, T. (2002). A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking. IEEE Transactions on Signal Processing, 50(2), 174-188. Retrieved from http://liu.diva-portal.org/smash/get/diva2:360880/FULLTEXT01\\n\\nThese references provide a comprehensive understanding of how particle filters are derived from Bayesian principles and their application in the SLAM problem within robotics.\"},{\"task_id\":\"weight_of_evidence\",\"query\":\"Explore Weight of Evidence in hypothesis confirmation, e.g., I.J. Good, and connection to mutual information by expectation (averaging) and cite literature references.\\n\",\"response\":\"The concept of \\\"Weight of Evidence\\\" (WoE) in hypothesis confirmation is a measure of how much a piece of evidence supports or contradicts a hypothesis. It was extensively discussed by the British statistician I.J. Good, who contributed significantly to Bayesian statistics and the philosophy of probability.\\n\\nIn Good's framework, the weight of evidence is often quantified as the logarithm of the likelihood ratio. If we have two hypotheses, H1 and H2, and evidence E, the weight of evidence provided by E in favor of H1 over H2 is given by:\\n\\nWoE = log(P(E|H1)/P(E|H2))\\n\\nwhere P(E|H1) is the probability of observing the evidence E given that hypothesis H1 is true, and P(E|H2) is the probability of observing E given that H2 is true.\\n\\nThis concept is closely related to mutual information, which in information theory is a measure of the amount of information that one random variable contains about another random variable. Mutual information can be seen as an expectation (average) of the weight of evidence that an observation provides about the state of the world.\\n\\nThe connection between weight of evidence and mutual information can be understood through the lens of Bayesian inference. Mutual information quantifies the reduction in uncertainty (or increase in certainty) about one random variable due to the knowledge of another. In the context of hypothesis testing, mutual information can be seen as the expected weight of evidence across all possible observations.\\n\\nFor literature references, you can look at the following sources:\\n\\n1. Good, I. J. (1960). \\\"Weight of evidence, corroboration, explanatory power, information and the utility of experiments\\\". Journal of the Royal Statistical Society. Series B (Methodological), 22(3), 319-331. This paper discusses the concept of weight of evidence in the context of hypothesis confirmation and its relation to information theory.\\n\\n2. Good, I. J. (1950). \\\"Probability and the Weighing of Evidence\\\". Charles Griffin, London. This book by Good provides a comprehensive look at the use of probability in evaluating evidence and making decisions.\\n\\n3. Cover, T. M., & Thomas, J. A. (2006). \\\"Elements of Information Theory\\\" (2nd ed.). Wiley-Interscience. This textbook includes a discussion of mutual information and its properties, which are relevant to understanding the connection to weight of evidence.\\n\\n4. Kullback, S., & Leibler, R. A. (1951). \\\"On Information and Sufficiency\\\". The Annals of Mathematical Statistics, 22(1), 79-86. This paper introduces the Kullback-Leibler divergence, which is closely related to the concept of weight of evidence.\\n\\n5. MacKay, D. J. C. (2003). \\\"Information Theory, Inference, and Learning Algorithms\\\". Cambridge University Press. This book provides an introduction to information theory with a focus on inference and learning algorithms, which are relevant to the concept of weight of evidence.\\n\\nThese references should provide a solid foundation for understanding the weight of evidence in hypothesis confirmation and its connection to mutual information.\"}]})\n",
    "print(sum([len(cq['response']) for cq in context_dict['responses']]))\n",
    "print([len(cq['response'].split(' ')) > 850/1.75 for cq in context_dict['responses']])\n",
    "\n",
    "cqs = [ComputeQuery(**cq_dict) for cq_dict in context_dict['responses']]\n",
    "total_chars = sum([ len(cq.response) for cq in cqs ])\n",
    "if total_chars > 20000:\n",
    "    cqs_truncated = []\n",
    "    for cq in cqs:\n",
    "        response_split = cq.response.split(' ')\n",
    "        if len(response_split) > 485: # If too long, truncate response\n",
    "            print(f\"\\t\\t... truncating id={cq.task_id} ...\")\n",
    "            response_split = response_split[:485] + ['...']\n",
    "        cq.response = ' '.join(response_split)\n",
    "        cqs_truncated.append(cq)\n",
    "    cqs = cqs_truncated\n",
    "total_chars_new = sum([ len(cq.response) for cq in cqs ])\n",
    "print({'total_chars': total_chars,'total_chars_new': total_chars_new})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc90af6e-7350-4418-8c82-7bb8568df338",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_bayesian_taxonomy_new =  await assess_proposition2(\n",
    "    proposition = 'A Conceptual Taxonomy of abstracted concepts ties together all of the [CONCEPTS] into a cohesive field of scientific inquiry.', \n",
    "    short_name = 'bayesian_taxonomy_new', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Derive a query plan that results in a Conceptual Taxonomy for the [CONCEPTS] given below.\n",
    "Be sure to leverage the `search_query` field of `Query` nodes by assigning `search_query` to a list that has a query string for each of the [CONCEPTS].\n",
    "\n",
    "Explore the following [CONCEPTS], being sure to cite the literature references that explain them:\n",
    "\n",
    "==================================\n",
    "\n",
    "[CONCEPTS]\n",
    "\n",
    "* Bayesian blocks, e.g., Scargle, (explored as an example of dynamic programming)\n",
    "* Optimal stopping rules (explored with respect to derivation from Bayesian principles and as an example of dynamic programming)\n",
    "* Kalman filters and smoothing (explored with respect to derivation from Bayesian principles and as an example of dynamic programming)\n",
    "* Robotics SLAM (Simultaneous Localization and Mapping) with particle filters (explored with respect to derivation from Bayesian principles)\n",
    "* Bayesian Optimization of expensive, uncertain objective functions as applied to experimental design\n",
    "* Good-Turing estimation (explored with respect to derivation from Bayesian principles)\n",
    "* Bayesian hypothesis confirmation theory, e.g., Branden Fitelsen\n",
    "* Weight of Evidence in hypothesis confirmation, e.g., I.J. Good, and connection to mutual information by expectation (averaging)\n",
    "\n",
    "==================================\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = False,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 30,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_bayesian_taxonomy_new['kb_dot'])\n",
    "if assessment_bayesian_taxonomy_new.get('plan_result'):\n",
    "    _=print_summary(assessment_bayesian_taxonomy_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4570ca-c2f8-4718-aec2-84f8a5d03a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_bayesian_taxonomy_improve =  await assess_proposition2(\n",
    "    proposition = 'A Conceptual Taxonomy of abstracted concepts ties together all of the [CONCEPTS] into a cohesive field of scientific inquiry.', \n",
    "    short_name = 'bayesian_taxonomy_improve', \n",
    "    improvement_round = True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Derive a query plan that results in a Conceptual Taxonomy for the [CONCEPTS] given below.\n",
    "Be sure to leverage the `search_query` field of `Query` nodes by assigning `search_query` to a list that has a query string for each of the [CONCEPTS].\n",
    "\n",
    "Explore the following [CONCEPTS], being sure to cite the literature references that explain them:\n",
    "\n",
    "==================================\n",
    "\n",
    "[CONCEPTS]\n",
    "\n",
    "* Bayesian blocks, e.g., Scargle, (explored as an example of dynamic programming)\n",
    "* Optimal stopping rules (explored with respect to derivation from Bayesian principles and as an example of dynamic programming)\n",
    "* Kalman filters and smoothing (explored with respect to derivation from Bayesian principles and as an example of dynamic programming)\n",
    "* Robotics SLAM (Simultaneous Localization and Mapping) with particle filters (explored with respect to derivation from Bayesian principles)\n",
    "* Bayesian Optimization of expensive, uncertain objective functions as applied to experimental design\n",
    "* Good-Turing estimation (explored with respect to derivation from Bayesian principles)\n",
    "* Bayesian hypothesis confirmation theory, e.g., Branden Fitelsen\n",
    "* Weight of Evidence in hypothesis confirmation, e.g., I.J. Good, and connection to mutual information by expectation (averaging)\n",
    "\n",
    "==================================\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = False,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 30,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_bayesian_taxonomy_improve['kb_dot'])\n",
    "if assessment_bayesian_taxonomy_improve.get('plan_result'):\n",
    "    _=print_summary(assessment_bayesian_taxonomy_improve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ec4e0-931b-4042-977b-b82b619e64fb",
   "metadata": {},
   "source": [
    "#### Explain I.J. Good's Weight of Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be72c4-cfcc-4ae3-9c4a-95da2a996670",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_explain_wt_evidence =  await assess_proposition2(\n",
    "    proposition =  \"Explain I.J. Good's Weight of Evidence for Hypothesis Confirmation.\", \n",
    "    short_name = 'explain_wt_evidence', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Do so in a manner that a 7th-grader can understand.\n",
    "Provide many examples in a clear and concise manner.\n",
    "\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Be sure to leverage internet searches while investigating the following:\n",
    "* Definition of I.J. Good's Weight of Evidence.\n",
    "* The connection of Good's Weight of Evidence to other concepts: Bayes Factor, Likelihood Ratio, Kullback-Leibler Divergence, etc.\n",
    "* Related issues like Branden Fitelsen's work on Hypothesis Confirmation Theory.\n",
    "* Applications like Yuan et al. \"Most Relevant Explanation in Bayesian Networks\".\n",
    "* Alternative approaches to estimating the strength of evidence with respect to a hypothesis.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = 'gpt-3.5-turbo', #\"gpt-4-turbo-preview\", \n",
    "    #use_plan = assessment_explain_wt_evidence['plan'],\n",
    "    dynamic = True,\n",
    "    force_run = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_explain_wt_evidence['kb_dot'])\n",
    "_=print_summary(assessment_explain_wt_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26973b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_explain_wt_evidence_gpt4 =  await assess_proposition2(\n",
    "    proposition =  \"Explain I.J. Good's Weight of Evidence.\", \n",
    "    short_name = 'explain_wt_evidence_gpt4', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Do so in a manner that a 7th-grader can understand.\n",
    "Provide many examples in a clear and concise manner.\n",
    "\n",
    "Be sure to leverage internet searches while investigating the following:\n",
    "\n",
    "* Definition of I.J. Good's Weight of Evidence.\n",
    "* The connection of Good's Weight of Evidence to other concepts: Bayes Factor, Likelihood Ratio, Kullback-Leibler Divergence, etc.\n",
    "* Related issues like Branden Fitelsen's work on Hypothesis Confirmation Theory.\n",
    "* Applications like Yuan et al. \"Most Relevant Explanation in Bayesian Networks\".\n",
    "* Alternative approaches to estimating the strength of evidence with respect to a hypothesis.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    #use_plan = assessment_explain_wt_evidence['plan'],\n",
    "    force_run = False,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 24,\n",
    "    print_level = 2\n",
    ")\n",
    "display(visualize_knowledge_graph(convert_to_graph(assessment_explain_wt_evidence_gpt4['plan0'],proposition='copilotkt_gamechanger')))\n",
    "display(assessment_explain_wt_evidence_gpt4['kb_dot'])\n",
    "if assessment_explain_wt_evidence_gpt4.get('plan_result'):\n",
    "    _ = print_summary(assessment_explain_wt_evidence_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37a7ff-4a4c-4842-a2f6-3e48b2a8333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_explain_wt_evidence_gpt4['plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2760d-53a0-48d9-b8ae-29a295af281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rprt = generate_report(assessment_explain_wt_evidence_gpt4['plan'].query_plan_outline,list(assessment_explain_wt_evidence_gpt4['plan_result'].values())[-1],assessment_explain_wt_evidence_gpt4['plan_result'],{q.id: q.dependencies for q in assessment_explain_wt_evidence_gpt4['plan'].query_graph},search_cache,'explain_woe', True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992af4b-8613-416b-abb7-66d18771009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rprt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e32e8-a84e-4216-a9d5-64e5edfec0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_processing(assessment_explain_wt_evidence_gpt4['plan'],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574dfe9-7b42-4e9c-af04-988cc1600254",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_explain_wt_evidence['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8834f-d183-4eff-a10d-e1b5dba3f61f",
   "metadata": {},
   "source": [
    "#### Explain Real Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1be28-8f28-416f-bf90-320138b8aff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_explain_real_analysis =  await assess_proposition2(\n",
    "    proposition =  \"Explain Real Analysis.\", \n",
    "    short_name = 'explain_real_analysis', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Do so in a manner that a 7th-grader can understand.\n",
    "Provide many examples in a clear and concise manner.\n",
    "\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate textbooks, such as 'Baby Rudin'.\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d605d-aa82-4758-838a-fd9ee060f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_explain_real_analysis['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a5df08-556a-461e-b335-ae5cd080cf5b",
   "metadata": {},
   "source": [
    "#### Explain Hilbert Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ed72e-9548-48e9-b165-ffd2a8362b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_hilbert_spaces =  await assess_proposition2(\n",
    "    proposition =  \"Explain Hilbert spaces.\", \n",
    "    short_name = 'hilbert_spaces', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Do so in a manner that a 7th-grader can understand.\n",
    "Provide many examples in a clear and concise manner.\n",
    "\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3579ce9-5fd3-4a31-816a-67730c50793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_hilbert_spaces['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c6c11b-f97b-4540-a24f-6944e999bcfa",
   "metadata": {},
   "source": [
    "#### Comparison -- Gemini Ultra vs GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa8fe8-5eb7-4eb5-a53c-a0b99edffb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_compare_llm =  await assess_proposition2(\n",
    "    proposition =  \"GPT-4 outperforms Gemini Ultra.\", \n",
    "    short_name = 'compare_llm', \n",
    "    search_cache = search_cache, \n",
    "    improvement_round=True,\n",
    "    force_run=False,\n",
    "    added_instructions = \"\"\"\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these facts.\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* OpenAI developed GPT-4.\n",
    "* Google developed Gemini Ultra.\n",
    "\n",
    "==========\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level=2\n",
    ")\n",
    "display(assessment_compare_llm['kb_dot'])\n",
    "if assessment_compare_llm.get('plan_result'):\n",
    "    print_summary(assessment_compare_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9765c01-6ab1-462f-b5f7-cdda80034000",
   "metadata": {},
   "source": [
    "#### Gravity & Reality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4542c8-3437-486c-8b28-646e16fa0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_gravity_reality =  await assess_proposition2(\n",
    "    proposition =  \"Gravity, when at the extremes of mass, force, and speed, bends reality in a way such that humans perceive the world as magical phenomena -- incomprehensible in the context of everyday life.\", \n",
    "    short_name = 'gravity_reality', \n",
    "    use_plan = None,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes either lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\n",
    "Accept the <<GIVEN FACTS>> as truths that are not to be questioned. DO NOT create Query nodes for any of the <<GIVEN FACTS>>.\n",
    "\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* Einstein's Theory of Relativity.\n",
    "\n",
    "==========\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f0e60e-1274-472b-92af-54075fdbb50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_gravity_reality['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431dab7b-bf32-4fd9-9354-3dbbfad20d78",
   "metadata": {},
   "source": [
    "#### Obesity & Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c753a9-60a8-405c-a793-887a64a342fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_obesity_diabetes =  await assess_proposition(\n",
    "    proposition = \"The best management of obesity and diabetes -- in terms of being efficacious and likely to be sustained as a long-term lifestyle -- is a low-carb diet and mild exercise.\", \n",
    "    short_name = 'obesity_diabetes', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\",\n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab81f1-35eb-4fae-ba41-c71ee20600aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_obesity_diabetes['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a8035a-42c9-4701-9f90-fcc05fd5881c",
   "metadata": {},
   "source": [
    "#### Japan in Generative AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfc064-8a83-4ea6-b392-e357af173f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_japan_ai =  await assess_proposition(\n",
    "    proposition = \"Japan is far behind America, China, and Europe in the race to deploy Generative AI.\", \n",
    "    short_name = 'japan_ai', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1810e6-69d1-41ae-88d2-d15586b7644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_japan_ai['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb17f1-f5f4-4f42-8955-46666e519259",
   "metadata": {},
   "source": [
    "#### Generative AI is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9440d21-a473-49be-a756-5a707b58d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_generative_ai_different =  await assess_proposition2(\n",
    "    proposition =  \"Generative AI will have impacts on society that are fundamentally different and more profound than previous technologies because it surpasses human creativity.\", \n",
    "    short_name = 'generative_ai_different', \n",
    "    #use_plan = ai_plan,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence against as well as for each subproposition.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb400d-f7b2-412b-b403-bab04580d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_generative_ai_different = pickle_it({'short_name' : 'generative_ai_different'}, save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91d2a7-ca69-4669-940a-1200a5dc63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_generative_ai_different2 =  await assess_proposition2(\n",
    "    proposition =  \"Generative AI will have impacts on society that are fundamentally different and more profound than previous technologies because it surpasses human creativity.\", \n",
    "    short_name = 'generative_ai_different2', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 30,\n",
    "    force_run = False,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_generative_ai_different2['kb_dot'])\n",
    "if assessment_generative_ai_different2.get('plan_result'):\n",
    "    _ = print_summary( assessment_generative_ai_different2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a12cc-8ff7-48e6-846c-2ff4909bc1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the pattern to match the context with possible nested occurrences\n",
    "pattern = r\"Context: ##`(.+)`##\"\n",
    "\n",
    "def extract_dictionaries(text):\n",
    "    # List to hold all dictionary representations\n",
    "    dicts = []\n",
    "    \n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.finditer(pattern, text)\n",
    "    for match in matches:\n",
    "        # Extract the dictionary string\n",
    "        dict_str = match.group(1)\n",
    "        print(dict_str)\n",
    "        \n",
    "        # Check if the dictionary string itself contains nested patterns\n",
    "        if re.search(pattern, dict_str):\n",
    "            # If nested patterns exist, recurse and append the result\n",
    "            print(50*'=')\n",
    "            dicts.append(extract_dictionaries(dict_str))\n",
    "        else:\n",
    "            # If no further nesting, evaluate the string as a dictionary\n",
    "            dicts.append(eval(dict_str))\n",
    "    \n",
    "    return dicts\n",
    "\n",
    "# Your string here\n",
    "text = \"Context: ##`{'key': 'value', 'nested': 'Context: ##`{\\\"nested_key\\\": \\\"nested_value\\\"}`##'}`##\"\n",
    "\n",
    "# Extract and print the nested dictionaries\n",
    "nested_dicts = extract_dictionaries(text)\n",
    "print(nested_dicts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da46dd82-e6a1-4efc-9fae-990e48669d93",
   "metadata": {},
   "source": [
    "## Business"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a204f1-0e4e-4f2c-8fed-f324293a1869",
   "metadata": {},
   "source": [
    "#### El Che Steakhouse, Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d0c07-16fc-411b-8ee2-2747c8da025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_el_che_chicago =  await assess_proposition2(\n",
    "    proposition =  \"El Che Steakhouse is the best steakhouse in Chicago.\", \n",
    "    short_name = 'el_che_chicago', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"Be sure to entertain alternative hypotheses and to obtain evidence for and against the root proposition.\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 35,\n",
    "    print_level = 2,\n",
    "    force_run = True\n",
    ")\n",
    "display(assessment_el_che_chicago['kb_dot'])\n",
    "if assessment_el_che_chicago.get('plan_result'):\n",
    "    print_summary(assessment_el_che_chicago)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a00835-4314-4e6d-9939-e51a1b9ee19d",
   "metadata": {},
   "source": [
    "#### Zeiss Holocam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a528bf4d-2dd4-4c92-b116-0c4aa9c9bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_zeiss_holocam =  await assess_proposition2(\n",
    "    proposition =  \"Privacy has been forever lost due to Holocam by Zeiss.\", \n",
    "    short_name = 'zeiss_holocam', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these facts.\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* Zeiss' \"Holocam\" is a transparent camera powered by holography that turns glass windows into cameras (https://www.digitalcameraworld.com/news/this-holographic-camera-turns-any-window-into-an-invisible-camera).\n",
    "\n",
    "==========\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = False,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4487a88-0411-4792-ad51-c83f009201e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_zeiss_holocam['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e772f75-df04-47df-8036-6a02461c07e3",
   "metadata": {},
   "source": [
    "#### Rabbit r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d24840-3763-4045-a15f-d50bfa88f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_rabbit_r1 =  await assess_proposition2(\n",
    "    proposition = \"The Rabbit r1 is a revolutionary new product.\", \n",
    "    short_name = 'rabbit_r1', \n",
    "    search_cache = search_cache, \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\",\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level=2\n",
    ")\n",
    "\n",
    "display(assessment_rabbit_r1['kb_dot'])\n",
    "if assessment_rabbit_r1.get('plan_result'):\n",
    "    _=print_summary(assessment_rabbit_r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5848c-6dab-4881-b48b-b028ed8f54b2",
   "metadata": {},
   "source": [
    "#### KUOE Kyoto watches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f98433-e02f-4052-8736-28d323c2ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_kuoe_watches =  await assess_proposition2(\n",
    "    proposition =  \"KUOE Kyoto is the best brand currently available for field/military automatic watches under $1000\", \n",
    "    short_name = 'kuoe_watches', \n",
    "    use_plan = None,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes either lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these given facts.\n",
    "\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* KUOE Kyoto is a watch brand headquartered in Kyoto, Japan.\n",
    "* They make field/military watches.\n",
    "\n",
    "==========\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b3558-6647-449e-9ec0-5f626117b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_kuoe_watches['kb_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964752c-be8a-4b18-84c2-a0a93ab3c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_kuoe_watches_improve =  await assess_proposition2(\n",
    "    proposition =  \"KUOE Kyoto is the best brand currently available for field/military automatic watches under $1000\", \n",
    "    short_name = 'kuoe_watches_improve',  \n",
    "    improvement_round = True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes either lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these given facts.\n",
    "\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* KUOE Kyoto is a watch brand headquartered in Kyoto, Japan.\n",
    "* They make field/military watches.\n",
    "\n",
    "==========\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\",\n",
    "    force_run = True,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_kuoe_watches_improve['kb_dot'])\n",
    "if assessment_kuoe_watches_improve.get('plan_result'):\n",
    "    _=print_summary(assessment_kuoe_watches_improve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f65ff-6e80-43cf-9fcd-8260a519e5e9",
   "metadata": {},
   "source": [
    "#### from ChatGPT4\n",
    "\n",
    "* Prompt: What does 'impeccable record' mean in the context of wealth management services?\n",
    "\n",
    "* Response:\n",
    "\n",
    "In the context of wealth management services, an \"impeccable record\" usually refers to a consistently excellent performance history and a reputation for integrity and reliability. Here's what it typically encompasses:\r\n",
    "\r\n",
    "1. **Performance**: It indicates a history of strong returns on investments or consistent achievement of financial goals tailored to clients' needs. The firm or individual has demonstrated the ability to perform well across different market conditions while managing risks effectively.\r\n",
    "\r\n",
    "2. **Regulatory Compliance**: Maintaining an impeccable record means adhering strictly to all regulatory requirements and industry standards. This includes being transparent in dealings, providing accurate reporting, and engaging in ethical practices.\r\n",
    "\r\n",
    "3. **Client Satisfaction**: High levels of client satisfaction and retention, positive testimonials, and referrals. This implies that the service has successfully met or exceeded client expectations over time.\r\n",
    "\r\n",
    "4. **Expertise and Stability**: Longevity and stability in the industry, along with a team of professionals who are experienced and highly qualified. This suggests a depth of knowledge and a breadth of experience in handling various aspects of wealth management.\r\n",
    "\r\n",
    "5. **Error and Dispute Handling**: A minimal history of errors, disputes, or legal issues, and an effective, fair approach to resolving any that do arise. An impeccable record would indicate that any issues are resolved promptly and satisfactorily.\r\n",
    "\r\n",
    "In summary, an impeccable record in wealth management signifies trustworthiness and a proven track record of success and professionalism. It's a powerful statement of credibility and quality that can be a deciding factor for potential clients looking for wealth management services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb0a78-2837-4afa-9836-fd5a81cb1e4b",
   "metadata": {},
   "source": [
    "#### Walker Bros. Original Pancake House, Wilmette, IL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e98d30d-d4c0-4dcc-b384-5c3017f06b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_walker_bros =  await assess_proposition2(\n",
    "    proposition = \"Walker Bros. Original Pancake House is the best pancake place in the entire Chicago metropolitan area.\", \n",
    "    short_name = 'walker_bros', \n",
    "    search_cache = search_cache, \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    force_run = False,\n",
    "    save_pickle = True\n",
    ")\n",
    "display(assessment_walker_bros['kb_dot'])\n",
    "_=print_summary(assessment_walker_bros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf38a3-99c8-42c2-b2a2-c082eae93743",
   "metadata": {},
   "source": [
    "#### Tesla Model Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf8c58-2816-4c22-8be3-4fa1d1422bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_tesla_y =  await assess_proposition2(\n",
    "    proposition = \"Tesla Model Y is the best electric vehicle currently available.\", \n",
    "    short_name = 'tesla_y', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model =\"gpt-3.5-turbo\", \n",
    "    report_to_file = False,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda04193-0bbb-461e-828f-0d40f5d21279",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_tesla_y['kb_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbf71c7-b751-42dd-b988-502a18804359",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = re.sub(r\"[^']*('[^\\n]+')[^?]+\\?\\n.+\",r\"\\1\",list(assessment_tesla_y['plan_result'].values())[-1].query,flags=re.DOTALL)\n",
    "mrkdwn_header = f\"\"\"---\n",
    "title: \"Proposition: {prop}\"\n",
    "author: \"GPT-4, with Michael L. Thompson\"\n",
    "date: \"`r Sys.Date()`\"\n",
    "output:\n",
    "  pdf_document:\n",
    "    toc: yes\n",
    "    toc_depth: 4\n",
    "linkcolor: red\n",
    "urlcolor: blue\n",
    "---\n",
    "\n",
    "```{{r setup, include=FALSE}}\n",
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "```\n",
    "\\newpage\n",
    "\n",
    "\"\"\"\n",
    "filename_stem = assessment_tesla_y['short_name']\n",
    "mrkdwn_file_path = f'{filename_stem}.Rmd'\n",
    "with open(mrkdwn_file_path, 'w') as file:\n",
    "    file.write(mrkdwn_header + to_ascii(assessment_tesla_y['report']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f2818-0013-4e18-9899-ddb8664e4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(assessment_tesla_y['plan_result'].values())[-1].response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe72841d-cb84-46e6-8f6f-4683f6abd002",
   "metadata": {},
   "source": [
    "#### SpaceX vs. NASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d309a0-608a-471e-ad8b-798e3f8ce55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_spacex_nasa =  await assess_proposition(\n",
    "    proposition =  \"SpaceX is much more innovative than NASA.\", \n",
    "    short_name = 'spacex_nasa', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407bfd7c-54fb-427f-b559-081127b3495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_spacex_nasa['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5190a0-4cb7-4580-b179-904b87b39b4f",
   "metadata": {},
   "source": [
    "#### SpaceX vs. NASA, currently & specificly in launch technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b10b4-2c12-456b-8b4e-4f4932461bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_spacex_nasa2 =  await assess_proposition(\n",
    "    proposition =  \"Currently and specificly in launch and space flight technology, SpaceX is much more innovative than NASA.\", \n",
    "    short_name = 'spacex_nasa2', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6a48b-413e-4844-bdf5-b5143235a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_spacex_nasa2['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f2220-9ae1-49f5-b2be-9ea0e5202fca",
   "metadata": {},
   "source": [
    "#### Tech startups DEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3e00a-bc44-4e81-bcfd-e245a8d1d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.patch(OpenAI())\n",
    "assessment_startup_dei =  await assess_proposition(\n",
    "    proposition = \"American tech startups have killed DEI in 2023 and forward.\", \n",
    "    short_name = 'startup_dei', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = 'Ignore gains in DEI prior to 2023. The proposition is about 2023 and the future.', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2b7e2-073a-42f5-bcc6-b3097251b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_startup_dei['kb_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c73ad-60ec-4211-8aae-08bcab7fadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.patch(OpenAI())\n",
    "assessment_startup_dei2 =  await assess_proposition2(\n",
    "    proposition = \"American tech startups have killed DEI in 2023 and forward.\", \n",
    "    short_name = 'startup_dei2', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = 'Ignore gains in DEI prior to 2023. The proposition is about 2023 and the future.', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d482fd6-953e-4299-abbd-20dcad2dfa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_startup_dei2['kb_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec478cdf-f65c-412e-993d-5fec48b5714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_hamilton_goat['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9146602-d91b-4c04-9aaa-e9b8301d8179",
   "metadata": {},
   "source": [
    "## Politics/Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde4460-e933-4feb-b189-f016a05c3f15",
   "metadata": {},
   "source": [
    "#### Fairness of current Supreme Court"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d6a55-a12a-45a4-8f41-60947bfd4b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_scotus_fairness =  await assess_proposition2(\n",
    "    proposition =  \"The current Supreme Court of the United States will deliver a fair ruling on Donald Trump's eligibility for the 2024 Presidential election.\", \n",
    "    short_name = 'scotus_fairness', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    force_run = False,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_scotus_fairness['kb_dot'])\n",
    "if assessment_scotus_fairness.get('plan_result'):\n",
    "    print_summary(assessment_scotus_fairness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc3d9f-fffc-403a-a354-9dba1fe8103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_scotus_fairness2 =  await assess_proposition2(\n",
    "    proposition =  \"The current Supreme Court of the United States will deliver a fair ruling on Donald Trump's eligibility for the 2024 Presidential election.\", \n",
    "    short_name = 'scotus_fairness2', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"Especially, consider the following issues:\n",
    "\n",
    "* Recent ethics challenges of, monetary influence on, and failures to recuse themselves by the current U.S. Supreme Court justices.\n",
    "* Recent ideologically-based rulings of the current U.S. Supreme Court justices.\n",
    "* Partisan indebtedness of and political pressure on the current U.S. Supreme Court Justices.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    force_run = False,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_scotus_fairness2['kb_dot'])\n",
    "if assessment_scotus_fairness2.get('plan_result'):\n",
    "    print_summary(assessment_scotus_fairness2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798fd7b5-49ca-40bd-a7f3-26048f390c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_scotus_fairness3 =  await assess_proposition2(\n",
    "    proposition =  \"The current Supreme Court of the United States will deliver a fair ruling on Donald Trump's eligibility for the 2024 Presidential election.\", \n",
    "    short_name = 'scotus_fairness3', \n",
    "    use_plan = assessment_scotus_fairness2['plan0'], # try the previously generated plan prior to the 'improvement round'.\n",
    "    improvement_round=False,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"Especially, consider the following issues:\n",
    "\n",
    "* Recent ethics challenges of, monetary influence on, and failures to recuse themselves by the current U.S. Supreme Court justices.\n",
    "* Recent ideologically-based rulings of the current U.S. Supreme Court justices.\n",
    "* Partisan indebtedness of and political pressure on the current U.S. Supreme Court Justices.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    force_run = False,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_scotus_fairness3['kb_dot'])\n",
    "if assessment_scotus_fairness3.get('plan_result'):\n",
    "    print_summary(assessment_scotus_fairness3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009fb08-f1bd-4fcb-ba15-d6580e9eadfd",
   "metadata": {},
   "source": [
    "#### Voting Rights Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eef4ef-3652-4e06-8086-23eca98f5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_voting_rights =  await assess_proposition2(\n",
    "    proposition =  \"The Voting Rights Act is still necessary and effective despite attacks by states, Congress, and the Supreme Court.\", \n",
    "    short_name = 'voting_rights', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    force_run = False,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_voting_rights['kb_dot'])\n",
    "if assessment_voting_rights.get('plan_result'):\n",
    "    print_summary(assessment_voting_rights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d82fa-e423-49fb-879f-ae35dec57598",
   "metadata": {},
   "source": [
    "#### Presidential conviction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f75637-7189-4070-9b3f-581ffc1311d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_president_conviction =  await assess_proposition2(\n",
    "    proposition =  \"It is impossible to convict a former or sitting U.S. President of a felony without starting a civil war.\", \n",
    "    short_name = 'president_conviction', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    force_run = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_president_conviction['kb_dot'])\n",
    "if assessment_president_conviction.get('plan_result'):\n",
    "    print_summary(assessment_president_conviction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf789dd-dc96-44be-833d-caf9279e6140",
   "metadata": {},
   "source": [
    "#### Biden & Inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9685ad0-e249-4877-9f3e-81c220c84e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_biden_inflation =  await assess_proposition2(\n",
    "    proposition =  \"Inflation in the U.S. economy is because of failures in the Biden administration economic policy.\", \n",
    "    short_name = 'biden_inflation', \n",
    "    search_cache = search_cache, \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_biden_inflation['kb_dot'])\n",
    "if assessment_biden_inflation.get('plan_result'):\n",
    "    _=print_summary(assessment_biden_inflation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e771e39-0ace-4d46-a02a-c5de75bfa1fe",
   "metadata": {},
   "source": [
    "#### Biden economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27051d5c-e485-4960-8554-0488b8a6a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_biden_economy =  await assess_proposition2(\n",
    "    proposition =  \"The Biden administration economic policy has failed America.\", \n",
    "    short_name = 'biden_economy', \n",
    "    search_cache = search_cache, \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_biden_economy['kb_dot'])\n",
    "if assessment_biden_economy.get('plan_result'):\n",
    "    _=print_summary(assessment_biden_economy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919924e4-7b16-4bb7-a51d-ed6f8cc86830",
   "metadata": {},
   "source": [
    "#### Trump economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661230a9-6522-4f6c-ac80-643641fdb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_trump_econom_getplan =  await assess_proposition2(\n",
    "    proposition =  \"The Trump administration economic policy failed America.\", \n",
    "    short_name = 'trump_economy_getplan', \n",
    "    plan_only = True,\n",
    "    search_cache = search_cache, \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = False,\n",
    "    save_pickle = False,\n",
    "    report_to_file = False,\n",
    "    print_level = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb5670-0dc5-4419-98af-934a391b7996",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_trump_econom_getplan['kb_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b79b26-3f6e-4da6-91f5-0706f9a10937",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_trump_economy =  await assess_proposition2(\n",
    "    proposition =  \"The Trump administration economic policy failed America.\", \n",
    "    short_name = 'trump_economy', \n",
    "    use_plan = assessment_trump_econom_getplan['plan'],\n",
    "    search_cache = search_cache, \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_trump_economy['kb_dot'])\n",
    "if assessment_trump_economy.get('plan_result'):\n",
    "    _=print_summary(assessment_trump_economy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4300e-edfb-482f-a838-87583c4aa9ac",
   "metadata": {},
   "source": [
    "#### Rents are too high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780925ee-ee37-4ec7-96e7-184398692d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_high_rents =  await assess_proposition2(\n",
    "    proposition =  \"More renters than ever now spend too much of their incomes on housing.\", \n",
    "    short_name = 'high_rents', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Pursue this issue through the 2024 study by the Joint Center for Housing Studies of Harvard University.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_high_rents['kb_dot'])\n",
    "_=print_summary(assessment_high_rents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512509be-ce00-4d39-a74e-dce8e8b6dfd5",
   "metadata": {},
   "source": [
    "#### American Democracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8d114-871b-4c56-963e-04c7ecb333c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_name = 'american_democracy' #'long_proposition'\n",
    "assessment_results = pickle_it(assessment_results = {'short_name': short_name}, save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea89798-de7e-4dba-9179-6506e6405129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = 'results_american_democracy.pkl'\n",
    "#with open(file_path, 'rb') as state_file:\n",
    "#    json_string = pickle.load(state_file)\n",
    "#print(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a05d27-83be-4348-a2e1-7f9d996f9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_american_democracy =  await assess_proposition2(\n",
    "    proposition =  \"American democracy is stronger than it has ever been.\", \n",
    "    short_name = 'american_democracy', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes either lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_american_democracy['kb_dot'])\n",
    "_=print_summary(assessment_american_democracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ca364-9efc-42c7-88ed-bc68c7ed301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_text = search_GoogleSerperAPI(topic = 'Ohio Nov. 2023 Ballot Issue 1') #(topic = \"Yes on Ohio Issue 1 is good or bad?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37d173-5ba4-4d90-9975-841d4a67038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f304cb4-e59c-41ce-9e75-7a5b38439058",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.patch(OpenAI())\n",
    "plan_ohio0 = query_planner( \"Given proposition 'Yes on Ohio Nov. 2023 Ballot Issue 1 is good', first identify all subpropositions and their subpropositions, and so on, that need to be true to support it, then identify all factual axiomatic statements as evidence that support or refute each and every one of the propositions. Finally, Is Yes on Ohio Issue 1 good?\",client)\n",
    "pprint(plan_ohio0.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917304b-9228-4fbd-a11a-6e9ef66bdd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph: KnowledgeGraph = convert_to_graph(plan_ohio0,'Yes on Ohio Nov. 2023 Ballot Issue 1 is good',label_wrap=20)\n",
    "kb_dot = visualize_knowledge_graph(graph,filename_stem='plan_ohio0',flip=True, aspect_ratio= 'auto')\n",
    "kb_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a78b6-47b0-4f70-81e5-01b710b11a01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plan_ohio0_result = await plan_ohio0.execute(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2de468-dfb3-4d58-b80c-d06d4432d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "last_node_ohio0 = list(plan_ohio0_result.values())[-1]\n",
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',last_node_ohio0.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + last_node_ohio0.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee0f446-4a37-4c19-86bd-299d54c993b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.patch(OpenAI())\n",
    "parent_proposition = 'Voting \"Yes\" on the \"Ohio Nov. 2023 Ballot Issue 1\" was best for Ohio.'\n",
    "context = \"Investigate the Ohio November 2023 Ballot Issue 1 related to Reproductive Rights. DO NOT investigate any other Issue 1's, e.g., related to threshold for constitutional ammendments.\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are one of two players in the World Championship of a game called \\\"Deep Planning to Investigate a Hypothesis!\\\" \\\n",
    "As such, you are a world-class hypothesis confirmation algorithm capable of breaking apart a proposition into its subpropositions, their sub-subpropositions, \\\n",
    "and so on. This creates a <<QUERY PLAN>>, which is a graph of subpropositions that go from bottom-level axiomatic primitives and logically lead up to the original top-level proposition. \\\n",
    "You also identify key factual evidence that supports or refutes each and every subproposition such that an argument, which is objective, thorough, and logical, \\\n",
    "can be made for or against each subproposition. When logically assessing the evidence, you consider all alternative explanations for the existence of that evidence and not only those \\\n",
    "subpropositions that lead back to the top-level proposition. \\\n",
    "The player that generates the (1) deepest, (2) most thorough & complete, (3) most accurate as verifiable by evidence from the internet, and (4) most insightful research plan into the hypothesis \\\n",
    "will be crowned the World Champion Hypothesis Analyzer!!! \\\n",
    "\\\n",
    "So, think step-by-step in a logical reasoning manner to get a better understanding of the problem. Then generate this inverted 'network' of logical dependencies leading from \\\n",
    "the parent top-level proposition at 'root' to the 'branch' subpropositions and to the 'leaf' pieces of evidence supporting or refuting each subproposition.\\\n",
    "\\\n",
    "Before you respond with your finalized plan, consider that your opponent player will defeat you if they have found additional subpropositions for any proposition that is in common with yours \\\n",
    "or if they have a more thorough search of the internet than you do to gather evidence. So, make sure that your plan is both deep in terms of levels of subpropositions and bushy in terms of \\\n",
    "lots of leaf nodes to uncover evidence through internet searches. \\\n",
    "\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"CONTEXT: '{context}'\n",
    "--------------------------------------------------------------\n",
    "TOP-LEVEL PROPOSITION: {parent_proposition}\n",
    "--------------------------------------------------------------\n",
    "TASK: Generate the correct QUERY PLAN -- i.e., \n",
    "the network of top-level proposition, branches of \n",
    "subpropositions and their sub-subpropositions, \n",
    "and so on, and finally, the total set of leaves of evidence.\n",
    "--------------------------------------------------------------\n",
    "RESPONSE: <<QUERY PLAN>>\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "plan_ohio1 = client.chat.completions.create(\n",
    "    model='gpt-4-turbo-preview',\n",
    "    temperature=0,\n",
    "    response_model=QueryPlan,\n",
    "    messages=messages,\n",
    "    max_retries = 2,\n",
    "    max_tokens=3000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6764ae-aeb9-4ba9-9f11-35b44375b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(plan_ohio1.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67660c3-aafb-4cbf-a927-e0fa4d3ec500",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph: KnowledgeGraph = convert_to_graph(plan_ohio1,'Yes on \"Ohio Nov. 2023 Ballot Issue 1\" is good',label_wrap=20)\n",
    "kb_dot = visualize_knowledge_graph(graph,filename_stem='plan_ohio1',flip=True, aspect_ratio= 'auto')\n",
    "kb_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc11e5-767d-4904-9558-a6312f96bee9",
   "metadata": {},
   "source": [
    "**Execute the plan using GPT-3.5 Turbo...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d450a-5948-4b95-9b9e-b5df6eb4f042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plan_ohio1_result = await plan_ohio1.execute(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da065eb2-7298-4a8b-9d0f-efe8088c7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "last_node_ohio1 = list(plan_ohio1_result.values())[-1]\n",
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',last_node_ohio1.query.replace(\"\\\\n\",\"\\n\"),flags=re.DOTALL).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + last_node_ohio1.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a948e10e-a5d7-4c04-b839-a98fa8724ca9",
   "metadata": {},
   "source": [
    "#### American Stature During Biden Administration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e262b-b961-4f52-a657-354bd40aab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposition = \"America's stature in the world has worsened during the Biden administration in comparison to what it was under the Trump administration.\"\n",
    "key_question = f\"\"\"Given proposition '{proposition}'. First identify all sub-propositions and their sub-propositions, and so on, that need to be true to support it.\n",
    "Then identify all factual axiomatic statements as evidence that support or refute each and every one of the propositions. \n",
    "Finally, answer this question: Is '{proposition}' the truth?\"\"\"\n",
    "plan = query_planner( key_question )\n",
    "pprint(plan.model_dump())\n",
    "plan_biden_result = await plan.execute() # Execute using GPT-3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea7eb7c-5ac0-466f-acdf-61ca99f00ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',plan_biden_result.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + plan_biden_result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06a6a9-89ba-4b3e-a0cd-ae79d0309545",
   "metadata": {},
   "source": [
    "#### Trump Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38d2f2-1d49-4fa5-86b4-f76192a905fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_trump_corruption =  await assess_proposition2(\n",
    "    proposition = \"Donald Trump is no more or less corrupt than any typical U.S. President.\", \n",
    "    short_name = 'trump_corruption', \n",
    "    improvement_round=True,\n",
    "    search_cache = search_cache, \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\",\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    print_level=2\n",
    ")\n",
    "display(assessment_trump_corruption['kb_dot'])\n",
    "if assessment_trump_corruption.get('plan_result'):\n",
    "    _=print_summary(assessment_trump_corruption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8f97c-37c8-474f-9f2c-12a2a18b848f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proposition = \"Donald Trump is no more or less corrupt than any typical U.S. President.\"\n",
    "key_question = f\"\"\"Given proposition '{proposition}'. First identify all sub-propositions and their sub-propositions, and so on, that need to be true to support it.\n",
    "Then identify all factual axiomatic statements as evidence that support or refute each and every one of the propositions. \n",
    "Finally, answer this question: Is '{proposition}' the truth?\"\"\"\n",
    "plan = query_planner( key_question )\n",
    "pprint(plan.model_dump())\n",
    "plan_trump_result = await plan.execute() # Execute using GPT-3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1057230-3f6f-4c66-b819-ac3c24b47fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',plan_trump_result.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + plan_trump_result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba952a-de3b-4d1e-9f18-66e22930a1fb",
   "metadata": {},
   "source": [
    "#### Biden Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff7805-a4da-493d-89ab-523a2a2a42a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qp_dict = eval(assessment_trump_corruption['plan'].model_dump_json().replace(\"Donald Trump\",\"Joe Biden\"))\n",
    "qp_dict['context'] = None\n",
    "qp = QueryPlan(**qp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb790e1-570f-40c1-a81b-03d8fde5406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33435845-df5f-44a1-913f-de16e8324ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_biden_corruption =  await assess_proposition2(\n",
    "    proposition = \"Joe Biden is no more or less corrupt than any typical U.S. President.\", \n",
    "    short_name = 'biden_corruption', \n",
    "    use_plan = qp,\n",
    "    search_cache = search_cache, \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\",\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 25,\n",
    "    print_level=2\n",
    ")\n",
    "display(assessment_biden_corruption['kb_dot'])\n",
    "if assessment_biden_corruption.get('plan_result'):\n",
    "    _=print_summary(assessment_biden_corruption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4b3bc-f367-49ff-b02a-566747f34f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proposition = \"Joe Biden is no more or less corrupt than any typical U.S. President.\"\n",
    "key_question = f\"\"\"Given proposition '{proposition}'. First identify all sub-propositions and their sub-propositions, and so on, that need to be true to support it.\n",
    "Then identify all factual axiomatic statements as evidence that support or refute each and every one of the propositions. \n",
    "In generating the query plan, identify these specific sub-propositions and the specific pieces of evidence needed to confirm and refute them.\n",
    "Finally, answer this question: Is '{proposition}' the truth?\"\"\"\n",
    "plan = query_planner( key_question )\n",
    "pprint(plan.model_dump())\n",
    "plan_biden2_result = await plan.execute() # Execute using GPT-3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917c4fd-9153-4f32-b667-cccad9c0f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',plan_biden2_result.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + plan_biden2_result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc30b0-2aa3-4666-9dc9-de69174be2f1",
   "metadata": {},
   "source": [
    "#### Presidential immunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab3d47-54c3-4fc9-a6d0-d360fc88f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_president_immune =  await assess_proposition2(\n",
    "    proposition =  \"Is the U.S. President immune from prosecution for any and all actions they commit during their term?\", \n",
    "    short_name = 'president_immune', \n",
    "    use_plan = None,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes either lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = True,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7542532-5b36-4fb7-8749-6fdf157d6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_president_immune['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae680fc-9df8-4e28-8f39-ae0aa31a0b67",
   "metadata": {},
   "source": [
    "#### Florida Director of Health on Vaccines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af890b-604e-4eb1-ba55-033cfa160ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_florida_vaccines =  await assess_proposition2(\n",
    "    proposition =  \"The current Director of Health for the state of Florida is right about vaccines.\", \n",
    "    short_name = 'florida_vaccines', \n",
    "    use_plan = None,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes either lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = True,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c56524-630b-4c92-8855-066a34e3a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_florida_vaccines['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22851ac-8426-42ca-b11e-43b9969ad3be",
   "metadata": {},
   "source": [
    "The query graph shows that the three leafe nodes involving evidence should actually be parents of the node for the ultimate question. So, we need to add them to the `dependencies` list of the ultimate question node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7ed06-5725-4dd7-afb7-73e0d3cadbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assessment_florida_vaccines['plan'])\n",
    "plan_florida_vaccines.query_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc0c9b-f1fe-45ac-9e83-26adac6f6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "ultimate_question_node = None\n",
    "assessment_florida_vaccines['plan'].query_graph[0].dependencies = list(set(assessment_florida_vaccines['plan'].query_graph[0].dependencies))\n",
    "plan_florida_vaccines = QueryPlan(**(assessment_florida_vaccines['plan'].model_dump()))\n",
    "ultimate_question = [iq for iq in enumerate(plan_florida_vaccines.query_graph) if 'right about vaccines' in iq[1].question][0]\n",
    "i_ultimate,ultimate_question_node  = ultimate_question\n",
    "print(i_ultimate)\n",
    "evidence_nodes = [q.id for q in plan_florida_vaccines.query_graph if 'evidence' in q.question]\n",
    "evidence_nodes2 = [q.id for q in plan_florida_vaccines.query_graph if 'alternative explanations' in q.question]\n",
    "ultimate_question_node.dependencies.extend(evidence_nodes + evidence_nodes2)\n",
    "ultimate_question_node.dependencies = list(set(ultimate_question_node.dependencies))\n",
    "plan_florida_vaccines.query_graph[i_ultimate] = ultimate_question_node\n",
    "graph_florida_vaccines: KnowledgeGraph = convert_to_graph(plan_florida_vaccines,\"The current Director of Health for the state of Florida is right about vaccines.\",label_wrap=20)\n",
    "kb_dot_florida_vaccines = visualize_knowledge_graph(graph_florida_vaccines,flip=True, aspect_ratio= 'auto')\n",
    "kb_dot_florida_vaccines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29fe017-c5db-465a-b525-e9b9af13b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_florida_vaccines2 =  await assess_proposition2(\n",
    "    proposition =  \"The current Director of Health for the state of Florida is right about vaccines.\", \n",
    "    short_name = 'florida_vaccines2', \n",
    "    use_plan = plan_florida_vaccines,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes either lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence for and evidence against each subproposition.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = True,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038db0b-1d20-440a-a0b8-9319491a748d",
   "metadata": {},
   "source": [
    "#### Vaccination risk-benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d7dcf-c931-4f42-93a4-8e913c457418",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_antivax2 =  await assess_proposition2(\n",
    "    proposition = \"Vaccinations do more harm than good\", \n",
    "    short_name = 'antivax2', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215776a1-54ba-4f1a-a42a-73132522e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(assessment_antivax2['kb_dot'])\n",
    "if assessment_antivax2.get('plan_result'):\n",
    "    _ = print_summary(assessment_antivax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd512b-9ba3-4545-ad7b-226b70cac386",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assessment_antivax2['plan'].model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1adad54-32d0-4610-a77c-77a746e0e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_antivax3 =  await assess_proposition(\n",
    "    proposition = \"Vaccinations do more harm than good\", \n",
    "    short_name = 'antivax3', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    print_level = 1\n",
    ")\n",
    "display(assessment_antivax3['kb_dot'])\n",
    "if assessment_antivax3.get('plan_result'):\n",
    "    _ = print_summary(assessment_antivax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4c1b2-2c87-47a8-ae34-b33f034dac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_antivax2['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d268119-02cd-4d22-820f-2f8ae69af693",
   "metadata": {},
   "source": [
    "#### Belichick-Brady Dynasty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f39f6e-4219-41bb-8451-21c0cee6ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_nfl_dynasty =  await assess_proposition2(\n",
    "    proposition = \"Currently, the Belichick-Brady Era New England Patriots are the greatest NFL dynasty of all time.\", \n",
    "    short_name = 'nfl_dynasty', \n",
    "    added_instructions = \"\"\"\n",
    "Build a `query_plan_outline` of the following form:\n",
    "\n",
    "Is [TARGET ENTITY] better than [OTHER ENTITIES] wrt [TRAIT]? (for N other entities)\n",
    "\n",
    "* What are the indicators of [TRAIT]? (find M indicators)\n",
    "* How do [ENTITY]'s statistics on these indicators compare to the statistics of the comparison entities? \n",
    "  * [INDICATOR_1] \n",
    "    * Tabulate the statistics of [OTHER ENTITIES] on [INDICATOR_1]\n",
    "      * What is [INDICATOR_1] for [TARGET ENTITY]?\n",
    "      * What is [INDICATOR_1] for [OTHER ENTITY_1]?\n",
    "      * What is [INDICATOR_1] for [OTHER ENTITY_2]?\n",
    "      ...\n",
    "      * What is [INDICATOR_1] for [OTHER ENTITY_N]?\n",
    "  * [INDICATOR_2] \n",
    "    * Tabulate the statistics of [OTHER ENTITIES] on [INDICATOR_2]\n",
    "      * What is [INDICATOR_2] for [TARGET ENTITY]?\n",
    "      * What is [INDICATOR_2] for [OTHER ENTITY_1]?\n",
    "      * What is [INDICATOR_2] for [OTHER ENTITY_2]?\n",
    "      ...\n",
    "      * What is [INDICATOR_2] for [OTHER ENTITY_N]?\n",
    "  ...\n",
    "  * [INDICATOR_M] \n",
    "    * Tabulate the statistics of [OTHER ENTITIES] on [INDICATOR_M]\n",
    "      * What is [INDICATOR_M] for [TARGET ENTITY]?\n",
    "      * What is [INDICATOR_M] for [OTHER ENTITY_1]?\n",
    "      * What is [INDICATOR_M] for [OTHER ENTITY_2]?\n",
    "     ...\n",
    "      * What is [INDICATOR_M] for [OTHER ENTITY_N]?\n",
    "\n",
    "\n",
    "In this case:\n",
    "[TARGET ENTITY] is Belichick-Brady Era New England Patriots\n",
    "[TRAIT] is great NFL dynasty\n",
    "[OTHER ENTITIES] are at least N=3 other great NFL dynasties, which you must identify.\n",
    "You must determine the appropriate number, N, and the names of [OTHER ENTITY_1], [OTHER ENTITY_2],..., [OTHER ENTITY_M].\n",
    "\n",
    "Based upon the top proposition, determine the relevant indicators for [TRAIT]: [INDICATOR_1], [INDICATOR_2], ..., [INDICATOR_M].\n",
    "You must determine the appropriate number, M, and the names of [INDICATOR_1], [INDICATOR_2],..., [INDICATOR_M].\n",
    "\n",
    "In the `query_graph` of the query plan, for `Query` nodes corresponding to \"Tabulate the statistics...\", set the `question` field \n",
    "to that tabulation statement, i.e., \"Tabulate the statistics of ...\" so that a table is returned for those nodes.\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    search_cache = search_cache, \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run=False,\n",
    "    report_to_file = True,\n",
    "    print_level=2\n",
    ")\n",
    "display(assessment_nfl_dynasty['kb_dot'])\n",
    "if assessment_nfl_dynasty.get('plan_result'):\n",
    "    _ = print_summary(assessment_nfl_dynasty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ef7de-183d-4818-a8aa-ac8ca2b8cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_nfl_dynasty2 =  await assess_proposition2(\n",
    "    proposition = \"Currently, the Belichick-Brady Era New England Patriots are the greatest NFL dynasty of all time.\", \n",
    "    short_name = 'nfl_dynasty2', \n",
    "    added_instructions = \"\"\"\n",
    "Build a `query_plan_outline` of the following form:\n",
    "\n",
    "Is [TARGET ENTITY] better than [OTHER ENTITIES] wrt [TRAIT]? (for N other entities)\n",
    "\n",
    "* What are the indicators of [TRAIT]? (find M indicators)\n",
    "  * Be sure to create a `Query` node that searches for these indicators and that then is included in the `dependencies` field of the other `Query` nodes that need [INDICATOR_1], [INDICATOR_2], ..., [INDICATOR_M].\n",
    "* What are the [OTHER ENTITIES] in contention wrt [TRAIT]? (Identify specific entities by name as [OTHER ENTITY_1], [OTHER ENTITY_2],..., [OTHER ENTITY_N].)\n",
    "  * Be sure to create a `Query` node that searches for these other entities and that then is included in the `dependencies` field of the other `Query` nodes that need [OTHER ENTITY_1], [OTHER ENTITY_2], ..., [OTHER ENTITY_N].\n",
    "* How do [ENTITY]'s statistics on these indicators compare to the statistics of the comparison entities? \n",
    "  * [INDICATOR_1] \n",
    "    * Tabulate the statistics of [OTHER ENTITIES] on [INDICATOR_1]\n",
    "      * What is [INDICATOR_1] for [TARGET ENTITY]?\n",
    "      * What is [INDICATOR_1] for [OTHER ENTITY_1]?\n",
    "      * What is [INDICATOR_1] for [OTHER ENTITY_2]?\n",
    "      ...\n",
    "      * What is [INDICATOR_1] for [OTHER ENTITY_N]?\n",
    "  * [INDICATOR_2] \n",
    "    * Tabulate the statistics of [OTHER ENTITIES] on [INDICATOR_2]\n",
    "      * What is [INDICATOR_2] for [TARGET ENTITY]?\n",
    "      * What is [INDICATOR_2] for [OTHER ENTITY_1]?\n",
    "      * What is [INDICATOR_2] for [OTHER ENTITY_2]?\n",
    "      ...\n",
    "      * What is [INDICATOR_2] for [OTHER ENTITY_N]?\n",
    "  ...\n",
    "  * [INDICATOR_M] \n",
    "    * Tabulate the statistics of [OTHER ENTITIES] on [INDICATOR_M]\n",
    "      * What is [INDICATOR_M] for [TARGET ENTITY]?\n",
    "      * What is [INDICATOR_M] for [OTHER ENTITY_1]?\n",
    "      * What is [INDICATOR_M] for [OTHER ENTITY_2]?\n",
    "     ...\n",
    "      * What is [INDICATOR_M] for [OTHER ENTITY_N]?\n",
    "\n",
    "\n",
    "In this case:\n",
    "\n",
    "* [TARGET ENTITY] = Belichick-Brady Era New England Patriots\n",
    "* [TRAIT] = great NFL dynasty\n",
    "* [OTHER ENTITIES] = at least N=3 other great NFL dynasties, which you must identify by specific names.\n",
    "  * You must determine the appropriate number, N, and the names of [OTHER ENTITY_1], [OTHER ENTITY_2],..., [OTHER ENTITY_M].\n",
    "\n",
    "Based upon the top proposition, determine the relevant indicators for [TRAIT]: [INDICATOR_1], [INDICATOR_2], ..., [INDICATOR_M].\n",
    "You must determine the appropriate number, M, and the names of [INDICATOR_1], [INDICATOR_2],..., [INDICATOR_M].\n",
    "\n",
    "In the `query_graph` of the query plan, for `Query` nodes corresponding to \"Tabulate the statistics...\", set the `question` field \n",
    "to that tabulation statement, i.e., \"Tabulate the statistics of ...\" so that a table is returned for those nodes.\n",
    "\n",
    "Make sure that you assign the `Query` nodes for indicators and for other entities as members of the `dependencies` field list of other `Query` nodes that require them.\n",
    "\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    search_cache = search_cache, \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run=False,\n",
    "    report_to_file = True,\n",
    "    print_level=2\n",
    ")\n",
    "display(assessment_nfl_dynasty2['kb_dot'])\n",
    "if assessment_nfl_dynasty2.get('plan_result'):\n",
    "    _ = print_summary(assessment_nfl_dynasty2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f92486-1e0f-499f-8164-1587207e1707",
   "metadata": {},
   "source": [
    "#### Brady vs. Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279eb9b3-e21c-4f01-bb63-bcda95943e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_brady_vs_others =  await assess_proposition2(\n",
    "    proposition = \"Tom Brady is the greatest NFL player of all time.\", \n",
    "    short_name = 'brady_vs_others', \n",
    "    added_instructions = \"\"\"\n",
    "Build a `query_plan_outline` of the following form:\n",
    "\n",
    "Is [TARGET ENTITY] better than [OTHER ENTITIES] wrt [TRAIT]? (for N other entities)\n",
    "\n",
    "* What are the indicators of [TRAIT]? (find M indicators)\n",
    "* Who are the [OTHER ENTITIES] in contention wrt [TRAIT]? (Identify specific entities by name as [OTHER ENTITY_1], [OTHER ENTITY_2],..., [OTHER ENTITY_M].)\n",
    "* How do [ENTITY]'s statistics on these indicators compare to the statistics of the comparison entities? \n",
    "  * [INDICATOR_1] \n",
    "    * Tabulate the statistics of [OTHER ENTITIES] on [INDICATOR_1]\n",
    "      * What is [INDICATOR_1] for [TARGET ENTITY]?\n",
    "      * What is [INDICATOR_1] for [OTHER ENTITY_1]?\n",
    "      * What is [INDICATOR_1] for [OTHER ENTITY_2]?\n",
    "      ...\n",
    "      * What is [INDICATOR_1] for [OTHER ENTITY_N]?\n",
    "  * [INDICATOR_2] \n",
    "    * Tabulate the statistics of [OTHER ENTITIES] on [INDICATOR_2]\n",
    "      * What is [INDICATOR_2] for [TARGET ENTITY]?\n",
    "      * What is [INDICATOR_2] for [OTHER ENTITY_1]?\n",
    "      * What is [INDICATOR_2] for [OTHER ENTITY_2]?\n",
    "      ...\n",
    "      * What is [INDICATOR_2] for [OTHER ENTITY_N]?\n",
    "  ...\n",
    "  * [INDICATOR_M] \n",
    "    * Tabulate the statistics of [OTHER ENTITIES] on [INDICATOR_M]\n",
    "      * What is [INDICATOR_M] for [TARGET ENTITY]?\n",
    "      * What is [INDICATOR_M] for [OTHER ENTITY_1]?\n",
    "      * What is [INDICATOR_M] for [OTHER ENTITY_2]?\n",
    "     ...\n",
    "      * What is [INDICATOR_M] for [OTHER ENTITY_N]?\n",
    "\n",
    "\n",
    "In this case:\n",
    "\n",
    "* [TARGET ENTITY] is Tom Brady\n",
    "* [TRAIT] is great NFL player\n",
    "* [OTHER ENTITIES] are at least N=3 other great NFL players, which you must identify.\n",
    "  * You must determine the appropriate number, N, and the names of [OTHER ENTITY_1], [OTHER ENTITY_2],..., [OTHER ENTITY_M].\n",
    "\n",
    "Based upon the top proposition, determine the relevant indicators for [TRAIT]: [INDICATOR_1], [INDICATOR_2], ..., [INDICATOR_M].\n",
    "You must determine the appropriate number, M, and the names of [INDICATOR_1], [INDICATOR_2],..., [INDICATOR_M].\n",
    "\n",
    "In the `query_graph` of the query plan, for `Query` nodes corresponding to \"Tabulate the statistics...\", set the `question` field \n",
    "to that tabulation statement, i.e., \"Tabulate the statistics of ...\" so that a table is returned for those nodes.\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    search_cache = search_cache, \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run=False,\n",
    "    report_to_file = True,\n",
    "    print_level=2\n",
    ")\n",
    "display(assessment_brady_vs_others['kb_dot'])\n",
    "if assessment_brady_vs_others.get('plan_result'):\n",
    "    _ = print_summary(assessment_brady_vs_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8d3f9-95a1-4edb-9069-24e37c22b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_brady_vs_others2 =  await assess_proposition2(\n",
    "    proposition = \"Tom Brady is the greatest NFL player of all time.\", \n",
    "    short_name = 'brady_vs_others2', \n",
    "    added_instructions = \"\"\"\n",
    "Build a `query_plan_outline` of the following form:\n",
    "\n",
    "Is [TARGET ENTITY] better than [OTHER ENTITIES] wrt [TRAIT]? (for N other entities)\n",
    "\n",
    "* What are the indicators of [TRAIT]? (find M indicators)\n",
    "  * Be sure to create a `Query` node that searches for these indicators and that then is included in the `dependencies` field of the other `Query` nodes that need [INDICATOR_1], [INDICATOR_2], ..., [INDICATOR_M].\n",
    "* Who are the [OTHER ENTITIES] in contention wrt [TRAIT]? (Identify specific entities by name as [OTHER ENTITY_1], [OTHER ENTITY_2],..., [OTHER ENTITY_M].)\n",
    "  * Be sure to create a `Query` node that searches for these other entities and that then is included in the `dependencies` field of the other `Query` nodes that need [OTHER ENTITY_1], [OTHER ENTITY_2], ..., [OTHER ENTITY_N].\n",
    "* How do [ENTITY]'s statistics on these indicators compare to the statistics of the comparison entities? \n",
    "  * [INDICATOR_1] \n",
    "    * Tabulate the statistics of [OTHER ENTITIES] on [INDICATOR_1]\n",
    "      * What is [INDICATOR_1] for [TARGET ENTITY]?\n",
    "      * What is [INDICATOR_1] for [OTHER ENTITY_1]?\n",
    "      * What is [INDICATOR_1] for [OTHER ENTITY_2]?\n",
    "      ...\n",
    "      * What is [INDICATOR_1] for [OTHER ENTITY_N]?\n",
    "  * [INDICATOR_2] \n",
    "    * Tabulate the statistics of [OTHER ENTITIES] on [INDICATOR_2]\n",
    "      * What is [INDICATOR_2] for [TARGET ENTITY]?\n",
    "      * What is [INDICATOR_2] for [OTHER ENTITY_1]?\n",
    "      * What is [INDICATOR_2] for [OTHER ENTITY_2]?\n",
    "      ...\n",
    "      * What is [INDICATOR_2] for [OTHER ENTITY_N]?\n",
    "  ...\n",
    "  * [INDICATOR_M] \n",
    "    * Tabulate the statistics of [OTHER ENTITIES] on [INDICATOR_M]\n",
    "      * What is [INDICATOR_M] for [TARGET ENTITY]?\n",
    "      * What is [INDICATOR_M] for [OTHER ENTITY_1]?\n",
    "      * What is [INDICATOR_M] for [OTHER ENTITY_2]?\n",
    "     ...\n",
    "      * What is [INDICATOR_M] for [OTHER ENTITY_N]?\n",
    "\n",
    "\n",
    "In this case:\n",
    "\n",
    "* [TARGET ENTITY] is Tom Brady\n",
    "* [TRAIT] is great NFL player\n",
    "* [OTHER ENTITIES] are at least N=3 other great NFL players, which you must identify.\n",
    "  * You must determine the appropriate number, N, and the names of [OTHER ENTITY_1], [OTHER ENTITY_2],..., [OTHER ENTITY_M].\n",
    "\n",
    "Based upon the top proposition, determine the relevant indicators for [TRAIT]: [INDICATOR_1], [INDICATOR_2], ..., [INDICATOR_M].\n",
    "You must determine the appropriate number, M, and the names of [INDICATOR_1], [INDICATOR_2],..., [INDICATOR_M].\n",
    "\n",
    "In the `query_graph` of the query plan, for `Query` nodes corresponding to \"Tabulate the statistics...\", set the `question` field \n",
    "to that tabulation statement, i.e., \"Tabulate the statistics of ...\" so that a table is returned for those nodes.\n",
    "\n",
    "Make sure that you assign the `Query` nodes for indicators and for other entities as members of the `dependencies` field list of other `Query` nodes that require them.\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    search_cache = search_cache, \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run=False,\n",
    "    report_to_file = True,\n",
    "    print_level=2\n",
    ")\n",
    "display(assessment_brady_vs_others2['kb_dot'])\n",
    "if assessment_brady_vs_others2.get('plan_result'):\n",
    "    _ = print_summary(assessment_brady_vs_others2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b1445-6cfb-413d-8927-4cdb25d61fb5",
   "metadata": {},
   "source": [
    "#### Critical Race Theory in K-12 schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18485d8a-80b4-4310-bade-7ebef8d51c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_crt =  await assess_proposition(\n",
    "    proposition = 'Critical Race Theory is not actually taught in American K-12 schools.', \n",
    "    short_name = 'crt', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9098d1-56be-4a4c-9f9e-f6ff876ebac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_crt['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f98eca-eab0-4fb4-ae83-984cc841e71b",
   "metadata": {},
   "source": [
    "#### 2020 election stolen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab589bee-8f74-44f6-9f74-a969868fa74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_election_2020 =  await assess_proposition(\n",
    "    proposition = \"The 2020 U.S. Presidential election was stolen.\", \n",
    "    short_name = 'election_2020', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e8c6b-4f57-4340-8b5c-8c5df53e5958",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_election_2020['kb_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbeb947-6912-45cf-aff0-1b1004f7000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_election_2020_max =  await assess_proposition(\n",
    "    proposition = \"The 2020 U.S. Presidential election was stolen.\", \n",
    "    short_name = 'election_2020_max', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72abfa8b-a7b0-4c68-9224-82468a727642",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_election_2020_max['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec63511-31c8-4658-9f8e-11681cdab4b3",
   "metadata": {},
   "source": [
    "#### 2016 election stolen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f0f45-8ecb-4d87-8559-93344452c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_election_2016 =  await assess_proposition(\n",
    "    proposition = \"The 2016 U.S. Presidential election was stolen.\", \n",
    "    short_name = 'election_2016', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2c50f-e5fc-429d-99fd-785e4bf9f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_election_2016['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85896be-6de8-4c6c-ae35-562e709c7105",
   "metadata": {},
   "source": [
    "#### Russian interference, 2016 election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8aa06-7a68-4b97-b661-4be818e8400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_election_2016a =  await assess_proposition(\n",
    "    proposition = \"Russian interference swayed the outcome of the 2016 U.S. Presidential election.\", \n",
    "    short_name = 'election_2016a', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e21a84-f166-4a5d-a0e2-9aafdd1679a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_election_2016a['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e43f56-b28f-405e-a36c-ae940bf40fcd",
   "metadata": {},
   "source": [
    "#### NRA and 2nd Amendment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39b349-039f-4c88-a1fb-fd20b30980fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_nra_2nd =  await assess_proposition(\n",
    "    proposition = \"The 2nd Amendment of the U.S. Constitution protects a narrower set of rights than the NRA claims that the 2nd Amendment does.\", \n",
    "    short_name = 'nra_2nd', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4696608-86ab-4d39-8366-1e1e6387c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_nra_2nd['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74123a-8b6a-4f41-9eb8-58d858b25c2f",
   "metadata": {},
   "source": [
    "#### College Value Proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1618b86f-5ab2-4b4b-a290-b51f0bff2671",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_college_value =  await assess_proposition(\n",
    "    proposition =  \"A college education is a bad value proposition for the majority of graduating high school seniors in America today.\", \n",
    "    short_name = 'college_value', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166f96b7-c0b5-4e8c-95a0-5a094079c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_college_value['kb_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf205f4-6c39-4880-8e3a-0660657a1781",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_college_value2 =  await assess_proposition(\n",
    "    proposition =  \"A college education is a good value proposition for the majority of graduating high school seniors in America today.\", \n",
    "    short_name = 'college_value2', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c593e-ba69-4629-b151-7404ff9918b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_college_value2['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8a895-ab8e-4d74-811b-1ad4b2bc7ccf",
   "metadata": {},
   "source": [
    "#### Elon Musk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac547bf7-b48e-4183-b126-fbf45e52d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_musk =  await assess_proposition(\n",
    "    proposition =  \"Elon Musk's current success is due more to his wealth and autonomy in decision-making than due to his brilliance and judgement.\", \n",
    "    short_name = 'musk', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e89ac1-0059-46d8-9c26-9b1c9f4d1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_musk['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0c03f-fa0e-4a7e-ac72-e2c6e59ed5e5",
   "metadata": {},
   "source": [
    "#### Elon Musk, hate-monger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9826b0-ab70-4e0e-9bc8-52d8e2bfa885",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_musk2 =  await assess_proposition(\n",
    "    proposition =  \"Elon Musk's current success is due more to his wealth and autonomy in decision-making than due to his intellect and judgement and despite his engagement in and enablement of hate speech on X (nee Twitter).\", \n",
    "    short_name = 'musk2', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d634352-f734-429a-a85a-bcd5331f7c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_musk2['kb_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a738127-c87d-4ddf-8bfe-6d528537ada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_musk3 =  await assess_proposition2(\n",
    "    proposition = \"Elon Musk is a hatemonger because he actively retweets hateful content and enables hate speech on X (Twitter).\", \n",
    "    short_name = 'musk3',\n",
    "    search_cache = search_cache, \n",
    "    added_instructions =  \"\"\"\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned. DO NOT create Query nodes for these facts.\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* Elon Musk owns X (Twitter).\n",
    "\n",
    "==========\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\",\n",
    "    dynamic = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20,\n",
    "    force_run = True,\n",
    "    save_pickle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3237706-fe6f-4450-b6fb-b6bd6746abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_musk3['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767911ea-395f-44fe-b5eb-54ab11555824",
   "metadata": {},
   "source": [
    "#### Populism in American politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8f8a8-415a-47b6-b807-16c5ff67ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_populism =  await assess_proposition(\n",
    "    proposition =  \"'Populism' ('ordinary people vs. elites') in current American political discourse is a misnomer for 'xenophobia' and 'racism' -- where 'ordinary people' connotes ordinary white, native-born Americans and 'elites' connotes all non-white people as well as white elites.\", \n",
    "    short_name = 'populism', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Be sure to include the <<CONTEXT>> of <<'in current American politics'>> in all searches and queries that you pose.\n",
    "=====\n",
    "\n",
    "<<CONTEXT>>: In current American politics.\n",
    "\n",
    "=====\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e770b8c7-6599-47d4-a5ca-2234342abd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_populism['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba50a54-530e-48f9-9a08-b9d8c4f56a71",
   "metadata": {},
   "source": [
    "#### Ultra-wealthy hegemony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a306df9-7bd2-4034-8b89-2c2fd61b613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_wealth_hegemony =  await assess_proposition(\n",
    "    proposition =  \"Currently, ultra-wealthy American individuals wield the economic hegemony and broader powers that allow them to operate outside of the laws of nations.\", \n",
    "    short_name = 'wealth_hegemony', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f5d8662-61f2-40e9-94d0-0fe147601ae5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bd5ba04-d6a5-42e6-b7c4-10082ed8c58e",
   "metadata": {},
   "source": [
    "#### America post-2024-presidential-election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31c0be-2622-4ad9-979c-d5a9d87fc8b9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "assessment_america_2024 =  await assess_proposition(\n",
    "    proposition =  \"America will be a great place to live after the 2024 U.S. Presidential Election.\", \n",
    "    short_name = 'america_2024', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea6a256-5446-4e04-a191-c815d9227bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_america_2024['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af8bf95-ddf7-4b59-b4e2-198d942f71de",
   "metadata": {},
   "source": [
    "#### America desperately needs a younger president"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3ea3f-42d8-456f-87d6-63d0c4871b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_younger_president =  await assess_proposition(\n",
    "    proposition =  \"America desperately needs a candidate younger than 60-years-old to win the 2024 U.S. Presidential Election.\", \n",
    "    short_name = 'younger_president', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = '', \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcef314-e980-48e7-a0e9-2fc31738bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_younger_president['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f049c423-7943-43ab-a0af-fdf6d8e6a305",
   "metadata": {},
   "source": [
    "#### Guns for personal safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d42e53-cb2c-4f5c-9b87-a7693ccdaac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_get_a_gun =  await assess_proposition(\n",
    "    proposition =  \"I must carry a gun for personal safety because it is visibly obvious that I a member of an ethnic group that is currently often threatened by hate crimes.\", \n",
    "    short_name = 'get_a_gun', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    dynamic = True,\n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048aac0-da44-4ecc-a5c4-c1990f648d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_get_a_gun['kb_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac93e6-2dbe-4dca-aabd-b8e5043b778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_get_a_gun2 =  await assess_proposition(\n",
    "    proposition =  \"I must carry a gun for personal safety because it is visibly obvious that I a member of an ethnic group that is currently often threatened by hate crimes.\", \n",
    "    short_name = 'get_a_gun2', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Accept the <<GIVEN FACTS>> as truths that needn't be questioned.\n",
    "=========\n",
    "\n",
    "<<GIVEN FACTS>>:\n",
    "* I am Jewish.\n",
    "* It is visibly obvious that I am Jewish.\n",
    "* I live in New York City.\n",
    "\n",
    "==========\n",
    "\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = True,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380bff5e-9650-49cf-861d-90e278d3987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_get_a_gun2['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f49b70a-eb8a-40cd-93a8-6dde8d2e71a1",
   "metadata": {},
   "source": [
    "#### Harvard & Asian admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aade36-4a38-40e3-a05d-5b2a985eeb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_harvard_asian =  await assess_proposition2(\n",
    "    proposition =  \"During the years 2002 to 2022, Harvard explicitly applied racial and ethnic quotas to limit the number of Asians admitted to the university.\", \n",
    "    short_name = 'harvard_asian', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\"\"\n",
    "Ensure that all Query nodes lead directly or indirectly back to the main proposition. \n",
    "So, the query graph has a single \"sink\" node capturing the main proposition that generates the ultimate conclusions.\n",
    "\n",
    "Investigate alternative explanations for each subproposition.\n",
    "Investigate evidence against as well as for each subproposition.\n",
    "\"\"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    force_run = False,\n",
    "    save_pickle = True,\n",
    "    report_to_file = True,\n",
    "    kb_label_wrap = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6140265-99b9-49e1-aab8-4f316242f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_harvard_asian['kb_dot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e3bbd-40db-4484-b223-9f4f76a508b4",
   "metadata": {},
   "source": [
    "#### Harvard antisemitism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c51b84-fe21-4945-abbe-374046d03685",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_harvard_antisemitism =  await assess_proposition2(\n",
    "    proposition =  \"Harvard has a serious anti-semitism problem.\", \n",
    "    short_name = 'harvard_antisemitism', \n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_harvard_antisemitism['kb_dot'])\n",
    "_=print_summary(assmnt=assessment_harvard_antisemitism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36f786-ba43-4852-80b1-b11e4317afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = QueryPlan(**(eval(re.sub(\"anti-semitism\",\"anti-Arab racism\", assessment_harvard_antisemitism['plan'].model_dump_json()))))\n",
    "visualize_knowledge_graph(convert_to_graph(a,proposition = \"Harvard has a serious anti-Arab racism problem.\"))\n",
    "assessment_harvard_antiarabracism =  await assess_proposition2(\n",
    "    proposition =  \"Harvard has a serious anti-Arab racism problem.\", \n",
    "    short_name = 'harvard_antiarabracism', \n",
    "    use_plan = a,\n",
    "    search_cache = search_cache, \n",
    "    added_instructions = \"\", \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    print_level = 2\n",
    ")\n",
    "#display(assessment_harvard_antiarabracism['kb_dot'])\n",
    "_=print_summary(assmnt=assessment_harvard_antiarabracism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7879db-46e7-4223-a942-19d9c94df754",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_harvard_racism =  await assess_proposition2(\n",
    "    proposition =  \"Harvard has a serious problem with racism.\", \n",
    "    added_instructions = \"\"\"\n",
    "Separately perform deep investigations into racism against each of the following individual groups:\n",
    "\n",
    "* Jews\n",
    "* Arabs\n",
    "* African-Americans\n",
    "* Latinos\n",
    "* Asians\n",
    "\n",
    "Compile the findings into a thorough report on the proposition.\n",
    "\n",
    "\"\"\",\n",
    "    short_name = 'harvard_racism', \n",
    "    search_cache = search_cache, \n",
    "    client = client,\n",
    "    model = \"gpt-4-turbo-preview\", \n",
    "    report_to_file = True,\n",
    "    print_level = 2\n",
    ")\n",
    "display(assessment_harvard_racism['kb_dot'])\n",
    "_=print_summary(assmnt=assessment_harvard_racism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65967224-af10-4734-b6ca-8a4adf6d3c31",
   "metadata": {},
   "source": [
    "#### Tesla Model Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9eff3-603a-477e-8cd0-e073684e8b27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proposition = \"Tesla Model Y is the best electric vehicle currently available.\"\n",
    "\n",
    "key_question = f\"\"\"# Mandate:\n",
    "Given proposition '{proposition}'. \n",
    "1. Enumerate all sub-propositions, and their sub-propositions and so on, that need to be true to support it.\n",
    "2. Enumerate all factual axiomatic statements as evidence that support or refute each and every one of the propositions. \n",
    "3. Record these propositions and evidence as a textual outline (in 'query_plan_outline') describing the query plan.\n",
    "3. Generate the query plan as a tree of query nodes (in 'query_graph').\n",
    "   - Assign a separate query node for each specific sub-proposition.\n",
    "   - Assign a separate query node for each specific piece of evidence needed to confirm and refute each sub-proposition. \n",
    "   - Be sure to specify SEARCH-type query nodes when needed to gather the latest factual information as evidence.\n",
    "\"\"\"\n",
    "\n",
    "instructions = \"\"\"\n",
    "## Two-Way Contingency Table: \n",
    "This is a tool to maximize thoroughness and objectivity in analyzing each and every proposition.\n",
    "In considering each proposition H and each piece of evidence E, assess the two-way contingency table with 2x2=4 probabilities P(H=True,E=True), P(H=True,E=False), P(H=False,E=True), and P(H=False,E=False).\n",
    "From these joint probabilities, consider the conditional probalities P(E=True | H=True) and P(E=True | H=False), which are the Likelihoods of the evidence given the proposition.\n",
    "Then assuming the observed evidence E=True, compute (1) the Bayes Factor, BF(H;E) = P(E=True | H=True) / P(E=True | H=False), which is a measure of the strength of evidence supporting (if > 1) or refuting (if < 1) the proposition;\n",
    "and (2) the Posterior Probability, P( H=True | E=True ) = P(E=True | H=True) x P(H=True) / P(E=True), where P(E=True) =  P(E=True | H=True) x P(H=True) +  P(E=True | H=False) x P(H=False).\n",
    "\n",
    "### Note: \n",
    "The Weight of Evidence, measured in decibans, WoE(H;E) = 10 x log10 BF(H;E), is considered substantial if |WoE(H;E)| >= 5 decibans.  \n",
    "Also, BF(H;E) = Odds(H=True|E=True)/Odds(H=True), where Odds(x) = P(x)/(1 - P(x)). So, P( H=True | E=True ) = BF(H;E)/(BF(H;E) + 1/Odds(H=True)).\n",
    "In sum, BF & WoE measure how strongly E moves H from its prior beliefs P(H) to its posterior beliefs P(H|E); and P(H|E) & Odds(H|E) measure how strongly beliefs about H ends up in light of E.\n",
    "\n",
    "## Contextual Knowledge -- Assigning Numerical Probabilities\n",
    "For any proposition H = X -> Y (like, X=Next_car, and Y=Is_red), it is often easier to arrive at values for the probability P(H) by considering the chances of different \n",
    "alternative propositions $H_alt_i = X & \\neg Y_i (like, Y_1=Is_blue, Y_2=Is_green, Y_M=Is_any_other_color). Then we can assign the probabilities of the alternative relative to the central proposition\n",
    "of interest: R(H_alt_i) = P(H_alt_i)/P(H)$.  Given that, we arrive at the probability P(H) by $P(H) = 1/(1 + sum_{i}{R(H_alt_i)}$.\n",
    "And this is also useful for assigning conditional probabilities like for Likelihoods P(E|H), consider $R(E|H_alt_i)=P(E|H_alt_i)/P(E|H)$. Then $P(E|H) = 1/(1 + sum_{i}{R(E|H_alt_i)}$.\n",
    "To assign numerical values , consider the probability ratio R(...) by half-orders of magnitude: which is approximately 1/1000, 1/300, 1/100, 1/30, 1/10, 1/3, 1, 3, 10, 30, 100, 300, 1000.\n",
    "Pick the one most suitable based upon knowledge of the subject and any information given in context or found by search.\n",
    "\"\"\"\n",
    "\n",
    "mandate = f\"\"\"\n",
    "# The Question for which we require a Query Plan:\n",
    "Generate a query plan to answer this question: Is '{proposition}' the truth?\"\"\"\n",
    "\n",
    "plan_tesla2 = query_planner( key_question + mandate )\n",
    "#plan = query_planner( key_question + instructions + mandate )\n",
    "pprint(plan_tesla2.model_dump())\n",
    "plan_tesla2_result = await plan_tesla2.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f5d3b5-dc4e-4415-aa90-5413943b0d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_node_result = list(plan_tesla2_result.values())[-1]\n",
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',last_node_result.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + last_node_result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88bdea-3feb-4103-9c9d-69a4ac7b5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla2_search_keys = [q.search_query for q in plan_tesla2.query_graph if len(q.search_query)>0]\n",
    "tesla2_search = {key: summarize_raw_search(eval(search_cache[key])) for key in tesla2_search_keys if search_cache.get(key,None) is not None}\n",
    "\n",
    "report_markdown = make_report(computed_query = last_node_result,search_dict=tesla2_search)\n",
    "\n",
    "mrkdwn_file_path = 'plan_tesla2.Rmd'\n",
    "with open(mrkdwn_file_path, 'w') as file:\n",
    "    file.write(to_ascii(report_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797e0c12-1d54-494a-ba7e-a77d9bd416b3",
   "metadata": {},
   "source": [
    "#### Universal Basic Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe230357-2755-42b2-a008-51c6ef03c4d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proposition = \"Universal Basic Income is the most likely result, and a necessary result, of widespread adoption of Generative AI.\"\n",
    "\n",
    "key_question = f\"\"\"# Mandate:\n",
    "Given proposition '{proposition}'. \n",
    "1. Enumerate all sub-propositions, and their sub-propositions and so on, that need to be true to support it.\n",
    "2. Enumerate all factual axiomatic statements as evidence that support or refute each and every one of the propositions. \n",
    "3. Record these propositions and evidence as a textual outline (in 'query_plan_outline') describing the query plan.\n",
    "3. Generate the query plan as a tree of query nodes (in 'query_graph').\n",
    "   - Assign a separate query node for each specific sub-proposition.\n",
    "   - Assign a separate query node for each specific piece of evidence needed to confirm and refute each sub-proposition. \n",
    "   - Be sure to specify search_flag=True for a query node when needed to gather the latest factual information as evidence.\n",
    "   - Demand that actual specific data and statistics are gathered as evidence and that the answers to the propositions are stated in terms of these data.\n",
    "\"\"\"\n",
    "\n",
    "mandate = f\"\"\"\n",
    "# The Question for which we require a Query Plan:\n",
    "Is '{proposition}' the truth?\"\"\"\n",
    "\n",
    "plan_ubiai = query_planner( key_question + mandate )\n",
    "#plan = query_planner( key_question + instructions + mandate )\n",
    "\n",
    "pprint(plan_ubiai.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c535dd74-bce9-4853-bf2c-3c0f801b027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph: KnowledgeGraph = convert_to_graph(plan_ubiai,proposition,label_wrap=20)\n",
    "kb_dot = visualize_knowledge_graph(graph,flip=True, aspect_ratio= 'auto')\n",
    "kb_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd1532-76bb-4b4c-a27f-ffa1ff7e27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_ubiai_result = await plan_ubiai.execute(model='gpt-4-turbo-preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef309c5-96d4-44f7-adda-48ed0cf1fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_node_ubiai_result_gpt4 = list(plan_ubiai_result.values())[-1]\n",
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',last_node_ubiai_result_gpt4.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + last_node_ubiai_result_gpt4.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40142e26-69fa-445a-8858-ff76538a5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(search_cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8ab97-044b-46be-90a6-972f3b125d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ubiai_search_keys = [\n",
    " 'economic impacts of Generative AI',\n",
    " 'Generative AI impact on employment and income distribution',\n",
    " 'alternative results of widespread adoption of Generative AI',\n",
    " 'precedents of technological advancements leading to policy changes',\n",
    " 'societal and economic conditions that necessitate UBI',\n",
    " 'alternatives to UBI for addressing impacts of Generative AI',\n",
    " 'current implementations or considerations of UBI in response to AI',\n",
    " 'expert opinions on Generative AI and UBI',\n",
    " 'arguments against UBI necessity in AI context',\n",
    " 'expert opinions against Generative AI and UBI']\n",
    "\n",
    "ubiai_search = {key: summarize_raw_search(eval(search_cache[key])) for key in ubiai_search_keys if search_cache.get(key,None) is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09709f7-d866-423f-be34-635708c679b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_markdown_ubiai_gpt4 = generate_report(\n",
    "    plan_outline = plan_ubiai.query_plan_outline,\n",
    "    lastnode=last_node_ubiai_result_gpt4,\n",
    "    computed_queries=plan_ubiai_result,\n",
    "    search_dict=ubiai_search, \n",
    "    filename_stem='plan_ubiai_gpt4', \n",
    "    write_to_file=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc0bd7-a94a-4ea3-b477-7a15536940e7",
   "metadata": {},
   "source": [
    "#### Republican State Legislatures vs. Democrat State Legislatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23145851-6665-4af1-859d-643dcebbea9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#proposition = \"Currently, the American citizens of states with Republican-controlled legislatures are prospering greater than the citizens of Democratic-controlled legislatures.\"\n",
    "proposition = \"Currently, the citizens of Texas are prospering greater than the citizens of California.\"\n",
    "key_question = f\"\"\"# Mandate:\n",
    "Given proposition '{proposition}'. \n",
    "1. Enumerate all sub-propositions, and their sub-propositions and so on, that need to be true to support it.\n",
    "2. Enumerate all factual axiomatic statements as evidence that support or refute each and every one of the propositions. \n",
    "3. Record these propositions and evidence as a textual outline (in 'query_plan_outline') describing the query plan.\n",
    "3. Generate the query plan as a tree of query nodes (in 'query_graph').\n",
    "   - Assign a separate query node for each specific sub-proposition.\n",
    "   - Assign a separate query node for each specific piece of evidence needed to confirm and refute each sub-proposition. \n",
    "   - Be sure to specify SEARCH-type query nodes when needed to gather the latest factual information as evidence.\n",
    "   - Demand that actual specific data and statistics are gathered as evidence and that the answers to the propositions are stated in terms of these data.\n",
    "\"\"\"\n",
    "\n",
    "mandate = f\"\"\"\n",
    "# Question for which we require a Query Plan:\n",
    "Generate a query plan to answer this question: \n",
    "Is '{proposition}' the truth?\"\"\"\n",
    "\n",
    "plan_states = query_planner( key_question + mandate )\n",
    "#plan = query_planner( key_question + instructions + mandate )\n",
    "\n",
    "pprint(plan_states.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ebe40-ab83-4978-8894-4f26879c77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph: KnowledgeGraph = convert_to_graph(plan_states,proposition,label_wrap=20)\n",
    "kb_dot = visualize_knowledge_graph(graph,flip=True, aspect_ratio= 'auto')\n",
    "kb_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c577a753-1fac-4d2f-a96b-95bb49032a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plan_states.query_plan_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e03d3c-cc1a-4fb7-b049-f43e25ad1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_state_govt_result = await plan_states.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22bd9c-8213-41f7-97ba-3cb4739efc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_node_state_govt_result = list(plan_state_govt_result.values())[-1]\n",
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',last_node_state_govt_result.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + last_node_state_govt_result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d557a-9254-4ef5-80e1-a33c54b6433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_state_govt_result_gpt4 = await plan_states.execute(model=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60717e-5e43-48ce-b384-21702e27ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_node_state_govt_result_gpt4 = list(plan_state_govt_result_gpt4.values())[-1]\n",
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',last_node_state_govt_result_gpt4.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + last_node_state_govt_result_gpt4.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b246a998-bf6e-42b6-85b9-c43f758a212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_graph = \"\"\"\n",
    "1. Is 'Currently, the citizens of Texas are prospering greater than the citizens of California.' the truth?\n",
    "   A. What does it mean for citizens to be prospering?\n",
    "   B. Are the citizens of Texas prospering?\n",
    "      i. What are the economic indicators of prosperity in Texas?\n",
    "         a. What is the current unemployment rate in Texas?\n",
    "         b. What is the current GDP per capita in Texas?\n",
    "         c. What is the current median household income in Texas?\n",
    "      ii. What are the social indicators of prosperity in Texas?\n",
    "         a. What is the current education level in Texas?\n",
    "         b. What is the current health status in Texas?\n",
    "   C. Are the citizens of California prospering?\n",
    "      i. What are the economic indicators of prosperity in California?\n",
    "         a. What is the current unemployment rate in California?\n",
    "         b. What is the current GDP per capita in California?\n",
    "         c. What is the current median household income in California?\n",
    "      ii. What are the social indicators of prosperity in California?\n",
    "         a. What is the current education level in California?\n",
    "         b. What is the current health status in California?\n",
    "   D. Comparison of prosperity between Texas and California\n",
    "      i. Compare economic indicators of prosperity between Texas and California\n",
    "      ii. Compare social indicators of prosperity between Texas and California\n",
    "\"\"\"\n",
    "revised_graph = \"\"\"\n",
    "1. Is 'Currently, the citizens of Texas are prospering greater than the citizens of California.' the truth?\n",
    "   A. What does it mean for citizens to be prospering?\n",
    "   B. Are the citizens of Texas prospering?\n",
    "      i. What are the economic indicators of prosperity in Texas?\n",
    "         a. What is the current unemployment rate in Texas?\n",
    "         b. What is the current GDP per capita in Texas?\n",
    "         c. What is the current median household income in Texas?\n",
    "      ii. What are the social indicators of prosperity in Texas?\n",
    "         a. What is the current education level in Texas?\n",
    "         b. What is the current health status in Texas?\n",
    "   C. Are the citizens of California prospering?\n",
    "      i. What are the economic indicators of prosperity in California?\n",
    "         a. What is the current unemployment rate in California?\n",
    "         b. What is the current GDP per capita in California?\n",
    "         c. What is the current median household income in California?\n",
    "      ii. What are the social indicators of prosperity in California?\n",
    "         a. What is the current education level in California?\n",
    "         b. What is the current health status in California?\n",
    "   D. Comparison of prosperity between Texas and California\n",
    "      i. Compare economic indicators of prosperity between Texas and California\n",
    "      ii. Compare social indicators of prosperity between Texas and California\n",
    "\"\"\"\n",
    "\n",
    "result2_queryplan: QueryPlan = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo-preview\", temperature=0, messages=[{'role':'system', 'content':system_prompt},{'role':'user', 'content':query}], max_tokens=2000,\n",
    "    response_model = QueryPlan\n",
    "    #model=\"gpt-3.5-turbo\", temperature=0, messages=[{'role':'user', 'content':query}], max_tokens=10000\n",
    ")\n",
    "\n",
    "print(result2_queryplan.query_plan_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa7313-8587-427c-a9db-20e4e4029c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(search_cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fee2d9-e03a-446a-b118-c9ee9e6ab8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_govt_search_keys = [  'current unemployment rate in Texas',\n",
    " 'current GDP per capita in Texas',\n",
    " 'current median household income in Texas',\n",
    " 'current education level in Texas',\n",
    " 'current health status in Texas',\n",
    " 'current unemployment rate in California',\n",
    " 'current GDP per capita in California',\n",
    " 'current median household income in California',\n",
    " 'current education level in California',\n",
    " 'current health status in California'\n",
    "]\n",
    "\n",
    "state_govt_search = {key: summarize_raw_search(eval(search_cache[key])) for key in state_govt_search_keys if search_cache.get(key,None) is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd8c5d-5d79-417d-bc35-d6e2421a6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_markdown_state_govt = make_report(computed_query = last_node_state_govt_result,search_dict=state_govt_search)\n",
    "\n",
    "mrkdwn_file_path = 'plan_state_govt.Rmd'\n",
    "with open(mrkdwn_file_path, 'w') as file:\n",
    "    file.write(to_ascii(report_markdown_state_govt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2b97b-aa31-46ab-bbc5-2310c3943cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_markdown_state_govt_gpt4 = make_report(computed_query = last_node_state_govt_result_gpt4,search_dict=state_govt_search)\n",
    "\n",
    "mrkdwn_file_path = 'plan_state_govt_gpt4.Rmd'\n",
    "with open(mrkdwn_file_path, 'w') as file:\n",
    "    file.write(to_ascii(report_markdown_state_govt_gpt4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c844bf7-cc6b-491c-b2e3-81393f34477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_markdown_state_govt_gpt4b = generate_report(\n",
    "    plan_outline = plan_states.query_plan_outline,\n",
    "    lastnode=last_node_state_govt_result_gpt4,\n",
    "    computed_queries=plan_state_govt_result_gpt4,\n",
    "    search_dict=state_govt_search, \n",
    "    filename_stem='plan_state_govt_gpt4b', \n",
    "    write_to_file=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc59fb6-a2aa-4fe3-a24e-4a77ee6cadc5",
   "metadata": {},
   "source": [
    "#### Discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a9972-b440-4544-8906-3f6e2f925592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proposition = \"In America today, white Christians are under greater persecution by governmental and private organizations that any Americans of other ethnicities and religions.\"\n",
    "key_question = f\"\"\"# Mandate:\n",
    "Given proposition '{proposition}'. \n",
    "1. Enumerate all sub-propositions, and their sub-propositions and so on, that need to be true to support it.\n",
    "2. Enumerate all factual axiomatic statements as evidence that support or refute each and every one of the propositions. \n",
    "3. Record these propositions and evidence as a textual outline (in 'query_plan_outline') describing the query plan.\n",
    "3. Generate the query plan as a tree of query nodes (in 'query_graph').\n",
    "   - Assign a separate query node for each specific sub-proposition.\n",
    "   - Assign a separate query node for each specific piece of evidence needed to confirm and refute each sub-proposition. \n",
    "   - Be sure to specify SEARCH-type query nodes when needed to gather the latest factual information as evidence.\n",
    "\"\"\"\n",
    "\n",
    "instructions = \"\"\"\n",
    "## Two-Way Contingency Table: \n",
    "This is a tool to maximize thoroughness and objectivity in analyzing each and every proposition.\n",
    "In considering each proposition H and each piece of evidence E, assess the two-way contingency table with 2x2=4 probabilities P(H=True,E=True), P(H=True,E=False), P(H=False,E=True), and P(H=False,E=False).\n",
    "From these joint probabilities, consider the conditional probalities P(E=True | H=True) and P(E=True | H=False), which are the Likelihoods of the evidence given the proposition.\n",
    "Then assuming the observed evidence E=True, compute (1) the Bayes Factor, BF(H;E) = P(E=True | H=True) / P(E=True | H=False), which is a measure of the strength of evidence supporting (if > 1) or refuting (if < 1) the proposition;\n",
    "and (2) the Posterior Probability, P( H=True | E=True ) = P(E=True | H=True) x P(H=True) / P(E=True), where P(E=True) =  P(E=True | H=True) x P(H=True) +  P(E=True | H=False) x P(H=False).\n",
    "\n",
    "### Note: \n",
    "The Weight of Evidence, measured in decibans, WoE(H;E) = 10 x log10 BF(H;E), is considered substantial if |WoE(H;E)| >= 5 decibans.  \n",
    "Also, BF(H;E) = Odds(H=True|E=True)/Odds(H=True), where Odds(x) = P(x)/(1 - P(x)). So, P( H=True | E=True ) = BF(H;E)/(BF(H;E) + 1/Odds(H=True)).\n",
    "In sum, BF & WoE measure how strongly E moves H from its prior beliefs P(H) to its posterior beliefs P(H|E); and P(H|E) & Odds(H|E) measure how strongly beliefs about H ends up in light of E.\n",
    "\n",
    "## Contextual Knowledge -- Assigning Numerical Probabilities\n",
    "For any proposition H = X -> Y (like, X=Next_car, and Y=Is_red), it is often easier to arrive at values for the probability P(H) by considering the chances of different \n",
    "alternative propositions $H_alt_i = X & \\neg Y_i (like, Y_1=Is_blue, Y_2=Is_green, Y_M=Is_any_other_color). Then we can assign the probabilities of the alternative relative to the central proposition\n",
    "of interest: R(H_alt_i) = P(H_alt_i)/P(H)$.  Given that, we arrive at the probability P(H) by $P(H) = 1/(1 + sum_{i}{R(H_alt_i)}$.\n",
    "And this is also useful for assigning conditional probabilities like for Likelihoods P(E|H), consider $R(E|H_alt_i)=P(E|H_alt_i)/P(E|H)$. Then $P(E|H) = 1/(1 + sum_{i}{R(E|H_alt_i)}$.\n",
    "To assign numerical values , consider the probability ratio R(...) by half-orders of magnitude: which is approximately 1/1000, 1/300, 1/100, 1/30, 1/10, 1/3, 1, 3, 10, 30, 100, 300, 1000.\n",
    "Pick the one most suitable based upon knowledge of the subject and any information given in context or found by search.\n",
    "\"\"\"\n",
    "\n",
    "mandate = f\"\"\"\n",
    "# The Question for which we require a Query Plan:\n",
    "Finally, derive a query plan to answer this question: Is '{proposition}' the truth?\"\"\"\n",
    "\n",
    "plan = query_planner( key_question + mandate )\n",
    "#plan = query_planner( key_question + instructions + mandate )\n",
    "\n",
    "pprint(plan.model_dump())\n",
    "plan_discrimination_result = await plan.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac1a17b-b0a2-4464-93fc-a77665a5c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',plan_discrimination_result.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + plan_discrimination_result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0124621-cbc6-4a92-8b38-fe40c56861f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plan.query_plan_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1edcb46-b4cd-45e8-be45-b98c418b5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(search_cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8ac08-ced6-4a65-8e10-748944837c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrimination_search_keys = [ 'Identify the governmental and private organizations in America.',\n",
    " 'Search for recent incidents of persecution against white Christians.',\n",
    " 'Search for policies or actions by governmental and private organizations targeting white Christians.',\n",
    " 'Search for recent incidents of persecution against other ethnicities and religions.',\n",
    " 'Search for policies or actions by governmental and private organizations targeting other ethnicities and religions.'\n",
    "]\n",
    "\n",
    "discrimination_search = {key: summarize_raw_search(eval(search_cache[key])) for key in discrimination_search_keys if search_cache.get(key,None) is not None}\n",
    "\n",
    "report_markdown = make_report(computed_query = plan_discrimination_result,search_dict=discrimination_search)\n",
    "\n",
    "mrkdwn_file_path = 'plan_discrimination.Rmd'\n",
    "with open(mrkdwn_file_path, 'w') as file:\n",
    "    file.write(to_ascii(report_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16546b86-ee0f-4e49-81fc-e5b42c6b3836",
   "metadata": {},
   "source": [
    "#### Google Unreliable Source of Factual News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59af0de5-fb40-43c7-817f-135b52b7bfdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proposition = \"Google searches are an unreliable source of factual news.\"\n",
    "key_question = f\"\"\"# Mandate:\n",
    "Given proposition '{proposition}'. \n",
    "1. Enumerate all sub-propositions, and their sub-propositions and so on, that need to be true to support it.\n",
    "2. Enumerate all factual axiomatic statements as evidence that support or refute each and every one of the propositions. \n",
    "3. Record these propositions and evidence as a textual outline (in 'query_plan_outline') describing the query plan.\n",
    "3. Generate the query plan as a tree of query nodes (in 'query_graph').\n",
    "   - Assign a separate query node for each specific sub-proposition.\n",
    "   - Assign a separate query node for each specific piece of evidence needed to confirm and refute each sub-proposition. \n",
    "   - Be sure to specify SEARCH-type query nodes when needed to gather the latest factual information as evidence.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "instructions = \"\"\"\n",
    "## Two-Way Contingency Table: \n",
    "This is a tool to maximize thoroughness and objectivity in analyzing each and every proposition.\n",
    "In considering each proposition H and each piece of evidence E, assess the two-way contingency table with 2x2=4 probabilities P(H=True,E=True), P(H=True,E=False), P(H=False,E=True), and P(H=False,E=False).\n",
    "From these joint probabilities, consider the conditional probalities P(E=True | H=True) and P(E=True | H=False), which are the Likelihoods of the evidence given the proposition.\n",
    "Then assuming the observed evidence E=True, compute (1) the Bayes Factor, BF(H;E) = P(E=True | H=True) / P(E=True | H=False), which is a measure of the strength of evidence supporting (if > 1) or refuting (if < 1) the proposition;\n",
    "and (2) the Posterior Probability, P( H=True | E=True ) = P(E=True | H=True) x P(H=True) / P(E=True), where P(E=True) =  P(E=True | H=True) x P(H=True) +  P(E=True | H=False) x P(H=False).\n",
    "\n",
    "### Note: \n",
    "The Weight of Evidence, measured in decibans, WoE(H;E) = 10 x log10 BF(H;E), is considered substantial if |WoE(H;E)| >= 5 decibans.  \n",
    "Also, BF(H;E) = Odds(H=True|E=True)/Odds(H=True), where Odds(x) = P(x)/(1 - P(x)). So, P( H=True | E=True ) = BF(H;E)/(BF(H;E) + 1/Odds(H=True)).\n",
    "In sum, BF & WoE measure how strongly E moves H from its prior beliefs P(H) to its posterior beliefs P(H|E); and P(H|E) & Odds(H|E) measure how strongly beliefs about H ends up in light of E.\n",
    "\n",
    "## Contextual Knowledge -- Assigning Numerical Probabilities\n",
    "For any proposition H = X -> Y (like, X=Next_car, and Y=Is_red), it is often easier to arrive at values for the probability P(H) by considering the chances of different \n",
    "alternative propositions $H_alt_i = X & \\neg Y_i (like, Y_1=Is_blue, Y_2=Is_green, Y_M=Is_any_other_color). Then we can assign the probabilities of the alternative relative to the central proposition\n",
    "of interest: R(H_alt_i) = P(H_alt_i)/P(H)$.  Given that, we arrive at the probability P(H) by $P(H) = 1/(1 + sum_{i}{R(H_alt_i)}$.\n",
    "And this is also useful for assigning conditional probabilities like for Likelihoods P(E|H), consider $R(E|H_alt_i)=P(E|H_alt_i)/P(E|H)$. Then $P(E|H) = 1/(1 + sum_{i}{R(E|H_alt_i)}$.\n",
    "To assign numerical values , consider the probability ratio R(...) by half-orders of magnitude: which is approximately 1/1000, 1/300, 1/100, 1/30, 1/10, 1/3, 1, 3, 10, 30, 100, 300, 1000.\n",
    "Pick the one most suitable based upon knowledge of the subject and any information given in context or found by search.\n",
    "\"\"\"\n",
    "\n",
    "mandate = f\"\"\"\n",
    "# The Question for which we require a Query Plan:\n",
    "Finally, derive a query plan to answer this question: Is '{proposition}' the truth?\"\"\"\n",
    "\n",
    "plan = query_planner( key_question + mandate )\n",
    "#plan = query_planner( key_question + instructions + mandate )\n",
    "\n",
    "pprint(plan.model_dump())\n",
    "plan_google_result = await plan.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca820b-3ff9-48d9-a135-8fae90e578e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',plan_google_result.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + plan_google_result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb8fa0-7ba7-4dcf-9dbf-c180e0cc053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plan.query_plan_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99c1cd-4c36-4f4a-9490-639c1e4a376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(search_cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d4d50-920f-4e9d-9269-c898f1b06439",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_search_keys = [ 'Identify the governmental and private organizations in America.',\n",
    " 'Search for recent incidents of persecution against white Christians.',\n",
    " 'Search for policies or actions by governmental and private organizations targeting white Christians.',\n",
    " 'Search for recent incidents of persecution against other ethnicities and religions.',\n",
    " 'Search for policies or actions by governmental and private organizations targeting other ethnicities and religions.'\n",
    "]\n",
    "\n",
    "google_search = {key: summarize_raw_search(eval(search_cache[key])) for key in google_search_keys if search_cache.get(key,None) is not None}\n",
    "\n",
    "report_markdown = make_report(computed_query = plan_google_result,search_dict=google_search)\n",
    "\n",
    "mrkdwn_file_path = 'plan_google.Rmd'\n",
    "with open(mrkdwn_file_path, 'w') as file:\n",
    "    file.write(to_ascii(report_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e178d1ce-26fe-4a01-8393-f1941faf3737",
   "metadata": {},
   "source": [
    "#### College Admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554291f-3350-4448-8b5d-625ae9f0f6b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#proposition = \"Elite American universities unfairly penalize applicants who are not Black or Latino.\"\n",
    "proposition = \"\"\"When evaluating an applicant, admissions policies at elite American universities should consider the applicant's \n",
    "(a) uniqueness -- i.e., unlikeliness of such an applicant achieving what they have in life given the conditional circumstances from which they come -- \n",
    "(b) value to on-campus community -- i.e., unique perspective & contributions to discourse and ideas in academic community given their circumstances -- \n",
    "(c) value to world after graduation -- i.e., role model in community from which applicant came; unique perspective & contributions to professional community within which graduate will work -- and \n",
    "(d) other standard criteria for college admission -- i.e., ability to excel in an academically demanding environment.\"\"\"\n",
    "\n",
    "proposition = \"\"\"When evaluating an applicant, admissions policies at elite American universities should consider the applicant's \n",
    "(a) unlikeliness of such an applicant achieving what they have in life given the conditional circumstances from which they come -- \n",
    "(b) unique perspective & contributions to discourse and ideas in the on-campus academic community given their circumstances -- \n",
    "(c) future as a role model in the neighborhood/community from which applicant came; and unique perspective & contributions to professional community within which graduate will work -- and \n",
    "(d) ability to excel in an academically demanding environment.\"\"\"\n",
    "\n",
    "key_question = f\"\"\"# Mandate:\n",
    "Given proposition '{proposition}'. \n",
    "1. Enumerate all sub-propositions, and their sub-propositions and so on, that need to be true to support it.\n",
    "2. Enumerate all factual axiomatic statements as evidence that support or refute each and every one of the propositions. \n",
    "3. Record these propositions and evidence as a textual outline (in 'query_plan_outline') describing the query plan.\n",
    "3. Generate the query plan as a tree of query nodes (in 'query_graph').\n",
    "   - Assign a separate query node for each specific sub-proposition.\n",
    "   - Assign a separate query node for each specific piece of evidence needed to confirm and refute each sub-proposition. \n",
    "   - Be sure to specify SEARCH-type query nodes when needed to gather the latest factual information as evidence. Assess and comment on the bias and agenda of all sources of search query responses.\n",
    "   - Demand that actual specific data and statistics are gathered as evidence and that the answers to the propositions are stated in terms of these data.\n",
    "\"\"\"\n",
    "\n",
    "mandate = f\"\"\"\n",
    "# Question for which we require a Query Plan\n",
    "Generate a query plan to answer this question: \n",
    "Is this true: '{proposition}' ?\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "plan_college = query_planner( key_question + mandate )\n",
    "#plan = query_planner( key_question + instructions + mandate )\n",
    "\n",
    "pprint(plan_college.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda2227-c1c2-42a6-80c0-b78de0ae971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph: KnowledgeGraph = convert_to_graph(plan_college,proposition,label_wrap=40)\n",
    "kb_dot = visualize_knowledge_graph(graph,flip=True, aspect_ratio= 'auto')\n",
    "kb_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2f6d9-3f9b-48c8-88bb-16643407ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_college2_result = await plan_college.execute(model=\"gpt4-1106-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665692ce-168a-4a3c-8db3-3ee0f83be001",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_node_college2_result = list(plan_college2_result.values())[-1]\n",
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',last_node_college2_result.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + last_node_college2_result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226af0b2-0ffc-4cf7-b132-5fdc2045ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(search_cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25cd7d8-7f31-478d-a439-59c55db205cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "college2_search_keys = ['current admissions policies at elite American universities 2023',\n",
    " \"impact of considering applicant's background on success in university admissions\",\n",
    " 'benefits of diverse perspectives in academic communities',\n",
    " 'impact of role models from diverse backgrounds in communities and professional settings',\n",
    " 'predictors of academic success in elite university settings']\n",
    "\n",
    "college2_search = {key: summarize_raw_search(eval(search_cache[key])) for key in college2_search_keys if search_cache.get(key,None) is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa4530-488b-427a-81fd-33555e3d6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_markdown_college2 = generate_report(\n",
    "    plan_outline = plan_college.query_plan_outline,\n",
    "    lastnode=last_node_college2_result,\n",
    "    computed_queries=plan_college2_result,\n",
    "    search_dict=college2_search, \n",
    "    filename_stem='plan_college2', \n",
    "    write_to_file=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049fc8a0-b5c9-420d-bb74-afbed5737ddb",
   "metadata": {},
   "source": [
    "#### College Value Proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e8480c-c881-4d93-b337-5eeef8500224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proposition = \"A college education is a bad value proposition for the majority of graduating high school seniors in America today.\"\n",
    "key_question = f\"\"\"# Mandate:\n",
    "Given proposition '{proposition}'. First identify all sub-propositions and their sub-propositions, and so on, that need to be true to support it.\n",
    "Then identify all factual axiomatic statements as evidence that support and refute each and every one of the propositions. \n",
    "\n",
    "In generating the query plan, identify these specific sub-propositions and the specific pieces of evidence needed to confirm and refute them.\n",
    "\"\"\"\n",
    "\n",
    "instructions = \"\"\"\n",
    "## Two-Way Contingency Table: \n",
    "This is a tool to maximize thoroughness and objectivity in analyzing each and every proposition.\n",
    "In considering each proposition H and each piece of evidence E, assess the two-way contingency table with 2x2=4 probabilities P(H=True,E=True), P(H=True,E=False), P(H=False,E=True), and P(H=False,E=False).\n",
    "From these joint probabilities, consider the conditional probalities P(E=True | H=True) and P(E=True | H=False), which are the Likelihoods of the evidence given the proposition.\n",
    "Then assuming the observed evidence E=True, compute (1) the Bayes Factor, BF(H;E) = P(E=True | H=True) / P(E=True | H=False), which is a measure of the strength of evidence supporting (if > 1) or refuting (if < 1) the proposition;\n",
    "and (2) the Posterior Probability, P( H=True | E=True ) = P(E=True | H=True) x P(H=True) / P(E=True), where P(E=True) =  P(E=True | H=True) x P(H=True) +  P(E=True | H=False) x P(H=False).\n",
    "\n",
    "### Note: \n",
    "The Weight of Evidence, measured in decibans, WoE(H;E) = 10 x log10 BF(H;E), is considered substantial if |WoE(H;E)| >= 5 decibans.  \n",
    "Also, BF(H;E) = Odds(H=True|E=True)/Odds(H=True), where Odds(x) = P(x)/(1 - P(x)). So, P( H=True | E=True ) = BF(H;E)/(BF(H;E) + 1/Odds(H=True)).\n",
    "In sum, BF & WoE measure how strongly E moves H from its prior beliefs P(H) to its posterior beliefs P(H|E); and P(H|E) & Odds(H|E) measure how strongly beliefs about H ends up in light of E.\n",
    "\n",
    "## Contextual Knowledge -- Assigning Numerical Probabilities\n",
    "For any proposition H = X -> Y (like, X=Next_car, and Y=Is_red), it is often easier to arrive at values for the probability P(H) by considering the chances of different \n",
    "alternative propositions $H_alt_i = X & \\neg Y_i (like, Y_1=Is_blue, Y_2=Is_green, Y_M=Is_any_other_color). Then we can assign the probabilities of the alternative relative to the central proposition\n",
    "of interest: R(H_alt_i) = P(H_alt_i)/P(H)$.  Given that, we arrive at the probability P(H) by $P(H) = 1/(1 + sum_{i}{R(H_alt_i)}$.\n",
    "And this is also useful for assigning conditional probabilities like for Likelihoods P(E|H), consider $R(E|H_alt_i)=P(E|H_alt_i)/P(E|H)$. Then $P(E|H) = 1/(1 + sum_{i}{R(E|H_alt_i)}$.\n",
    "To assign numerical values , consider the probability ratio R(...) by half-orders of magnitude: which is approximately 1/1000, 1/300, 1/100, 1/30, 1/10, 1/3, 1, 3, 10, 30, 100, 300, 1000.\n",
    "Pick the one most suitable based upon knowledge of the subject and any information given in context or found by search.\n",
    "\"\"\"\n",
    "\n",
    "mandate = f\"\"\"\n",
    "# The Question for which we require a Query Plan:\n",
    "Finally, derive a query plan to answer this question: Is '{proposition}' the truth?\"\"\"\n",
    "\n",
    "plan = query_planner( key_question + mandate )\n",
    "#plan = query_planner( key_question + instructions + mandate )\n",
    "\n",
    "pprint(plan.model_dump())\n",
    "plan_college2_result = await plan.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360151c-4ce9-44ae-958c-9de25f5858c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',plan_college2_result.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + plan_college2_result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4bf33-28ca-459d-9337-708236e2189c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proposition = \"A college education is a bad value proposition for the majority of graduating high school seniors in America today.\"\n",
    "key_question = f\"\"\"# Mandate:\n",
    "Given proposition '{proposition}'. First identify all sub-propositions and their sub-propositions, and so on, that need to be true to support it.\n",
    "Then identify all factual axiomatic statements as evidence that support and refute each and every one of the sub-propositions. Explicitly seek information and\n",
    "pose queries for each piece of refuting evidence and each piece of supporting evidence for each sub-proposition.\n",
    "\n",
    "In generating the query plan, identify these specific sub-propositions and the specific pieces of evidence needed to confirm and refute them.\n",
    "\"\"\"\n",
    "\n",
    "instructions = \"\"\"\n",
    "## Two-Way Contingency Table: \n",
    "This is a tool to maximize thoroughness and objectivity in analyzing each and every proposition.\n",
    "In considering each proposition H and each piece of evidence E, assess the two-way contingency table with 2x2=4 probabilities P(H=True,E=True), P(H=True,E=False), P(H=False,E=True), and P(H=False,E=False).\n",
    "From these joint probabilities, consider the conditional probalities P(E=True | H=True) and P(E=True | H=False), which are the Likelihoods of the evidence given the proposition.\n",
    "Then assuming the observed evidence E=True, compute (1) the Bayes Factor, BF(H;E) = P(E=True | H=True) / P(E=True | H=False), which is a measure of the strength of evidence supporting (if > 1) or refuting (if < 1) the proposition;\n",
    "and (2) the Posterior Probability, P( H=True | E=True ) = P(E=True | H=True) x P(H=True) / P(E=True), where P(E=True) =  P(E=True | H=True) x P(H=True) +  P(E=True | H=False) x P(H=False).\n",
    "\n",
    "### Note: \n",
    "The Weight of Evidence, measured in decibans, WoE(H;E) = 10 x log10 BF(H;E), is considered substantial if |WoE(H;E)| >= 5 decibans.  \n",
    "Also, BF(H;E) = Odds(H=True|E=True)/Odds(H=True), where Odds(x) = P(x)/(1 - P(x)). So, P( H=True | E=True ) = BF(H;E)/(BF(H;E) + 1/Odds(H=True)).\n",
    "In sum, BF & WoE measure how strongly E moves H from its prior beliefs P(H) to its posterior beliefs P(H|E); and P(H|E) & Odds(H|E) measure how strongly beliefs about H ends up in light of E.\n",
    "\n",
    "## Contextual Knowledge -- Assigning Numerical Probabilities\n",
    "For any proposition H = X -> Y (like, X=Next_car, and Y=Is_red), it is often easier to arrive at values for the probability P(H) by considering the chances of different \n",
    "alternative propositions $H_alt_i = X & \\neg Y_i (like, Y_1=Is_blue, Y_2=Is_green, Y_M=Is_any_other_color). Then we can assign the probabilities of the alternative relative to the central proposition\n",
    "of interest: R(H_alt_i) = P(H_alt_i)/P(H)$.  Given that, we arrive at the probability P(H) by $P(H) = 1/(1 + sum_{i}{R(H_alt_i)}$.\n",
    "And this is also useful for assigning conditional probabilities like for Likelihoods P(E|H), consider $R(E|H_alt_i)=P(E|H_alt_i)/P(E|H)$. Then $P(E|H) = 1/(1 + sum_{i}{R(E|H_alt_i)}$.\n",
    "To assign numerical values , consider the probability ratio R(...) by half-orders of magnitude: which is approximately 1/1000, 1/300, 1/100, 1/30, 1/10, 1/3, 1, 3, 10, 30, 100, 300, 1000.\n",
    "Pick the one most suitable based upon knowledge of the subject and any information given in context or found by search.\n",
    "\"\"\"\n",
    "\n",
    "mandate = f\"\"\"\n",
    "# The Question for which we require a Query Plan:\n",
    "Finally, derive a query plan to answer this question: Is '{proposition}' the truth?\"\"\"\n",
    "\n",
    "plan = query_planner( key_question + mandate )\n",
    "#plan = query_planner( key_question + instructions + mandate )\n",
    "\n",
    "pprint(plan.model_dump())\n",
    "plan_college2a_result = await plan.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40750960-ee08-4959-a72f-268f63a9ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('QUERY: ' + re.sub(r'\\\\\"response(s)*\\\\\"',r'\\n\"response\\1\"',plan_college2a_result.query.replace(\"\\\\n\",\"\\n\")).replace('\"query\"','\\n\"query\"').replace(',\"response\"',',\\n\\n* \"response\"') + '\\n\\n')\n",
    "print('RESPONSE: ' + plan_college2a_result.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
